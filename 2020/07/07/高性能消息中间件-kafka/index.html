<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>




  
  
    
      
    
    
      
    
  <script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css"/>























  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css"/>

<link rel="stylesheet" href="/css/main.css?v=6.7.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"right","display":"hide","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="一. Kafka介绍1. 概述 Kafka最先是由Linkedin公司开发，是一个分布式的、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当作MQ系统），常见可用于web/nginx日志、访问日志、消息服务等等。于2010年贡献给了Apache基金会并成为顶级开源项目。  主要应用场景：日志收集系统、消息系统 设计目标 以时间复杂度为O(1)的方式提供消息持久化能力">
<meta name="keywords" content="Kafka 消息队列">
<meta property="og:type" content="article">
<meta property="og:title" content="高性能消息中间件-Kafka">
<meta property="og:url" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/index.html">
<meta property="og:site_name" content="温良恭俭让">
<meta property="og:description" content="一. Kafka介绍1. 概述 Kafka最先是由Linkedin公司开发，是一个分布式的、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当作MQ系统），常见可用于web/nginx日志、访问日志、消息服务等等。于2010年贡献给了Apache基金会并成为顶级开源项目。  主要应用场景：日志收集系统、消息系统 设计目标 以时间复杂度为O(1)的方式提供消息持久化能力">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507184834852-1994140834.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190326476-771565746.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190443404-1266011458.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190731172-1317551019.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507192145249-1414897650.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507192840409-1435311830.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200343317-1340406332.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200533571-310409492.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200019142-182025107.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507193553697-2141118410.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507194612622-1788087919.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630212816733.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630161218434.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630165743413.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174523010.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174450976.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174734251.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174708339.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630185143846.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630185123707.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630190157507.png">
<meta property="og:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630191425762.png">
<meta property="og:updated_time" content="2020-07-07T13:31:09.585Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="高性能消息中间件-Kafka">
<meta name="twitter:description" content="一. Kafka介绍1. 概述 Kafka最先是由Linkedin公司开发，是一个分布式的、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当作MQ系统），常见可用于web/nginx日志、访问日志、消息服务等等。于2010年贡献给了Apache基金会并成为顶级开源项目。  主要应用场景：日志收集系统、消息系统 设计目标 以时间复杂度为O(1)的方式提供消息持久化能力">
<meta name="twitter:image" content="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507184834852-1994140834.png">



  <link rel="alternate" href="/atom.xml" title="温良恭俭让" type="application/atom+xml"/>




  <link rel="canonical" href="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>高性能消息中间件-Kafka | 温良恭俭让</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband">
    <a href="https://github.com/LumpCode" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    </div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
    <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">温良恭俭让</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Stay hungry Stay foolish</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br/>站点地图</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-baidusitemap">

    
    
    
      
    

    

    <a href="/baidusitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br/>baidusitemap</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://liugan96.top/2020/07/07/高性能消息中间件-kafka/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LumpCode"/>
      <meta itemprop="description" content="一个梦想发财的男孩子"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="温良恭俭让"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">高性能消息中间件-Kafka

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-07-07 21:27:58 / 修改时间：21:31:09" itemprop="dateCreated datePublished" datetime="2020-07-07T21:27:58+08:00">2020-07-07</time>
            

            
              

              
            
          </span>

          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <a href="/2020/07/07/高性能消息中间件-kafka/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2020/07/07/高性能消息中间件-kafka/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             热度： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            <span>℃</span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">71k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">1:05</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="一-Kafka介绍"><a href="#一-Kafka介绍" class="headerlink" title="一. Kafka介绍"></a>一. Kafka介绍</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507184834852-1994140834.png" alt="img"></p>
<p>Kafka最先是由Linkedin公司开发，是一个分布式的、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当作MQ系统），常见可用于web/nginx日志、访问日志、消息服务等等。于2010年贡献给了Apache基金会并成为顶级开源项目。</p>
<ul>
<li>主要应用场景：日志收集系统、消息系统</li>
<li>设计目标<ul>
<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。</li>
<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。</li>
<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。</li>
<li>同时支持离线数据处理和实时数据处理。</li>
<li>Scale out:支持在线水平扩展</li>
</ul>
</li>
</ul>
<h3 id="1-2-消息系统介绍"><a href="#1-2-消息系统介绍" class="headerlink" title="1.2 消息系统介绍"></a>1.2 消息系统介绍</h3><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需要关系数据，不需要关心数据之间是如何传递的。</p>
<p>分布式消息传递基于可靠的消息队列，在客户端和消息系统之间异步传递消息。</p>
<p>消息的传递模式：</p>
<ul>
<li><p>点对点传递</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190326476-771565746.png" alt="img"></p>
</li>
<li><p>发布-订阅模式（大部分消息系统选用此种方式）</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190443404-1266011458.png" alt="img"></p>
</li>
</ul>
<h3 id="1-3-Kafka的优点"><a href="#1-3-Kafka的优点" class="headerlink" title="1.3 Kafka的优点"></a>1.3 Kafka的优点</h3><ul>
<li>解耦</li>
<li>我们在项目开始的时候预测将来可能出现的需求是十分困难的。但是消息系统不用关心其内部处理过程，只需要遵循其约定好的消息的接口。这就允许你方便的扩展或修改其两边的处理过程，只需要保证其双方均遵循同样的接口约束。</li>
<li>冗余（副本）<ul>
<li>有些情况下处理消息数据的过程或许会失败，除非消息数据被持久化了，否则就会丢失。消息队列通常会把消息数据持久化直到确保他们已经被完全处理之后，通过这一方式避免消息丢失的风险。许多消息队列采取“插入-获取-删除”的策略模式，把一个消息删除之前需要你的系统明确的指定改条消息已经被处理完毕，从而保证了你的数据被安全的保存到你使用完毕。</li>
</ul>
</li>
<li>扩展性<ul>
<li>消息队列解耦了我们的处理过程，所以我们消息的入队和处理速率是很容易的，只需要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。</li>
</ul>
</li>
<li>灵活性&amp;峰值处理能力<ul>
<li>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</li>
</ul>
</li>
<li>可恢复性<ul>
<li>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</li>
</ul>
</li>
<li>顺序保证<ul>
<li>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。</li>
</ul>
</li>
<li><p>缓冲</p>
<ul>
<li>在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</li>
</ul>
</li>
<li><p>异步通信</p>
<ul>
<li>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</li>
</ul>
</li>
</ul>
<h3 id="1-4-Kafka中的术语解释"><a href="#1-4-Kafka中的术语解释" class="headerlink" title="1.4 Kafka中的术语解释"></a>1.4 Kafka中的术语解释</h3><p>在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190731172-1317551019.png" alt="img"></p>
<p>上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。</p>
<p>如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。</p>
<h4 id="1-4-1-broker"><a href="#1-4-1-broker" class="headerlink" title="1.4.1 broker"></a>1.4.1 broker</h4><p>Kafka 集群包含一个或多个服务器，服务器节点称为broker。</p>
<p>broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。</p>
<p>如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。</p>
<p>如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。</p>
<h4 id="1-4-2-Topic"><a href="#1-4-2-Topic" class="headerlink" title="1.4.2 Topic"></a>1.4.2 Topic</h4><p>每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p>
<p>类似于数据库的表名</p>
<h4 id="1-4-3-Partition"><a href="#1-4-3-Partition" class="headerlink" title="1.4.3 Partition"></a>1.4.3 Partition</h4><p>topic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。</p>
<h4 id="1-4-4-Producer"><a href="#1-4-4-Producer" class="headerlink" title="1.4.4 Producer"></a>1.4.4 Producer</h4><p>生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息<strong>追加</strong>到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。</p>
<h4 id="1-4-5-Consumer"><a href="#1-4-5-Consumer" class="headerlink" title="1.4.5 Consumer"></a>1.4.5 Consumer</h4><p>消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。</p>
<h4 id="1-4-6-Consumer-Group"><a href="#1-4-6-Consumer-Group" class="headerlink" title="1.4.6 Consumer Group"></a>1.4.6 Consumer Group</h4><p>每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p>
<h4 id="1-4-7-Leader"><a href="#1-4-7-Leader" class="headerlink" title="1.4.7 Leader"></a>1.4.7 Leader</h4><p>每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。</p>
<h4 id="1-4-8-Follower"><a href="#1-4-8-Follower" class="headerlink" title="1.4.8 Follower"></a>1.4.8 Follower</h4><p>Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。</p>
<h2 id="二-kafka安装和启动"><a href="#二-kafka安装和启动" class="headerlink" title="二. kafka安装和启动"></a>二. kafka安装和启动</h2><h3 id="Step-1-下载代码"><a href="#Step-1-下载代码" class="headerlink" title="Step 1: 下载代码"></a>Step 1: 下载代码</h3><p>下载<code>2.5.0</code>版本并且解压它。</p>
<h3 id="Step-2-启动服务"><a href="#Step-2-启动服务" class="headerlink" title="Step 2: 启动服务"></a>Step 2: 启动服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; tar -xzf kafka_2.12-2.5.0.tgz </span><br><span class="line">&gt; cd kafka_2.12-2.3.0</span><br></pre></td></tr></table></figure>
<p>2.12：  scala版本，2.5.0：kafka版本</p>
<p>运行kafka需要使用Zookeeper，所以你需要先启动Zookeeper，如果你没有Zookeeper，你可以使用kafka自带打包和配置好的Zookeeper。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/zookeeper-server-start.sh config/zookeeper.properties</span><br><span class="line">[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>现在启动kafka服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)</span><br><span class="line">[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="Step-3-创建一个主题-topic"><a href="#Step-3-创建一个主题-topic" class="headerlink" title="Step 3: 创建一个主题(topic)"></a>Step 3: 创建一个主题(topic)</h3><p>创建一个名为“test”的Topic，只有一个分区和一个备份：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure>
<p>创建好之后，可以通过运行以下命令，查看已创建的topic信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">test</span><br></pre></td></tr></table></figure>
<p>或者，除了手工创建topic外，你也可以配置你的broker，当发布一个不存在的topic时自动创建topic。</p>
<h3 id="Step-4-发送消息"><a href="#Step-4-发送消息" class="headerlink" title="Step 4: 发送消息"></a>Step 4: 发送消息</h3><p>Kafka提供了一个命令行的工具，可以从输入文件或者命令行中读取消息并发送给Kafka集群。每一行是一条消息。<br>运行producer（生产者）,然后在控制台输入几条消息到服务器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line">This is a message</span><br><span class="line">This is another message</span><br></pre></td></tr></table></figure>
<h3 id="Step-5-消费消息"><a href="#Step-5-消费消息" class="headerlink" title="Step 5: 消费消息"></a>Step 5: 消费消息</h3><p>Kafka也提供了一个消费消息的命令行工具，将存储的信息输出出来。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line">This is a message</span><br><span class="line">This is another message</span><br></pre></td></tr></table></figure>
<p>如果你有2台不同的终端上运行上述命令，那么当你在运行生产者时，消费者就能消费到生产者发送的消息。</p>
<h3 id="Step-6-设置多个broker集群"><a href="#Step-6-设置多个broker集群" class="headerlink" title="Step 6: 设置多个broker集群"></a>Step 6: 设置多个broker集群</h3><p>到目前，我们只是单一的运行一个broker，没什么意思。对于Kafka，一个broker仅仅只是一个集群的大小，所有让我们多设几个broker。</p>
<p>首先为每个broker创建一个配置文件: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; cp config/server.properties config/server-1.properties </span><br><span class="line">&gt; cp config/server.properties config/server-2.properties</span><br></pre></td></tr></table></figure>
<p>现在编辑这些新建的文件，设置以下属性：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config/server-1.properties: </span><br><span class="line">    broker.id=1 </span><br><span class="line">    listeners=PLAINTEXT://:9093 </span><br><span class="line">    log.dir=/tmp/kafka-logs-1</span><br><span class="line"></span><br><span class="line">config/server-2.properties: </span><br><span class="line">    broker.id=2 </span><br><span class="line">    listeners=PLAINTEXT://:9094 </span><br><span class="line">    log.dir=/tmp/kafka-logs-2</span><br></pre></td></tr></table></figure>
<p>复制</p>
<p><code>broker.id</code>是集群中每个节点的唯一且永久的名称，我们修改端口和日志目录是因为我们现在在同一台机器上运行，我们要防止broker在同一端口上注册和覆盖对方的数据。</p>
<p>我们已经运行了zookeeper和刚才的一个kafka节点，所有我们只需要在启动2个新的kafka节点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-server-start.sh config/server-1.properties &amp;</span><br><span class="line">... </span><br><span class="line">&gt; bin/kafka-server-start.sh config/server-2.properties &amp;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>现在，我们创建一个新topic，把备份设置为：3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>
<p>好了，现在我们已经有了一个集群了，我们怎么知道每个集群在做什么呢？运行命令“describe topics”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:</span><br><span class="line">Topic: my-replicated-topic    Partition: 0    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0</span><br></pre></td></tr></table></figure>
<p>输出解释：第一行是所有分区的摘要，其次，每一行提供一个分区信息，因为我们只有一个分区，所以只有一行。</p>
<ul>
<li>“leader”：该节点负责该分区的所有的读和写，每个节点的<code>leader</code>都是随机选择的。</li>
<li>“replicas”：备份的节点列表，无论该节点是否是leader或者目前是否还活着，只是显示。</li>
<li>“isr”：“同步备份”的节点列表，也就是活着的节点并且正在同步leader。</li>
</ul>
<p>我们运行这个命令，看看一开始我们创建的那个节点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test</span><br><span class="line">Topic:test    PartitionCount:1    ReplicationFactor:1    Configs:</span><br><span class="line">Topic: test    Partition: 0    Leader: 0    Replicas: 0    Isr: 0</span><br></pre></td></tr></table></figure>
<p>这并不奇怪，刚才创建的主题没有Replicas，并且在服务器“0”上，我们创建它的时候，集群中只有一个服务器，所以是“0”。</p>
<p>让我们来发布一些信息在新的topic上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic</span><br><span class="line"> ...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>现在，消费这些消息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic</span><br><span class="line"> ...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>我们要测试集群的容错，kill掉leader，Broker1作为当前的leader，也就是kill掉Broker1。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; ps | grep server-1.properties</span><br><span class="line">7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java... </span><br><span class="line">&gt; kill -9 7564</span><br></pre></td></tr></table></figure>
<p>在Windows上使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; wmic process where &quot;caption = &apos;java.exe&apos; and commandline like &apos;%server-1.properties%&apos;&quot; get processid</span><br><span class="line">ProcessId</span><br><span class="line">6016</span><br><span class="line">&gt; taskkill /pid 6016 /f</span><br></pre></td></tr></table></figure>
<p>备份节点之一成为新的leader，而broker1已经不在同步备份集合里了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:</span><br><span class="line">Topic: my-replicated-topic    Partition: 0    Leader: 2    Replicas: 1,2,0    Isr: 2,0</span><br></pre></td></tr></table></figure>
<p>但是，消息仍然没丢：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<h3 id="Step-7-使用-Kafka-Connect-来-导入-导出-数据"><a href="#Step-7-使用-Kafka-Connect-来-导入-导出-数据" class="headerlink" title="Step 7: 使用 Kafka Connect 来 导入/导出 数据"></a>Step 7: 使用 Kafka Connect 来 导入/导出 数据</h3><p>从控制台写入和写回数据是一个方便的开始，但你可能想要从其他来源导入或导出数据到其他系统。对于大多数系统，可以使用kafka Connect，而不需要编写自定义集成代码。</p>
<p><code>Kafka Connect</code>是导入和导出数据的一个工具。它是一个可扩展的工具，运行连接器，实现与自定义的逻辑的外部系统交互。在这个快速入门里，我们将看到如何运行Kafka Connect用简单的连接器从文件导入数据到Kafka主题，再从Kafka主题导出数据到文件。</p>
<p>首先，我们首先创建一些“种子”数据用来测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo -e &quot;foo\nbar&quot; &gt; test.txt</span><br></pre></td></tr></table></figure>
<p>windows上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; echo foo&gt; test.txt</span><br><span class="line">&gt; echo bar&gt;&gt; test.txt</span><br></pre></td></tr></table></figure>
<p>接下来，我们开始2个连接器运行在独立的模式，这意味着它们运行在一个单一的，本地的，专用的进程。我们提供3个配置文件作为参数。首先是Kafka Connect处理的配置，包含常见的配置，例如要连接的Kafka broker和数据的序列化格式。其余的配置文件都指定了要创建的连接器。包括连接器唯一名称，和要实例化的连接器类。以及连接器所需的任何其他配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</span><br></pre></td></tr></table></figure>
<p>kafka附带了这些示例的配置文件，并且使用了刚才我们搭建的本地集群配置并创建了2个连接器：第一个是源连接器，从输入文件中读取并发布到Kafka主题中，第二个是接收连接器，从kafka主题读取消息输出到外部文件。</p>
<p>在启动过程中，你会看到一些日志消息，包括一些连接器实例化的说明。一旦kafka Connect进程已经开始，导入连接器应该读取从</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.txt</span><br></pre></td></tr></table></figure>
<p>和写入到topic</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connect-test</span><br></pre></td></tr></table></figure>
<p>,导出连接器从主题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connect-test</span><br></pre></td></tr></table></figure>
<p>读取消息写入到文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.sink.txt</span><br></pre></td></tr></table></figure>
<p>. 我们可以通过验证输出文件的内容来验证数据数据已经全部导出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">more test.sink.txt</span><br><span class="line"> foo</span><br><span class="line"> bar</span><br></pre></td></tr></table></figure>
<p>注意，导入的数据也已经在Kafka主题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connect-test</span><br></pre></td></tr></table></figure>
<p>里,所以我们可以使用该命令查看这个主题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning</span><br><span class="line"> &#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false&#125;,&quot;payload&quot;:&quot;foo&quot;&#125;</span><br><span class="line">&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false&#125;,&quot;payload&quot;:&quot;bar&quot;&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>连接器继续处理数据，因此我们可以添加数据到文件并通过管道移动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;Another line&quot; &gt;&gt; test.txt</span><br></pre></td></tr></table></figure>
<p>你应该会看到出现在消费者控台输出一行信息并导出到文件。</p>
<h3 id="Step-8-使用Kafka-Stream来处理数据"><a href="#Step-8-使用Kafka-Stream来处理数据" class="headerlink" title="Step 8: 使用Kafka Stream来处理数据"></a>Step 8: 使用Kafka Stream来处理数据</h3><p>Kafka Stream是kafka的客户端库，用于实时流处理和分析存储在kafka broker的数据，这个快速入门示例将演示如何运行一个流应用程序。一个WordCountDemo的例子（为了方便阅读，使用的是java8 lambda表达式）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">KTable wordCounts = textLines</span><br><span class="line">    // Split each text line, by whitespace, into words.</span><br><span class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(&quot;W+&quot;)))</span><br><span class="line"></span><br><span class="line">    // Ensure the words are available as record keys for the next aggregate operation.</span><br><span class="line">    .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value))</span><br><span class="line"></span><br><span class="line">    // Count the occurrences of each word (record key) and store the results into a table named &quot;Counts&quot;.</span><br><span class="line">    .countByKey(&quot;Counts&quot;)</span><br></pre></td></tr></table></figure>
<p>它实现了wordcount算法，从输入的文本计算出一个词出现的次数。然而，不像其他的WordCount的例子，你可能会看到，在有限的数据之前，执行的演示应用程序的行为略有不同，因为它的目的是在一个无限的操作，数据流。类似的有界变量，它是一种动态算法，跟踪和更新的单词计数。然而，由于它必须假设潜在的无界输入数据，它会定期输出其当前状态和结果，同时继续处理更多的数据，因为它不知道什么时候它处理过的“所有”的输入数据。</p>
<p>现在准备输入数据到kafka的topic中，随后kafka Stream应用处理这个topic的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; echo -e &quot;all streams lead to kafka\nhello kafka streams\njoin kafka summit&quot; &gt; file-input.txt</span><br></pre></td></tr></table></figure>
<p>接下来，使用控制台的producer 将输入的数据发送到指定的topic（streams-file-input）中，（在实践中，stream数据可能会持续流入，其中kafka的应用将启动并运行）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --create \</span><br><span class="line">            --zookeeper localhost:2181 \</span><br><span class="line">            --replication-factor 1 \</span><br><span class="line">            --partitions 1 \</span><br><span class="line">            --topic streams-file-input</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list localhost:9092 --topic streams-file-input</span><br></pre></td></tr></table></figure>
<p>现在，我们运行 WordCount 处理输入的数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo</span><br></pre></td></tr></table></figure>
<p>不会有任何的STDOUT输出，除了日志，结果不断地写回另一个topic（streams-wordcount-output），demo运行几秒，然后，不像典型的流处理应用程序，自动终止。</p>
<p>现在我们检查WordCountDemo应用，从输出的topic读取。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./bin/kafka-console-consumer --zookeeper localhost:2181 </span><br><span class="line">            --topic streams-wordcount-output </span><br><span class="line">            --from-beginning </span><br><span class="line">            --formatter kafka.tools.DefaultMessageFormatter </span><br><span class="line">            --property print.key=true </span><br><span class="line">            --property print.key=true </span><br><span class="line">            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer </span><br><span class="line">            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</span><br></pre></td></tr></table></figure>
<p>输出数据打印到控台（你可以使用Ctrl-C停止）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">all     1</span><br><span class="line">streams 1</span><br><span class="line">lead    1</span><br><span class="line">to      1</span><br><span class="line">kafka   1</span><br><span class="line">hello   1</span><br><span class="line">kafka   2</span><br><span class="line">streams 2</span><br><span class="line">join    1</span><br><span class="line">kafka   3</span><br><span class="line">summit  1</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>第一列是message的key，第二列是message的value，要注意，输出的实际是一个连续的更新流，其中每条数据（即：原始输出的每行）是一个单词的最新的count，又叫记录键“kafka”。对于同一个key有多个记录，每个记录之后是前一个的更新。</p>
<h2 id="三-Kafka核心API与设计理念详解"><a href="#三-Kafka核心API与设计理念详解" class="headerlink" title="三. Kafka核心API与设计理念详解"></a>三. Kafka核心API与设计理念详解</h2><h3 id="1-Kafka的架构"><a href="#1-Kafka的架构" class="headerlink" title="1.Kafka的架构"></a>1.Kafka的架构</h3><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507192145249-1414897650.png" alt="img"></p>
<p>如上图所示，该图是一个典型的Kafka集群中包含若干Producer，若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p>
<h3 id="2-Topics和Partition"><a href="#2-Topics和Partition" class="headerlink" title="2.Topics和Partition"></a>2.Topics和Partition</h3><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507192840409-1435311830.png" alt="img"></p>
<p>对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配置<code>KAFKA_HOME/config/server.properties</code>，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># The minimum age of a log file to be eligible for deletion 符合删除条件的日志文件的最小生存时间</span><br><span class="line">log.retention.hours=168</span><br><span class="line"># The maximum size of a log segment file. When this size is reached a new log segment will be created.日志段文件的最大大小。达到此大小后，将创建一个新的日志段。</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"># The interval at which log segments are checked to see if they can be deleted according to the retention policies 检查日志段以了解是否可以根据保留策略将其删除的时间间隔</span><br><span class="line">log.retention.check.interval.ms=300000 # 300s </span><br><span class="line"># If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction. 如果设置了log.cleaner.enable = true，则将启用清理器，然后可以标记单个日志以进行日志压缩。</span><br><span class="line">log.cleaner.enable=false</span><br></pre></td></tr></table></figure>
<p>因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。</p>
<h4 id="2-1-创建Topic"><a href="#2-1-创建Topic" class="headerlink" title="2.1 创建Topic"></a>2.1 创建Topic</h4><p>创建 topic 的序列图如下所示：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200343317-1340406332.png" alt="img"></p>
<p>流程说明：</p>
<blockquote>
<p>1、 controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。<br>2、 controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：<br>     2.1、 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR<br>     2.2、 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state<br>3、 controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。</p>
</blockquote>
<h4 id="2-2-删除topic"><a href="#2-2-删除topic" class="headerlink" title="2.2 删除topic"></a>2.2 删除topic</h4><p>删除 topic 的序列图如下所示：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200533571-310409492.png" alt="img"></p>
<p>流程说明：</p>
<blockquote>
<p>1、 controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。<br>2、 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。</p>
</blockquote>
<h3 id="3-Producer消息路由"><a href="#3-Producer消息路由" class="headerlink" title="3.Producer消息路由"></a>3.Producer消息路由</h3><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在<code>KAFKA_HOME/config/server.properties</code>中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。</p>
<p>在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。</p>
<blockquote>
<p>Math.abs(“routerKey or groupName”.hashCode()) % ParititionNum</p>
</blockquote>
<h4 id="3-1-写入方式"><a href="#3-1-写入方式" class="headerlink" title="3.1 写入方式"></a>3.1 写入方式</h4><p>producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。</p>
<h4 id="3-2-消息路由"><a href="#3-2-消息路由" class="headerlink" title="3.2 消息路由"></a>3.2 消息路由</h4><p>producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：</p>
<blockquote>
<p>1、 指定了 patition，则直接使用；<br>2、 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition<br>3、 patition 和 key 都未指定，使用轮询选出一个 patition。</p>
</blockquote>
<h4 id="3-3-写入流程"><a href="#3-3-写入流程" class="headerlink" title="3.3 写入流程"></a>3.3 写入流程</h4><p>producer 写入消息序列图如下所示：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200019142-182025107.png" alt="img"></p>
<p>流程说明：</p>
<blockquote>
<p>1、 producer 先从 zookeeper 的 “/brokers/…/state” 节点找到该 partition 的 leader<br>2、 producer 将消息发送给该 leader<br>3、 leader 将消息写入本地 log<br>4、 followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK<br>5、 leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</p>
</blockquote>
<h3 id="4-Consumer-Group"><a href="#4-Consumer-Group" class="headerlink" title="4.Consumer Group"></a>4.Consumer Group</h3><p>使用Consumer high level API的时候,同一个Topic的一条消息只能被同一个Consumer Group中的一个Consumer消费,但是多个Consumer Group则可以同时消费这一信息.</p>
<blockquote>
<ol>
<li>保证消息消费的顺序性</li>
</ol>
<p>传统消息中间件存在一个queue中， c1， c2， c3三个消费者从queue中取得的顺序在消息中间件看起来是顺序一致的，但是三个消费者实际处理时可能不一定是顺序的，尤其是多个消费者之间存在业务严格明确依赖的情况下， 比如c1用户下单后，c2再优惠，c3再送积分，此时传统的消息中间件是没有一个很好发办法去处理的。Kafka就可以解决这个问题。</p>
<p>Kafka解决这个问题的办法：</p>
<ol>
<li>首先规定了一个分区只能有一个消费者，我们可以把有业务依赖性的消息往一个分区中发送</li>
<li>多个分区可以提高并发</li>
</ol>
<p>既可以满足并发，也可以满足消息的一致性</p>
</blockquote>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507193553697-2141118410.png" alt="img"></p>
<p>这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。</p>
<h3 id="5-Push-vs-Pull"><a href="#5-Push-vs-Pull" class="headerlink" title="5.Push vs. Pull"></a>5.Push vs. Pull</h3><p>作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。</p>
<p>push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。</p>
<p>对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p>
<h3 id="6-Kafka-delivery-guarantee"><a href="#6-Kafka-delivery-guarantee" class="headerlink" title="6.Kafka delivery guarantee"></a>6.Kafka delivery guarantee</h3><p>有这么几种可能的delivery guarantee：</p>
<blockquote>
<p>At most once 　　消息可能会丢，但绝不会重复传输</p>
<p>At least one 　　 消息绝不会丢，但可能会重复传输</p>
<p>Exactly once 　　 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。</p>
</blockquote>
<p>当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。</p>
<p>接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。</p>
<p><strong>Kafka默认保证At least once</strong>，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。</p>
<h2 id="四-Kafka的高可用"><a href="#四-Kafka的高可用" class="headerlink" title="四. Kafka的高可用"></a>四. Kafka的高可用</h2><h3 id="1-高可用的由来"><a href="#1-高可用的由来" class="headerlink" title="1. 高可用的由来"></a>1. 高可用的由来</h3><p>​        在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。</p>
<p>　　如果Producer使用同步模式则Producer会在尝试重新发送message.send.max.retries（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。</p>
<p>　　如果Producer使用异步模式，则Producer会尝试重新发送message.send.max.retries（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。同时，Kafka的Producer并未对异步模式提供callback接口。</p>
<p>　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。</p>
<p>　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。</p>
<p>　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。</p>
<h3 id="2-Kafka-HA的设计解析"><a href="#2-Kafka-HA的设计解析" class="headerlink" title="2. Kafka HA的设计解析"></a>2. Kafka HA的设计解析</h3><h4 id="2-1-如何将所有Replica均匀分布到整个集群"><a href="#2-1-如何将所有Replica均匀分布到整个集群" class="headerlink" title="2.1 如何将所有Replica均匀分布到整个集群"></a>2.1 如何将所有Replica均匀分布到整个集群</h4><p>为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。</p>
<p>Kafka分配Replica的算法如下：</p>
<p>1.将所有Broker（假设共n个Broker）和待分配的Partition排序</p>
<p>2.将第i个Partition分配到第（i mod n）个Broker上</p>
<p>3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</p>
<h4 id="2-2-Data-Replication（副本策略）"><a href="#2-2-Data-Replication（副本策略）" class="headerlink" title="2.2 Data Replication（副本策略）"></a>2.2 Data Replication（副本策略）</h4><p>Kafka的高可靠性的保障来源于其健壮的副本（replication）策略。</p>
<h5 id="2-2-1-消息传递同步策略"><a href="#2-2-1-消息传递同步策略" class="headerlink" title="2.2.1 消息传递同步策略"></a>2.2.1 消息传递同步策略</h5><p>Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少，Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。</p>
<p>为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。</p>
<p>Consumer读消息也是从Leader读取，只有被commit过的消息才会暴露给Consumer。</p>
<p>Kafka Replication的数据流如下图所示：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507194612622-1788087919.png" alt="img"></p>
<h5 id="2-2-2-ACK前需要保证有多少个备份"><a href="#2-2-2-ACK前需要保证有多少个备份" class="headerlink" title="2.2.2 ACK前需要保证有多少个备份"></a>2.2.2 ACK前需要保证有多少个备份</h5><p>对于Kafka而言，定义一个Broker是否“活着”包含两个条件：</p>
<ul>
<li>一是它必须维护与ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。</li>
<li>二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。</li>
</ul>
<p>Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在<code>KAFKA_HOME/config/server.properties</code>中通过<code>replica.lag.max.messages</code>配置，其默认值是4000）或者<code>Follower</code>超过一定时间（该值可在<code>KAFKA_HOME/config/server.properties</code>中通过<code>replica.lag.time.max.ms</code>来配置，其默认值是10000）未向<code>Leader</code>发送<code>fetch</code>请求。</p>
<p>Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。</p>
<p>需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。</p>
<h5 id="2-2-3-Leader-Election算法"><a href="#2-2-3-Leader-Election算法" class="headerlink" title="2.2.3 Leader Election算法"></a>2.2.3 Leader Election算法</h5><p>Leader选举本质上是一个分布式锁，有两种方式实现基于ZooKeeper的分布式锁：</p>
<ul>
<li>节点名称唯一性：多个客户端创建一个节点，只有成功创建节点的客户端才能获得锁</li>
<li>临时顺序节点：所有客户端在某个目录下创建自己的临时顺序节点，只有序号最小的才获得锁</li>
</ul>
<p>一种非常常用的选举leader的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在ZooKeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种方式。</p>
<p>Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。</p>
<p>虽然Majority Vote与ISR相比有不需等待最慢的Broker这一优势，但是Kafka作者认为Kafka可以通过Producer选择是否被commit阻塞来改善这一问题，并且节省下来的Replica和磁盘使得ISR模式仍然值得。</p>
<h5 id="2-2-4-如何处理所有Replica都不工作"><a href="#2-2-4-如何处理所有Replica都不工作" class="headerlink" title="2.2.4 如何处理所有Replica都不工作"></a>2.2.4 如何处理所有Replica都不工作</h5><p>在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：</p>
<p>1.等待ISR中的任一个Replica“活”过来，并且选它作为Leader</p>
<p>2.选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader</p>
<p>这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。</p>
<h5 id="2-2-5-选举Leader"><a href="#2-2-5-选举Leader" class="headerlink" title="2.2.5 选举Leader"></a>2.2.5 选举Leader</h5><p>最简单最直观的方案是，所有Follower都在ZooKeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（ZooKeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。</p>
<p>但是该方法会有3个问题：</p>
<p>1.split-brain(脑裂) 这是由ZooKeeper的特性引起的，虽然ZooKeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致</p>
<p>2.herd effect(惊群) 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</p>
<p>3.ZooKeeper负载过重 每个Replica都要为此在ZooKeeper上注册一个Watch，当集群规模增加到几千个Partition时ZooKeeper负载会过重。</p>
<p>Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比ZooKeeper Queue的方式更高效）通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。</p>
<h3 id="4-broker保存消息"><a href="#4-broker保存消息" class="headerlink" title="4. broker保存消息"></a>4. broker保存消息</h3><h4 id="4-1-存储方式"><a href="#4-1-存储方式" class="headerlink" title="4.1 存储方式"></a>4.1 存储方式</h4><p>物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630212816733.png" alt="image-20200630212816733"></p>
<h4 id="4-2-存储策略"><a href="#4-2-存储策略" class="headerlink" title="4.2 存储策略"></a>4.2 存储策略</h4><p>无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：</p>
<blockquote>
<p>1、 基于时间：log.retention.hours=168<br>2、 基于大小：log.retention.bytes=1073741824</p>
</blockquote>
<h2 id="五-Kafka与其他常用MQ对比"><a href="#五-Kafka与其他常用MQ对比" class="headerlink" title="五. Kafka与其他常用MQ对比"></a>五. Kafka与其他常用MQ对比</h2><table>
<thead>
<tr>
<th></th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>Kafka</th>
</tr>
</thead>
<tbody>
<tr>
<td>所属社区/公司</td>
<td>Apache</td>
<td>Mozikka Public License</td>
<td>Apache/LinkendIn</td>
</tr>
<tr>
<td>开发语言</td>
<td>Java</td>
<td>Erlang</td>
<td>Java</td>
</tr>
<tr>
<td>支持协议</td>
<td>OpenWire、STOMP、REST、XMPP、AMQP</td>
<td>AMQP</td>
<td>仿AMQP</td>
</tr>
<tr>
<td>事务</td>
<td>支持</td>
<td>支持（性能会下降）</td>
<td>支持</td>
</tr>
<tr>
<td>集群</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>负载均衡</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>动态扩容</td>
<td>不支持</td>
<td>不支持</td>
<td>支持（zk）</td>
</tr>
<tr>
<td>单机吞吐量TPS</td>
<td>万级</td>
<td>万级</td>
<td>十万级</td>
</tr>
<tr>
<td>顺序消息</td>
<td>不支持</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>消息确认</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>消息回溯</td>
<td>不支持</td>
<td>不支持</td>
<td>支持指定分区offset位置的回溯</td>
</tr>
<tr>
<td>消息重试</td>
<td>不支持</td>
<td>不支持，但是可以利用消息确认机制实现</td>
<td>不支持，但是可以利用kafka支持指定分区offset位置的回溯，可以实现消息重试。</td>
</tr>
<tr>
<td>并发度</td>
<td>高</td>
<td>极高</td>
<td>高</td>
</tr>
</tbody>
</table>
<h2 id="六-SpringBoot整合Kafka"><a href="#六-SpringBoot整合Kafka" class="headerlink" title="六. SpringBoot整合Kafka"></a>六. SpringBoot整合Kafka</h2><h3 id="1-在创建好的gradle工程中引入依赖"><a href="#1-在创建好的gradle工程中引入依赖" class="headerlink" title="1. 在创建好的gradle工程中引入依赖"></a>1. 在创建好的gradle工程中引入依赖</h3><p>Gadle版本: 6.0</p>
<p>JDK: 1.8</p>
<p>IDEA: 2020.1</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dependencies</span> &#123;</span><br><span class="line">	...</span><br><span class="line">    <span class="keyword">compile</span> <span class="keyword">group</span>: <span class="string">'org.springframework.boot'</span>, name: <span class="string">'spring-boot-starter-web'</span>, version:<span class="string">'2.2.0.RELEASE'</span></span><br><span class="line">    <span class="keyword">compile</span> <span class="keyword">group</span>: <span class="string">'org.springframework.kafka'</span>, name: <span class="string">'spring-kafka'</span>, version:<span class="string">'2.3.1.RELEASE'</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-编写配置文件"><a href="#2-编写配置文件" class="headerlink" title="2. 编写配置文件"></a>2. 编写配置文件</h3><p>配置文件有两种， 第一中是使用<code>application.yml</code>文件配置， 第二中是使用SpringBoot Java配置类来配置，两种配置如下</p>
<h4 id="applacation-yml"><a href="#applacation-yml" class="headerlink" title="applacation.yml"></a><code>applacation.yml</code></h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line"><span class="attr">  jackson:</span></span><br><span class="line"><span class="attr">    date-format:</span> <span class="string">yyyy-MM-dd</span> <span class="attr">HH:mm:ss</span>  <span class="comment">#日期序列化格式</span></span><br><span class="line"><span class="attr">  kafka:</span></span><br><span class="line"><span class="attr">    bootstrap-servers:</span> <span class="number">172.22</span><span class="number">.24</span><span class="number">.200</span><span class="string">:9092,</span> <span class="number">172.22</span><span class="number">.24</span><span class="number">.200</span><span class="string">:9093,</span> <span class="number">172.22</span><span class="number">.24</span><span class="number">.200</span><span class="string">:9094</span> <span class="comment"># 集群地址， 任意配一台可用地址即可</span></span><br><span class="line"><span class="attr">    producer:</span> <span class="comment"># 生产者配置</span></span><br><span class="line"><span class="attr">      retries:</span> <span class="number">0</span> <span class="comment"># 重试次数</span></span><br><span class="line"><span class="attr">      batch-size:</span> <span class="number">16384</span> <span class="comment"># 一次最多发送数据量</span></span><br><span class="line"><span class="attr">      buffer-memory:</span> <span class="number">33554432</span> <span class="comment"># 32M批处理缓冲区</span></span><br><span class="line"><span class="attr">      key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span> <span class="comment"># 序列化</span></span><br><span class="line"><span class="attr">      value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="attr">      linger:</span> <span class="comment"># 发送延迟</span></span><br><span class="line"><span class="attr">        ms:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">      acks:</span> <span class="string">"1"</span> <span class="comment"># 消息确认</span></span><br><span class="line"><span class="attr">    consumer:</span> <span class="comment"># 消费者配置</span></span><br><span class="line"><span class="attr">      group-id:</span> <span class="number">0</span> <span class="comment"># group-id</span></span><br><span class="line"><span class="attr">      enable-auto-commit:</span> <span class="literal">false</span> <span class="comment"># 是否开启自动提交</span></span><br><span class="line"><span class="attr">      auto-commit-interval:</span> <span class="number">100</span> <span class="comment"># consumer自动向zookeeper提交offset的频率</span></span><br><span class="line"><span class="attr">      properties:</span> <span class="comment"># 消费超时时间，大小不能超过session.timeout.ms，默认：3000</span></span><br><span class="line"><span class="attr">        session:</span></span><br><span class="line"><span class="attr">          timeout:</span></span><br><span class="line"><span class="attr">            ms:</span> <span class="number">15000</span></span><br><span class="line"><span class="attr">      key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span> <span class="comment"># 反序列化</span></span><br><span class="line"><span class="attr">      value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span> </span><br><span class="line"><span class="attr">      fetch-max-wait:</span> <span class="number">300000</span> <span class="comment"># 配置consumer最多等待response多久</span></span><br><span class="line"><span class="attr">      max-poll-records:</span> <span class="number">50</span> <span class="comment"># max.poll.records条数据需要在session.timeout.ms这个时间内处理完</span></span><br></pre></td></tr></table></figure>
<h4 id="KafkaConfig-java"><a href="#KafkaConfig-java" class="headerlink" title="KafkaConfig.java"></a><code>KafkaConfig.java</code></h4><p>注意: 配置写在yml和java类中均可, 但是某些工厂类如<code>KafkaTemplate</code>则需要在代码中使用<code>@Bean</code>注解交给<code>Spring</code>创建</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.config;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.EnableKafka;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.KafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.*;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.listener.ContainerProperties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Classname</span> KafkaConfig</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span></span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2020-06-29 17:12</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@EnableKafka</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.kafka.bootstrap-servers&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String hosts;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KafkaConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"kafka config init  -------------&gt;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * producer configuration</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> Map&lt;String, Object&gt; producerConfigs</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">producerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);</span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">0</span>);</span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1000</span>);</span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"1"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerFactory&lt;String, String&gt; <span class="title">producerFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * consumer configuration</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> Map&lt;String, Object&gt; consumerConfigs</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">consumerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">10</span>);</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"0"</span>);</span><br><span class="line">        <span class="comment">//自动控制提交offset</span></span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//提交延迟毫秒数</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">100</span>);</span><br><span class="line">        <span class="comment">//执行超时时间</span></span><br><span class="line">        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="string">"15000"</span>);</span><br><span class="line">        <span class="comment">// 每间隔max.poll.interval.ms我们就调用一次poll</span></span><br><span class="line">        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, <span class="string">"300000"</span>);</span><br><span class="line">        <span class="comment">// 一次poll最多返回的记录数</span></span><br><span class="line">        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, <span class="string">"50"</span>);</span><br><span class="line">        <span class="comment">//开始消费位置 earliest/latest/none</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"latest"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerFactory&lt;String, String&gt; <span class="title">consumerFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * kafka template configuration</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> KafkaTemplate&lt;String, String&gt; <span class="title">kafkaTemplate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> KafkaTemplate&lt;&gt;(producerFactory());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批量消费</span></span><br><span class="line"><span class="comment">     * MANUAL   当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> KafkaListenerContainerFactory&lt;?&gt; batchFactory(ConsumerFactory consumerFactory) &#123;</span><br><span class="line">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =</span><br><span class="line">                <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">        factory.setConsumerFactory(consumerFactory);</span><br><span class="line">        <span class="comment">// topic有5个分区，为了加快消费将并发设置为5，也就是有5个KafkaMessageListenerContainer</span></span><br><span class="line">        factory.setConcurrency(<span class="number">5</span>);</span><br><span class="line">        <span class="comment">// 设置拉取时间</span></span><br><span class="line">        factory.getContainerProperties().setPollTimeout(<span class="number">1500</span>);</span><br><span class="line">        <span class="comment">// 开启批量消费</span></span><br><span class="line">        factory.setBatchListener(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//配置手动提交offset</span></span><br><span class="line">  </span><br><span class="line">        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);</span><br><span class="line">        <span class="keyword">return</span> factory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="KafkaAdminConfig"><a href="#KafkaAdminConfig" class="headerlink" title="KafkaAdminConfig"></a><code>KafkaAdminConfig</code></h4><p>该文件可用来创建<code>topic</code>的相关操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.config;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.constant.MyTopic;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.AdminClientConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.NewTopic;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.EnableKafka;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaAdmin;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka admin config</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@EnableKafka</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaAdminConfig</span> </span>&#123;</span><br><span class="line">    <span class="comment">// yml配置文件中的变量</span></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.kafka.bootstrap-servers&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String hosts;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> KafkaAdmin <span class="title">admin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; configs = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">1</span>);</span><br><span class="line">        configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> KafkaAdmin(configs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic1</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">// 第一个是参数是topic名字，第二个参数是分区个数</span></span><br><span class="line">        <span class="comment">// 第三个是topic的复制因子个数</span></span><br><span class="line">        <span class="comment">// -----------------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;当broker个数为1个时会创建topic失败，</span></span><br><span class="line">        <span class="comment">//提示：replication factor: 2 larger than available brokers: 1</span></span><br><span class="line">        <span class="comment">//只有在集群中才能使用kafka的备份功能</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC1, <span class="number">5</span>, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic2</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC2, <span class="number">5</span>, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic3</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC3, <span class="number">5</span>, (<span class="keyword">short</span>) <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic4</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC4, <span class="number">3</span>, (<span class="keyword">short</span>) <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-编写Kafka通用工具类"><a href="#3-编写Kafka通用工具类" class="headerlink" title="3. 编写Kafka通用工具类"></a>3. 编写Kafka通用工具类</h3><ol>
<li>可以先创建一个工具类接口, 然后再去实现这个接口</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javafx.util.Pair;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.TopicListing;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.springframework.util.concurrent.ListenableFuture;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020/6/20 0020</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Time</span>: 11:37</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: KafkaService</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg      数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Boolean <span class="title">sendDataToTopic</span><span class="params">(String topicName, String msg)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic和key中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAndKey(String topicName, String key, String msg);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic的中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic     topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition 分区名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       指定的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAppointPartition(String topic, Integer partition, String key, String msg);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 校验topic是否已经存在于kafka中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否存在的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Boolean <span class="title">isExistTopic</span><span class="params">(String topicName)</span></span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建指定的topic</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName         topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicPartition    话题创建的分区</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> replicationFactor 话题创建的副本， 不能大于broker的数量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否创建成功</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Boolean <span class="title">createTopic</span><span class="params">(String topicName, Integer topicPartition, <span class="keyword">short</span> replicationFactor)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除话题</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicNames 话题名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 删除结果</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    List&lt;Pair&lt;String, Boolean&gt;&gt; deleteTopic(String[] topicNames);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取所有的topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> topic集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Map&lt;String, TopicListing&gt; <span class="title">getTopics</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>实现类</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.service.impl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.config.KafkaConfig;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.service.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> javafx.util.Pair;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.KafkaFuture;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaAdmin;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"><span class="keyword">import</span> org.springframework.util.concurrent.ListenableFuture;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020/6/20 0020</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Time</span>: 20:20</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Kafka封装操作类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtilsImpl</span> <span class="keyword">implements</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaConfig kafkaConfig;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaAdmin kafkaAdmin;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTimeout</span><span class="params">(<span class="keyword">int</span> timeout)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.timeout = timeout;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> timeout = <span class="number">6000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">sendDataToTopic</span><span class="params">(String topicName, String msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; result = kafkaTemplate.send(topicName, msg);</span><br><span class="line">            result.get();</span><br><span class="line">            <span class="keyword">return</span> !result.completable().isCompletedExceptionally();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.info(<span class="string">"发送普通消息失败, topic=&#123;&#125;, msg=&#123;&#125;, failure Message=&#123;&#125;"</span>, topicName, msg, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic和key中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAndKey(String topicName, String key, String msg) &#123;</span><br><span class="line">        <span class="keyword">return</span> kafkaTemplate.send(topicName, key, msg);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic的中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic     topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition 分区名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       指定的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAppointPartition(String topic, Integer partition, String key, String msg) &#123;</span><br><span class="line">        <span class="keyword">return</span> kafkaTemplate.send(topic, partition, key, msg);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 校验topic是否已经存在于kafka中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否存在的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">isExistTopic</span><span class="params">(String topicName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            ListTopicsOptions listTopicsOptions = <span class="keyword">new</span> ListTopicsOptions();</span><br><span class="line">            listTopicsOptions.listInternal(<span class="keyword">true</span>);</span><br><span class="line">            ListTopicsResult listTopicsResult = adminClient.listTopics(listTopicsOptions);</span><br><span class="line">            Boolean flag = listTopicsResult.names().get().contains(<span class="string">"topicName"</span>);</span><br><span class="line">            <span class="keyword">return</span> flag;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.info(<span class="string">"校验topic: &#123;&#125; 是否已经存在于kafka中异常 &#123;&#125;"</span>, topicName, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建指定的topic</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName         topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicPartition    话题创建的分区</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> replicationFactor 话题创建的副本， 不能大于broker的数量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否创建成功</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">createTopic</span><span class="params">(String topicName, Integer topicPartition, <span class="keyword">short</span> replicationFactor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            Boolean existTopic = isExistTopic(topicName);</span><br><span class="line">            <span class="keyword">if</span> (existTopic) &#123;</span><br><span class="line">                <span class="keyword">return</span> existTopic;</span><br><span class="line">            &#125;</span><br><span class="line">            NewTopic newTopic = <span class="keyword">new</span> NewTopic(topicName, topicPartition, replicationFactor);</span><br><span class="line">            List&lt;NewTopic&gt; newTopics = Collections.singletonList(newTopic);</span><br><span class="line">            adminClient.createTopics(newTopics);</span><br><span class="line">            <span class="keyword">return</span> isExistTopic(topicName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"创建话题&#123;&#125;失败, Cause by: &#123;&#125;"</span>, topicName, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定topic(如果broker那没有设置允许删除topic的话，此调用会持续等待最终超时返回)</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicNames 待删除的topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 删除是否成功</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Pair&lt;String, Boolean&gt;&gt; deleteTopic(String[] topicNames) &#123;</span><br><span class="line">        List&lt;Pair&lt;String, Boolean&gt;&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            DeleteTopicsOptions options = <span class="keyword">new</span> DeleteTopicsOptions();</span><br><span class="line">            options.timeoutMs(timeout);</span><br><span class="line">            DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(Arrays.asList(topicNames), options);</span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;String, KafkaFuture&lt;Void&gt;&gt; e : deleteTopicsResult.values().entrySet()) &#123;</span><br><span class="line">                String topic = e.getKey();</span><br><span class="line">                KafkaFuture&lt;Void&gt; future = e.getValue();</span><br><span class="line">                future.get();</span><br><span class="line">                result.add(<span class="keyword">new</span> Pair&lt;&gt;(topic, !future.isCompletedExceptionally()));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"删除话题&#123;&#125;失败, Cause by: &#123;&#125;"</span>, String.join(<span class="string">","</span>, topicNames), e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取所有的topic</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> topic集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, TopicListing&gt; <span class="title">getTopics</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ListTopicsOptions options = <span class="keyword">new</span> ListTopicsOptions();</span><br><span class="line">        <span class="comment">//设置超时时间</span></span><br><span class="line">        options.timeoutMs(timeout);</span><br><span class="line">        <span class="comment">//不列出kafka内部topic</span></span><br><span class="line">        options.listInternal(<span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            ListTopicsResult listTopicsResult = adminClient.listTopics(options);</span><br><span class="line">            <span class="keyword">return</span> listTopicsResult.namesToListings().get();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"查询话题失败失败, Cause by: &#123;&#125;"</span>, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注: @Slf4j注解是集成了lombok后可以方便打印日志使用的</p>
<h3 id="4-编写Kafka-producer"><a href="#4-编写Kafka-producer" class="headerlink" title="4. 编写Kafka producer"></a>4. 编写Kafka producer</h3><p>通用工具类编写好后, 可以独立出专门用来发送消息的<code>producer</code>类, 代码如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.constant.MyTopic;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.entity.KafkaMessage;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.service.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"><span class="keyword">import</span> org.springframework.util.concurrent.ListenableFuture;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Objects;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CompletableFuture;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Classname</span> KafkaProducer</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> 生产者</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2020-06-30 11:00</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducer</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaUtils kafkaUtils;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, String key, KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">        String msg = JSON.toJSONString(kafkaMessage);</span><br><span class="line">        kafkaUtils.sendDataToTopicAndKey(topic, key, msg).addCallback(success -&gt; &#123;</span><br><span class="line">            <span class="comment">// 消息发送到的topic</span></span><br><span class="line">            String successTopic = Objects.requireNonNull(success).getRecordMetadata().topic();</span><br><span class="line">            <span class="comment">// 消息发送到的分区</span></span><br><span class="line">            <span class="keyword">int</span> partition = success.getRecordMetadata().partition();</span><br><span class="line">            <span class="comment">// 消息在分区内的offset</span></span><br><span class="line">            <span class="keyword">long</span> offset = success.getRecordMetadata().offset();</span><br><span class="line">            log.info(<span class="string">"发送普通消息, topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, msg);</span><br><span class="line">        &#125;, failure -&gt; &#123;</span><br><span class="line">            log.info(<span class="string">"发送普通消息失败, topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;, failure Message=&#123;&#125;"</span>, topic, key, msg, failure.getMessage());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">send</span><span class="params">(String topic, Integer partition, String key, KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 或者 JSON.toJSONString(kafkaMessage, SerializerFeature.WriteDateUseDateFormat);</span></span><br><span class="line">        String msg = JSON.toJSONStringWithDateFormat(kafkaMessage, <span class="string">"yyyy-MM-dd HH:mm:ss"</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendResultListenableFuture = kafkaUtils.sendDataToTopicAppointPartition(topic, partition, key, msg);</span><br><span class="line">            log.info(<span class="string">"发送普通消息，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, msg);</span><br><span class="line">            CompletableFuture&lt;SendResult&lt;String, String&gt;&gt; completable = sendResultListenableFuture.completable();</span><br><span class="line">            completable.get();</span><br><span class="line">            <span class="keyword">return</span> !completable.isCompletedExceptionally();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"发送普通消息失败，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, msg);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, Integer partition, String key, String kafkaMessage)</span> </span>&#123;</span><br><span class="line">        kafkaUtils.sendDataToTopicAppointPartition(topic, partition, key, kafkaMessage);</span><br><span class="line">        log.info(<span class="string">"发送普通消息，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, kafkaMessage);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>首先将<code>KafkaUtils</code>自动注入。</li>
<li>然后就可以使用<code>KafkaUtils</code>中提供的<code>API</code>按照自己的需求进行二次封装，实现自己想要的逻辑处理。</li>
<li>发送消息后，可以通过其<code>addCallback</code>方法来处理发送成功或者失败后的逻辑，或者接收<code>ListenableFuture</code>类型的返回值并且使用<code>try-catch</code>来作逻辑判断，上述两种方式在代码中均有体现。</li>
</ol>
<h3 id="5-编写Kafka-consumer"><a href="#5-编写Kafka-consumer" class="headerlink" title="5. 编写Kafka consumer"></a>5. 编写Kafka consumer</h3><p><code>consumer</code>主要利用<code>SpringBoot</code> 提供的<code>@KafkaListener</code>注解来实现的。下面先来简单介绍一下<code>@KafkaListener</code>注解的相关内容：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="meta">@Target</span>(&#123; ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE &#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@MessageMapping</span></span><br><span class="line"><span class="meta">@Documented</span></span><br><span class="line"><span class="meta">@Repeatable</span>(KafkaListeners.class)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> KafkaListener &#123;</span><br><span class="line">	<span class="comment">// 消费者的id（唯一），当GroupId没有被配置的时候，默认id为GroupId，支持SpEL表达式#&#123;&#125;</span></span><br><span class="line">	<span class="function">String <span class="title">id</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">	<span class="comment">// 这里面配置的是监听容器工厂BeanName，常用于批量消费时指定消费工厂</span></span><br><span class="line">	<span class="function">String <span class="title">containerFactory</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">	<span class="comment">// 需要监听的Topic，可监听多个</span></span><br><span class="line">	String[] topics() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line">	<span class="comment">// 可配置更加详细的监听信息，必须监听某个Topic中的指定分区，或者从offset为200的偏移量开始监听</span></span><br><span class="line">	TopicPartition[] topicPartitions() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line">	<span class="comment">// 监听异常处理器，配置BeanName</span></span><br><span class="line">	<span class="function">String <span class="title">errorHandler</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">	<span class="comment">// 消费组ID</span></span><br><span class="line">	<span class="function">String <span class="title">groupId</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">	<span class="comment">// id是否为GroupId</span></span><br><span class="line">	<span class="function"><span class="keyword">boolean</span> <span class="title">idIsGroup</span><span class="params">()</span> <span class="keyword">default</span> <span class="keyword">true</span></span>;</span><br><span class="line">	<span class="comment">// 消费者Id前缀</span></span><br><span class="line">	<span class="function">String <span class="title">clientIdPrefix</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">	<span class="comment">// 真实监听容器的BeanName，需要在 BeanName前加 "__"</span></span><br><span class="line">	<span class="function">String <span class="title">beanRef</span><span class="params">()</span> <span class="keyword">default</span> "__listener"</span>;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实现<code>consumer</code>类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.consumer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.PartitionOffset;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.Acknowledgment;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Optional;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Classname</span> KafkaConsumer</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> 消费者</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019-11-06 17:01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）;</span></span><br><span class="line"><span class="comment">     * （1）使用一个全新的"group.id"（就是之前没有被任何消费者使用过）;</span></span><br><span class="line"><span class="comment">     * （2）指定"auto.offset.reset"参数的值为earliest;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> content 消息内容</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-1"</span>, topics = <span class="string">"topic4"</span>, groupId = <span class="string">"group2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage2</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"消费者topic4-1监听消息,消息内容=[&#123;&#125;]"</span>, content);s	</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-2"</span>, topics = <span class="string">"topic4"</span>, groupId = <span class="string">"group2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage3</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"消费者topic4-2监听消息,消息内容=[&#123;&#125;]"</span>, content);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批量消费</span></span><br><span class="line"><span class="comment">     * containerFactory: 需要声明消费工厂名</span></span><br><span class="line"><span class="comment">     * batchFactory： 在KafakaConfig中配置的消费者工厂类</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ack 消息确认对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> records 消息内容</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-3"</span>, topics = <span class="string">"topic3"</span>, groupId = <span class="string">"group1"</span>,</span><br><span class="line">            containerFactory = <span class="string">"batchFactory"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"client-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;"</span>, Thread.currentThread().getId(), records.size());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">                Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value());</span><br><span class="line">                <span class="keyword">if</span> (kafkaMessage.isPresent()) &#123;</span><br><span class="line">                    Object message = record.value();</span><br><span class="line">                    String topic = record.topic();</span><br><span class="line">                    <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">                    log.info(<span class="string">"client-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]"</span>, topic, offset, message);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 手动提交，设置offset</span></span><br><span class="line">            ack.acknowledge();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"client-3监听异常&#123;&#125;"</span>, e.getMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * id是消费者监听容器</span></span><br><span class="line"><span class="comment">     * 配置topic和分区：监听两个topic，分别为topic1、topic2，topic1只接收分区0，3的消息，</span></span><br><span class="line"><span class="comment">     * topic2接收分区0和分区1的消息，但是分区1的消费者初始位置为5</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> record 消费内容</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-4"</span>, clientIdPrefix = <span class="string">"my"</span>,</span><br><span class="line">            topicPartitions =</span><br><span class="line">                    &#123;<span class="meta">@TopicPartition</span>(topic = <span class="string">"topic1"</span>, partitions = &#123;<span class="string">"0"</span>, <span class="string">"3"</span>&#125;),</span><br><span class="line">                            <span class="meta">@TopicPartition</span>(topic = <span class="string">"topic2"</span>, partitions = <span class="string">"0"</span>,</span><br><span class="line">                                    partitionOffsets = <span class="meta">@PartitionOffset</span>(partition = <span class="string">"1"</span>, initialOffset = <span class="string">"4"</span>))</span><br><span class="line">                    &#125;)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"topic1消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-5"</span>, topics = &#123;<span class="string">"topic1"</span>, <span class="string">"topic2"</span>&#125;)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen2</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"topic1,topic2 多主题消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此，我们的生产者和消费者就已经都编写好了，至于要简单的编写一下单元测试或者<code>Controller</code>实现<code>RESTful API</code>就可以开始验证和简单的使用<code>Kafka</code>了。</p>
<h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">30</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">20.332</span>  INFO <span class="number">20320</span> --- [  topic-<span class="number">3</span>-<span class="number">2</span>-C-<span class="number">1</span>] o.s.k.l.KafkaMessageListenerContainer    : group1: partitions assigned: [topic3-<span class="number">4</span>]</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">30</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">20.336</span>  INFO <span class="number">20320</span> --- [  topic-<span class="number">3</span>-<span class="number">1</span>-C-<span class="number">1</span>] o.s.k.l.KafkaMessageListenerContainer    : group1: partitions assigned: [topic3-<span class="number">3</span>, topic3-<span class="number">2</span>]</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">30</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">20.336</span>  INFO <span class="number">20320</span> --- [  topic-<span class="number">3</span>-<span class="number">0</span>-C-<span class="number">1</span>] o.s.k.l.KafkaMessageListenerContainer    : group1: partitions assigned: [topic3-<span class="number">1</span>, topic3-<span class="number">0</span>]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>在消费工厂或者配置中设置并发量，小于或等于Topic的分区数<br><code>factory.setConcurrency(3);</code></p>
<p>我们设置concurrency为3，也就是将会启动3条线程进行监听，我们创建的topic则是有5个partition，意味着将有2条线程分配到2个partition和1条线程分配到1个partition。我们可以看到这段日志的最后3行，这就是每条线程分配到的partition.<br>注意：设置的并发量不能大于partition的数量，如果需要提高吞吐量，可以通过增加partition的数量达到快速提升吞吐量的效果。</p>
<h4 id="1-简单发送-订阅"><a href="#1-简单发送-订阅" class="headerlink" title="1. 简单发送-订阅"></a>1. 简单发送-订阅</h4><p>发送：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送数据到指定的topic的中</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic     topic名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partition 分区名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> key       指定的key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, String key, KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">    String msg = JSON.toJSONString(kafkaMessage);</span><br><span class="line">    kafkaUtils.sendDataToTopicAndKey(topic, key, msg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>订阅：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）;</span></span><br><span class="line"><span class="comment"> * （1）使用一个全新的"group.id"（就是之前没有被任何消费者使用过）;</span></span><br><span class="line"><span class="comment"> * （2）指定"auto.offset.reset"参数的值为earliest;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> content 消息内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic4-1"</span>, topics = <span class="string">"topic4"</span>, groupId = <span class="string">"group2"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage2</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"消费者topic4-1监听消息,消息内容=[&#123;&#125;]"</span>, content);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>控制台结果：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630161218434.png" alt="image-20200630161218434"></p>
<h4 id="2-批量消费"><a href="#2-批量消费" class="headerlink" title="2. 批量消费"></a>2. 批量消费</h4><ol>
<li><p>重新创建一份新的消费者配置，配置为一次拉取5条消息</p>
</li>
<li><p>创建一个监听容器工厂，设置其为批量消费并设置并发量为5，这个并发量根据分区数决定，必须小于等于分区数，否则会有线程一直处于空闲状态</p>
</li>
<li><p>创建一个分区数为5的Topic</p>
</li>
<li><p>创建监听方法，设置消费<code>id</code>为<code>batch</code>，<code>clientID</code>前缀为<code>batch</code>，监听<code>topic3</code>，使用<code>batchFactory</code>工厂创建该监听容器</p>
</li>
</ol>
<p>该方法在<code>KafkaConfig.java</code>中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; <span class="title">batchFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =</span><br><span class="line">            <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">    factory.setConsumerFactory(<span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()));</span><br><span class="line">    <span class="comment">//设置并发量，小于或等于Topic的分区数</span></span><br><span class="line">    factory.setConcurrency(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 开启批量消费</span></span><br><span class="line">    factory.setBatchListener(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> factory;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>生产者代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@ApiOperation</span>(value = <span class="string">"向Topic3中发送消息"</span>, notes=<span class="string">"向Topic3中发送消息"</span>)</span><br><span class="line"><span class="meta">@PostMapping</span>(path = <span class="string">"/topic3"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> ResponseEntity&lt;Boolean&gt; <span class="title">sendTopic3</span><span class="params">(@RequestBody KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        flag = kafkaProducer.send(MyTopic.TOPIC3, <span class="number">2</span>, <span class="string">"topic.*"</span>, kafkaMessage);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ResponseEntity.ok(flag);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>消费者代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * containerFactory: 需要声明消费工厂名</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> records 消息内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic-3"</span>, topics = <span class="string">"topic3"</span>, groupId = <span class="string">"group1"</span>,</span><br><span class="line">        containerFactory = <span class="string">"batchFactory"</span>, clientIdPrefix = <span class="string">"batch"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;"</span>, Thread.currentThread().getId(), records.size());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">            Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value());</span><br><span class="line">            <span class="keyword">if</span> (kafkaMessage.isPresent()) &#123;</span><br><span class="line">                Object message = record.value();</span><br><span class="line">                String topic = record.topic();</span><br><span class="line">                <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">                log.info(<span class="string">"topic-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]"</span>, topic, offset, message);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">"topic-3监听异常&#123;&#125;"</span>, e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>控制台结果：max.poll.records设置为5（一次poll最多返回的记录数）</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630165743413.png" alt="image-20200630165743413"></p>
<h4 id="3-确认机制"><a href="#3-确认机制" class="headerlink" title="3. 确认机制"></a>3. 确认机制</h4><p>与<a href="">2. 批量消费</a>的代码相似，使用Kafka的Ack机制比较简单，只需简单的三步即可：</p>
<ol>
<li>设置ENABLE_AUTO_COMMIT_CONFIG=false，禁止自动提交</li>
<li>设置AckMode=MANUAL_IMMEDIATE</li>
<li>监听方法加入Acknowledgment ack 参数</li>
</ol>
<p>Kafka是通过最新保存偏移量进行消息消费的，而且确认消费的消息并不会立刻删除，所以我们可以重复的消费未被删除的数据，当第一条消息未被确认，而第二条消息被确认的时候，Kafka会保存第二条消息的偏移量，也就是说第一条消息再也不会被监听器所获取，除非是根据第一条消息的偏移量手动获取。</p>
<p>拒绝消息只要在监听方法中不调用ack.acknowledge()即可</p>
<p>配置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; <span class="title">batchFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =</span><br><span class="line">            <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">    factory.setConsumerFactory(<span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()));</span><br><span class="line">    <span class="comment">//设置并发量，小于或等于Topic的分区数</span></span><br><span class="line">    factory.setConcurrency(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 开启批量消费</span></span><br><span class="line">    factory.setBatchListener(<span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//配置手动提交offset</span></span><br><span class="line">	factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);</span><br><span class="line">    <span class="keyword">return</span> factory;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * consumer configuration</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Map&lt;String, Object&gt; consumerConfigs</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">consumerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">10</span>);</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//自动控制提交offset,注意此处设置自动提交为false的意思时offset从由kafka自动提交转为由Spring自动提交了，实现真正的手动提交还需要在消费工厂类中配合factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);或者kafka-listener-ack-mode: manual 参数配置， 才能实现真正的手动提交</span></span><br><span class="line">    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> props;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>消费者代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * containerFactory: 需要声明消费工厂名</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> records 消息内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic-3"</span>, topics = <span class="string">"topic3"</span>, groupId = <span class="string">"group1"</span>,</span><br><span class="line">        containerFactory = <span class="string">"batchFactory"</span>, clientIdPrefix = <span class="string">"batch"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;"</span>, Thread.currentThread().getId(), records.size());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">            Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value());</span><br><span class="line">            <span class="keyword">if</span> (kafkaMessage.isPresent()) &#123;</span><br><span class="line">                Object message = record.value();</span><br><span class="line">                String topic = record.topic();</span><br><span class="line">                <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">                log.info(<span class="string">"topic-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]"</span>, topic, offset, message);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 手动提交，设置offset, 确认消息被消费</span></span><br><span class="line">        ack.acknowledge();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">"topic-3监听异常&#123;&#125;"</span>, e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编写测试方法，运行后可以方法监听方法能收到消息，紧接着注释ack.acknowledge()方法，重新测试，同样你会发现监听容器能接收到消息，这个时候如果你重启项目还是可以看到未被确认的那几条消息。</p>
<h4 id="4-多主题订阅"><a href="#4-多主题订阅" class="headerlink" title="4. 多主题订阅"></a>4. 多主题订阅</h4><p>消费者：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic1-2"</span>, topics = &#123;<span class="string">"topic1"</span>, <span class="string">"topic2"</span>&#125;)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen2</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic1, topic2 多主题消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174523010.png" alt="image-20200630174523010"></p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174450976.png" alt="image-20200630174450976"></p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174734251.png" alt="image-20200630174734251"></p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174708339.png" alt="image-20200630174708339"></p>
<h4 id="5-多主题指定分区指定偏移量订阅"><a href="#5-多主题指定分区指定偏移量订阅" class="headerlink" title="5. 多主题指定分区指定偏移量订阅"></a>5. 多主题指定分区指定偏移量订阅</h4><p>消费者：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * id是消费者监听容器</span></span><br><span class="line"><span class="comment"> * 配置topic和分区：监听两个topic，分别为topic1、topic2，topic1只接收分区0，3的消息，</span></span><br><span class="line"><span class="comment"> * topic2接收分区0和分区1的消息，但是分区1的消费者初始位置为5</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> record 消费内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic1-topic2"</span>, clientIdPrefix = <span class="string">"my"</span>,</span><br><span class="line">        topicPartitions =</span><br><span class="line">                &#123;</span><br><span class="line">                        <span class="meta">@TopicPartition</span>(topic = <span class="string">"topic1"</span>, partitions = &#123;<span class="string">"0"</span>, <span class="string">"3"</span>&#125;),</span><br><span class="line">                        <span class="meta">@TopicPartition</span>(topic = <span class="string">"topic2"</span>, partitions = <span class="string">"0"</span>,</span><br><span class="line">                                partitionOffsets = <span class="meta">@PartitionOffset</span>(partition = <span class="string">"1"</span>, initialOffset = <span class="string">"4"</span>))</span><br><span class="line">                &#125;)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic1-topic2消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>控制台：</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630185143846.png" alt="image-20200630185143846"></p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630185123707.png" alt="image-20200630185123707"></p>
<p>向topic1的分区2中发送消息，<code>topic1-topic2</code>未监听到，因为我们只监听了topic1的 0 3分区，topic2的 0 1分区。</p>
<p>自定义<code>offset</code>：<code>auto.offset.reset=&quot;earliest&quot;</code>时会从设置的<code>initialOffset</code>开始消费</p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630190157507.png" alt="image-20200630190157507"></p>
<p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630191425762.png" alt="image-20200630191425762"></p>
<h2 id="七-扩展"><a href="#七-扩展" class="headerlink" title="七. 扩展"></a>七. 扩展</h2><h3 id="1-Kafka常用命令"><a href="#1-Kafka常用命令" class="headerlink" title="1. Kafka常用命令"></a>1. Kafka常用命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动kafka服务，集群则在不同服务器上各自执行此命令</span></span><br><span class="line">./bin/kafka-server-start.sh config/server.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建主题（4个分区，2个副本）</span></span><br><span class="line">./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看服务器中所有的topic</span></span><br><span class="line">./kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看集群描述</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果说明：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这是输出的解释。第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。由于我们只有一个分 区用于此主题，因此只有一行。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> “leader”是负责给定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。（因为在kafka中 如果有多个副本的话，就会存在leader和follower的关系，表示当前这个副本为leader所在的broker是哪一个） </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> “replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。（所有副本列表0,1,2） </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> “isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。（可用的列表数）</span></span><br><span class="line">./kafka-topics.sh --describe --zookeeper localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看某个topic详情</span></span><br><span class="line">./kafka-topics.sh --topic &lt;topic-name&gt; --describe --zookeeper localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除某个topic</span></span><br><span class="line">./kafka-topics.sh --delete --zookeeper localhost:2181 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示某个消费组的消费详情（0.10.1.0版本+）</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group &lt;group-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 新消费者列表查询（支持0.10版本+）</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动生产者 发送消息</span></span><br><span class="line">./kafka-console-producer.sh --broker-list localhost:9092 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动消费者 接收消息</span></span><br><span class="line"> ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 高级点的用法</span></span><br><span class="line">./kafka-simple-consumer-shell.sh --brist localhost:9092 --topic test --partition 0 --offset 1234  --max-messages 10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看消费组详情</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group &lt;group-name&gt; --describe</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询topic的最大（小）offset 最大：--time -1 最小： --time -2</span></span><br><span class="line">./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic &lt;topic-name&gt; --time -1</span><br></pre></td></tr></table></figure>
<p>zookeeper:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 连接 zookeeper</span></span><br><span class="line">./zookeeper-shell.sh 127.0.0.1:2181</span><br><span class="line"></span><br><span class="line">ls /</span><br><span class="line">[zookeeper, brokers]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看broker的id</span></span><br><span class="line">ls /brokers/ids</span><br><span class="line">[3, 2, 0]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看消息</span></span><br><span class="line">ls /brokers/topics</span><br><span class="line">[new, __consumer_offsets, test]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看删除的消息</span></span><br><span class="line">ls /admin/delete_topics</span><br><span class="line">[account]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看broker的信息</span></span><br><span class="line">get /brokers/ids/0</span><br><span class="line">&#123;"jmx_port":-1,"timestamp":"1516184048700","endpoints":["PLAINTEXT://hs01:9092"],"host":"hs01","version":2,"port":9092&#125;</span><br><span class="line">cZxid = 0x29000048b2</span><br><span class="line">ctime = Wed Jan 17 18:14:08 CST 2018</span><br><span class="line">mZxid = 0x29000048b2</span><br><span class="line">mtime = Wed Jan 17 18:14:08 CST 2018</span><br><span class="line">pZxid = 0x29000048b2</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x261033b8886032a</span><br><span class="line">dataLength = 119</span><br><span class="line">numChildren = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删除消息</span></span><br><span class="line">rmr /brokers/topics/test</span><br><span class="line">rmr /admin/delete_topics/test</span><br></pre></td></tr></table></figure>
<h3 id="2-Kafka管理工具"><a href="#2-Kafka管理工具" class="headerlink" title="2. Kafka管理工具"></a>2. Kafka管理工具</h3><ul>
<li><p>KafkaOffsetMonitor：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。</p>
</li>
<li><p>Kafka Web Console：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。</p>
</li>
<li><p>Kafka Manager：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。</p>
</li>
</ul>
<h3 id="3-Spring-Boot-Kafka参数配置"><a href="#3-Spring-Boot-Kafka参数配置" class="headerlink" title="3. Spring Boot Kafka参数配置"></a>3. Spring Boot Kafka参数配置</h3><ul>
<li><p>生产者</p>
<ul>
<li>重要配置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 高优先级配置</span><br><span class="line"># 以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接</span><br><span class="line">spring.kafka.producer.bootstrap-servers=TopKafka1:9092,TopKafka2:9092,TopKafka3:9092</span><br><span class="line"> </span><br><span class="line"># 设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。</span><br><span class="line">spring.kafka.producer.retries=0</span><br><span class="line"> </span><br><span class="line"># 每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求，</span><br><span class="line"># 这有助于提升客户端和服务端之间的性能，此配置控制默认批量大小（以字节为单位），默认值为16384</span><br><span class="line">spring.kafka.producer.batch-size=16384</span><br><span class="line"> </span><br><span class="line"># producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。这项设置将和producer能够使用的总内存相关，但并不是一个硬性的限制，因为不是producer使用的所有内存都是用于缓存。一些额外的内存会用于压缩（如果引入压缩机制），同样还有一些用于维护请求。</span><br><span class="line">spring.kafka.producer.buffer-memory=33554432</span><br><span class="line"> </span><br><span class="line"># key的Serializer类，实现了org.apache.kafka.common.serialization.Serializer接口</span><br><span class="line">spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line"> </span><br><span class="line"># 值的Serializer类，实现了org.apache.kafka.common.serialization.Serializer接口</span><br><span class="line">spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line"> </span><br><span class="line"># procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：</span><br><span class="line"># acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。</span><br><span class="line"># acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。</span><br><span class="line"># acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。</span><br><span class="line"># 可以设置的值为：all, -1, 0, 1</span><br><span class="line">spring.kafka.producer.acks=-1</span><br><span class="line"> </span><br><span class="line"># 当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪</span><br><span class="line">spring.kafka.producer.client-id=1</span><br><span class="line"> </span><br><span class="line"># producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。压缩最好用于批量处理，批量处理消息越多，压缩性能越好</span><br><span class="line">spring.kafka.producer.compression-type=none</span><br></pre></td></tr></table></figure>
<ul>
<li>其他配置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># 中优先级配置</span><br><span class="line"># 以毫秒为单位的时间，是在我们强制更新metadata的时间间隔。即使我们没有看到任何partition leadership改变。默认值：5 * 60 * 1000 = 300000</span><br><span class="line">spring.kafka.producer.properties.metadata.max.age.ms=300000</span><br><span class="line"> </span><br><span class="line"># producer组将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。通常来说，这只有在记录产生速度大于发送速度的时候才能发生。然而，在某些条件下，客户端将希望降低请求的数量，甚至降低到中等负载一下。这项设置将通过增加小的延迟来完成–即，不是立即发送一条记录，producer将会等待给定的延迟时间以允许其他消息记录发送，这些消息记录可以批量处理。这可以认为是TCP种Nagle的算法类似。这项设置设定了批量处理的更高的延迟边界：一旦我们获得某个partition的batch.size，他将会立即发送而不顾这项设置，然而如果我们获得消息字节数比这项设置要小的多，我们需要“linger”特定的时间以获取更多的消息。 这个设置默认为0，即没有延迟。设定linger.ms=5，例如，将会减少请求数目，但是同时会增加5ms的延迟。</span><br><span class="line">spring.kafka.producer.properties.linger.ms=0</span><br><span class="line"> </span><br><span class="line"># 发送数据时的缓存空间大小，默认：128 * 1024 = 131072</span><br><span class="line">spring.kafka.producer.properties.send.buffer.bytes=131072</span><br><span class="line"> </span><br><span class="line"># socket的接收缓存空间大小,当阅读数据时使用，默认：32 * 1024 = 32768</span><br><span class="line">spring.kafka.producer.properties.receive.buffer.bytes=32768</span><br><span class="line"> </span><br><span class="line"># 请求的最大字节数。这也是对最大记录尺寸的有效覆盖。注意：server具有自己对消息记录尺寸的覆盖，这些尺寸和这个设置不同。此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。默认：1 * 1024 * 1024 = 1048576</span><br><span class="line">spring.kafka.producer.properties.max.request.size=1048576</span><br><span class="line"> </span><br><span class="line"># 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连，默认值：50</span><br><span class="line">spring.kafka.producer.properties.reconnect.backoff.ms=50</span><br><span class="line"> </span><br><span class="line"># producer客户端连接一个kafka服务（broker）失败重连的总时间，每次连接失败，重连时间都会指数级增加，每次增加的时间会存在20%的随机抖动，以避免连接风暴。默认：1000</span><br><span class="line"># spring.kafka.producer.properties.reconnect.backoff.max.ms=1000</span><br><span class="line"> </span><br><span class="line"># 控制block的时长,当buffer空间不够或者metadata丢失时产生block，默认：60 * 1000 = 60000</span><br><span class="line">spring.kafka.producer.properties.max.block.ms=60000</span><br><span class="line"> </span><br><span class="line"># 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中，默认：100</span><br><span class="line">spring.kafka.producer.properties.retry.backoff.ms=100</span><br><span class="line"> </span><br><span class="line"># metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口退出后，我们会擦除并重写最老的窗口，默认：30000</span><br><span class="line">spring.kafka.producer.properties.metrics.sample.window.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用于维护metrics的样本数，默认：2</span><br><span class="line">spring.kafka.producer.properties.metrics.num.samples=2</span><br><span class="line"> </span><br><span class="line"># 用于metrics的最高纪录等级。</span><br><span class="line"># spring.kafka.producer.properties.metrics.recording.level=Sensor.RecordingLevel.INFO.toString()</span><br><span class="line"> </span><br><span class="line"># 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计</span><br><span class="line">#spring.kafka.producer.properties.metric.reporters=Collections.emptyList()</span><br><span class="line"> </span><br><span class="line"># kafka可以在一个connection中发送多个请求，叫作一个flight,这样可以减少开销，但是如果产生错误，可能会造成数据的发送顺序改变,默认是5 (修改）</span><br><span class="line">spring.kafka.producer.properties.max.in.flight.requests.per.connection=5</span><br><span class="line"> </span><br><span class="line"># 关闭连接空闲时间，默认：9 * 60 * 1000 = 540000</span><br><span class="line">spring.kafka.producer.properties.connections.max.idle.ms=540000</span><br><span class="line"> </span><br><span class="line"># 分区类，默认：org.apache.kafka.clients.producer.internals.DefaultPartitioner</span><br><span class="line">spring.kafka.producer.properties.partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner</span><br><span class="line"> </span><br><span class="line"># 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认：30 * 1000 = 30000</span><br><span class="line">spring.kafka.producer.properties.request.timeout.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用户自定义interceptor。</span><br><span class="line">#spring.kafka.producer.properties.interceptor.classes=none</span><br><span class="line"> </span><br><span class="line"># 是否使用幂等性。如果设置为true，表示producer将确保每一条消息都恰好有一份备份；如果设置为false，则表示producer因发送数据到broker失败重试使，可能往数据流中写入多分重试的消息。</span><br><span class="line">#spring.kafka.producer.properties.enable.idempotence=false</span><br><span class="line"> </span><br><span class="line"># 在主动中止正在进行的事务之前，事务协调器将等待生产者的事务状态更新的最长时间（以ms为单位）。</span><br><span class="line">#spring.kafka.producer.properties.transaction.timeout.ms=60000</span><br><span class="line"> </span><br><span class="line"># 用于事务传递的TransactionalId。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的TransactionalId的事务已经完成。 如果没有提供TransactionalId，则生产者被限制为幂等传递。请注意，如果配置了TransactionalId，则必须启用enable.idempotence。默认值为空，这意味着无法使用事务。</span><br><span class="line">#spring.kafka.producer.properties.transactional.id=null</span><br><span class="line"></span><br><span class="line"># 连接风暴</span><br><span class="line"></span><br><span class="line">#应用启动的时候，经常可能发生各应用服务器的连接数异常飙升的情况。假设连接数的设置为：min值3,max值10，正常的业务使用连接数在5个左右，当重启应用时，各应用连接数可能会飙升到10个，瞬间甚至还有可能部分应用会报取不到连接。启动完成后接下来的时间内，连接开始慢慢返回到业务的正常值。这就是所谓的连接风暴。</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者</p>
<ul>
<li>重要配置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># 以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接</span><br><span class="line">spring.kafka.consumer.bootstrap-servers=TopKafka1:9092,TopKafka2:9092,TopKafka3:9092</span><br><span class="line"> </span><br><span class="line"># 用来唯一标识consumer进程所在组的字符串，如果设置同样的group id，表示这些processes都是属于同一个consumer group，默认：&quot;&quot;</span><br><span class="line">spring.kafka.consumer.group-id=TyyLoveZyy</span><br><span class="line"> </span><br><span class="line"># max.poll.records条数据需要在session.timeout.ms这个时间内处理完，默认：500</span><br><span class="line">spring.kafka.consumer.max-poll-records=500</span><br><span class="line"> </span><br><span class="line"># 消费超时时间，大小不能超过session.timeout.ms，默认：3000</span><br><span class="line">spring.kafka.consumer.heartbeat-interval=3000</span><br><span class="line"> </span><br><span class="line"># 如果为真，consumer所fetch的消息的offset将会自动的同步到zookeeper。这项提交的offset将在进程挂掉时，由新的consumer使用，默认：true</span><br><span class="line">spring.kafka.consumer.enable-auto-commit=true</span><br><span class="line"> </span><br><span class="line"># consumer自动向zookeeper提交offset的频率，默认：5000</span><br><span class="line">spring.kafka.consumer.auto-commit-interval=5000</span><br><span class="line"> </span><br><span class="line"># 没有初始化的offset时，可以设置以下三种情况：（默认：latest）</span><br><span class="line"># earliest</span><br><span class="line"># 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费</span><br><span class="line"># latest</span><br><span class="line"># 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</span><br><span class="line"># none</span><br><span class="line"># topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</span><br><span class="line">spring.kafka.consumer.auto-offset-reset=earliest</span><br><span class="line"> </span><br><span class="line"># 每次fetch请求时，server应该返回的最小字节数。如果没有足够的数据返回，请求会等待，直到足够的数据才会返回。默认：1</span><br><span class="line">spring.kafka.consumer.fetch-min-size=1</span><br><span class="line"> </span><br><span class="line"># Fetch请求发给broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长。这个配置就是来配置consumer最多等待response多久。</span><br><span class="line">spring.kafka.consumer.fetch-max-wait=500</span><br><span class="line"> </span><br><span class="line"># 消费者进程的标识。如果设置一个人为可读的值，跟踪问题会比较方便。。默认：&quot;&quot;</span><br><span class="line">spring.kafka.consumer.client-id=1</span><br><span class="line"> </span><br><span class="line"># key的反序列化类。实现了org.apache.kafka.common.serialization.Deserializer接口</span><br><span class="line">spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"> </span><br><span class="line"># 值的反序列化类。实现了org.apache.kafka.common.serialization.Deserializer接口</span><br><span class="line">spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer</span><br></pre></td></tr></table></figure>
<ul>
<li>其他配置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># consumer是通过拉取的方式向服务端拉取数据，当超过指定时间间隔max.poll.interval.ms没有向服务端发送poll()请求，而心跳heartbeat线程仍然在继续，会认为该consumer锁死，就会将该consumer退出group，并进行再分配。默认：300000</span><br><span class="line">spring.kafka.consumer.properties.max.poll.interval.ms=300000</span><br><span class="line"> </span><br><span class="line"># 会话的超时限制。如果consumer在这段时间内没有发送心跳信息，则它会被认为挂掉了，并且reblance将会产生，必须在[group.min.session.timeout.ms, group.max.session.timeout.ms]范围内。默认：10000</span><br><span class="line">spring.kafka.consumer.properties.session.timeout.ms=10000</span><br><span class="line"> </span><br><span class="line"># 在“range”和“roundrobin”策略之间选择一种作为分配partitions给consumer 数据流的策略； 循环的partition分配器分配所有可用的partitions以及所有可用consumer 线程。它会将partition循环的分配到consumer线程上。如果所有consumer实例的订阅都是确定的，则partitions的划分是确定的分布。循环分配策略只有在以下条件满足时才可以：（1）每个topic在每个consumer实例上都有同样数量的数据流。（2）订阅的topic的集合对于consumer group中每个consumer实例来说都是确定的。</span><br><span class="line">spring.kafka.consumer.properties.partition.assignment.strategy=range</span><br><span class="line"> </span><br><span class="line"># 一次fetch请求，从一个broker中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。默认：50 * 1024 * 1024 = 52428800</span><br><span class="line">spring.kafka.consumer.properties.fetch.max.bytes=52428800</span><br><span class="line"> </span><br><span class="line"># Metadata数据的刷新间隔。即便没有任何的partition订阅关系变更也能执行。默认：5 * 60 * 1000 = 300000</span><br><span class="line">spring.kafka.consumer.properties.metadata.max.age.ms=300000</span><br><span class="line"> </span><br><span class="line"># 一次fetch请求，从一个partition中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。broker、topic都会对producer发给它的message size做限制。所以在配置这值时，可以参考broker的message.max.bytes 和 topic的max.message.bytes的配置。默认：1 * 1024 * 1024 = 1048576</span><br><span class="line">spring.kafka.consumer.properties.max.partition.fetch.bytes=1048576</span><br><span class="line"> </span><br><span class="line"># 最大发送的TCP大小。默认：128 * 1024 = 131072，如果设置为 -1 则为操作系统默认大小</span><br><span class="line">spring.kafka.consumer.properties.send.buffer.bytes=131072</span><br><span class="line"> </span><br><span class="line"># 消费者接受缓冲区的大小。这个值在创建Socket连接时会用到。取值范围是：[-1, Integer.MAX]。默认值是：65536 （64 KB），如果值设置为-1，则会使用操作系统默认的值。默认：64 * 1024 = 65536</span><br><span class="line">spring.kafka.consumer.properties.receive.buffer.bytes=65536</span><br><span class="line"> </span><br><span class="line"># 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连，默认：50</span><br><span class="line">spring.kafka.consumer.properties.reconnect.backoff.ms=50</span><br><span class="line"> </span><br><span class="line"># producer客户端连接一个kafka服务（broker）失败重连的总时间，每次连接失败，重连时间都会指数级增加，每次增加的时间会存在20%的随机抖动，以避免连接风暴。默认：1000</span><br><span class="line">spring.kafka.consumer.properties.reconnect.backoff.max.ms=1000</span><br><span class="line"> </span><br><span class="line"># 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中，默认：100</span><br><span class="line">spring.kafka.consumer.properties.retry.backoff.ms=100</span><br><span class="line"> </span><br><span class="line"># metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口退出后，我们会擦除并重写最老的窗口，默认：30000</span><br><span class="line">spring.kafka.consumer.properties.metrics.sample.window.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用于维护metrics的样本数，默认：2</span><br><span class="line">spring.kafka.consumer.properties.metrics.num.samples=2</span><br><span class="line"> </span><br><span class="line"># 用于metrics的最高纪录等级。默认：Sensor.RecordingLevel.INFO.toString()</span><br><span class="line">#spring.kafka.consumer.properties.metrics.recording.level=Sensor.RecordingLevel.INFO.toString()</span><br><span class="line"> </span><br><span class="line"># 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计。默认：Collections.emptyList()</span><br><span class="line">#spring.kafka.consumer.properties.metric.reporters=Collections.emptyList()</span><br><span class="line"> </span><br><span class="line"># 自动检查所消耗记录的CRC32。这可以确保没有线上或磁盘损坏的消息发生。此检查会增加一些开销，因此在寻求极高性能的情况下可能会被禁用。默认：true</span><br><span class="line">spring.kafka.consumer.properties.check.crcs=true</span><br><span class="line"> </span><br><span class="line"># 连接空闲超时时间。因为consumer只与broker有连接（coordinator也是一个broker），所以这个配置的是consumer到broker之间的。默认：9 * 60 * 1000 = 540000</span><br><span class="line">spring.kafka.consumer.properties.connections.max.idle.ms=540000</span><br><span class="line"> </span><br><span class="line"># 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认：30000</span><br><span class="line">spring.kafka.consumer.properties.request.timeout.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用于阻止的KafkaConsumer API的默认超时时间。KIP还为这样的阻塞API添加了重载，以支持指定每个阻塞API使用的特定超时，而不是使用default.api.timeout.ms设置的默认超时。特别是，添加了一个新的轮询（持续时间）API，它不会阻止动态分区分配。旧的poll（long）API已被弃用，将在以后的版本中删除。还为其他KafkaConsumer方法添加了重载，例如partitionsFor，listTopics，offsetsForTimes，beginningOffsets，endOffsets和close，它们接收持续时间。默认：60 * 1000 = 60000</span><br><span class="line">spring.kafka.consumer.properties.default.api.timeout.ms=60000</span><br><span class="line"> </span><br><span class="line"># 用户自定义interceptor。默认：Collections.emptyList()</span><br><span class="line">#spring.kafka.consumer.properties.interceptor.classes=Collections.emptyList()</span><br><span class="line"> </span><br><span class="line"># 是否将内部topics的消息暴露给consumer。默认：true</span><br><span class="line">spring.kafka.consumer.properties.exclude.internal.topics=true</span><br><span class="line"> </span><br><span class="line"># 默认：true</span><br><span class="line">spring.kafka.consumer.properties.internal.leave.group.on.close=true</span><br><span class="line"> </span><br><span class="line"># 默认：IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT)</span><br><span class="line">#spring.kafka.consumer.properties.isolation.level=IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-一些使用规范"><a href="#4-一些使用规范" class="headerlink" title="4. 一些使用规范"></a>4. 一些使用规范</h3><p>一个真实的公司要求的使用规范实例</p>
<ul>
<li><p>Producer 部分参数设定:</p>
<ol>
<li><p>acks 设置为 “all” 即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失.</p>
</li>
<li><p>retries = MAX 无限重试，直到你意识到出现了问题.</p>
</li>
<li><p>使用 callback 来处理消息失败发送逻辑.</p>
</li>
<li><p>min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用.</p>
</li>
<li><p>其他一些超时参数: reconnect.backoff.ms, retry.backoff.ms , linger.ms 结合 batch.size 等.</p>
</li>
</ol>
</li>
<li><p>Consumer 部分参数设定:</p>
<ol>
<li><p>auto.offset.reset 设置为 “earliest” 避免 offset 丢失时跳过未消费的消息. 目前消息存储不统一, 部分使用 zookeeper, 部分使用 kafka topic.</p>
</li>
<li><p>enable.auto.commit=false  关闭自动提交位移, 在消息被完整处理之后再手动提交位移.</p>
</li>
<li><p>consumer 的并发受 partition 的限制. 如果消息处理量比较大的情况请提前与运维联系, 增加 partition 数量应对消费端并发. 默认topic partition 为6-8个.partition 也不是越多越好. 首先会增加 file 和 memory, 其次会延长选举时间, 并且会延长 offset 的查询时间.  partition可以扩容但无法缩减.</p>
</li>
</ol>
</li>
<li><p>极限情况的数据丢失现象.</p>
<ol>
<li><p>即使将 ack 设置为 “all” 也会在一定情况下丢失消息. 因为 kafka 的高性能特性, 消息在写入 kafka 时并没有落盘 而是写入了 OS buffer 中. 使用 OS 的脏页刷新策略周期性落盘, 就算落盘 仍然会有 raid buffer. 前者机器宕机数据丢失, 后者机器跳电数据丢失.</p>
</li>
<li><p>对数据可靠性较高的场景建议 offset 手动提交. 自动提交当遇到业务系统上线被关闭时, 消息读取并且 offset 已经提交, 但是数据没有存储或者仍没来得及消费时, 消息状态在内存中无法保留, 重启应用会跳过消息 致使消息丢失.</p>
</li>
</ol>
</li>
</ul>
<h3 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h3><p><a href="https://www.docs.spring.io/spring-kafka/docs/2.2.0.RELEASE" target="_blank" rel="noopener">Spring-Kafka doc</a></p>
<p><a href="https://www.orchome.com/kafka/index" target="_blank" rel="noopener">Kafka中文网</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">
            -------------本文结束
            <i class="fa fa-paw"></i>
            感谢您的阅读-------------
        </div>
    
</div>
      
    </div>
    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/kafka-消息队列/" rel="tag"># Kafka 消息队列</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/23/记一次使用node-js的child-process模块来启动其他子进程学习/" rel="next" title="记一次使用node.js的child_process模块来调用其他子进程学习">
                <i class="fa fa-chevron-left"></i> 记一次使用node.js的child_process模块来调用其他子进程学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">LumpCode</p>
              <p class="site-description motion-element" itemprop="description">一个梦想发财的男孩子</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.uisdc.com/" title="http://www.uisdc.com/" rel="noopener" target="_blank">优设</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.liaoxuefeng.com/" title="https://www.liaoxuefeng.com/" rel="noopener" target="_blank">廖雪峰</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.alloyteam.com/nav/" title="http://www.alloyteam.com/nav/" rel="noopener" target="_blank">Web前端导航</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.36zhen.com/t?id=3448" title="http://www.36zhen.com/t?id=3448" rel="noopener" target="_blank">前端书籍资料</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ife.baidu.com/" title="http://ife.baidu.com/" rel="noopener" target="_blank">百度前端技术学院</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://wf.uisdc.com/cn/" title="http://wf.uisdc.com/cn/" rel="noopener" target="_blank">google前端开发基础</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一-Kafka介绍"><span class="nav-number">1.</span> <span class="nav-text">一. Kafka介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-概述"><span class="nav-number">1.1.</span> <span class="nav-text">1. 概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-消息系统介绍"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 消息系统介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Kafka的优点"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Kafka的优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Kafka中的术语解释"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 Kafka中的术语解释</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-broker"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.4.1 broker</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-Topic"><span class="nav-number">1.4.2.</span> <span class="nav-text">1.4.2 Topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-3-Partition"><span class="nav-number">1.4.3.</span> <span class="nav-text">1.4.3 Partition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-4-Producer"><span class="nav-number">1.4.4.</span> <span class="nav-text">1.4.4 Producer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-5-Consumer"><span class="nav-number">1.4.5.</span> <span class="nav-text">1.4.5 Consumer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-6-Consumer-Group"><span class="nav-number">1.4.6.</span> <span class="nav-text">1.4.6 Consumer Group</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-7-Leader"><span class="nav-number">1.4.7.</span> <span class="nav-text">1.4.7 Leader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-8-Follower"><span class="nav-number">1.4.8.</span> <span class="nav-text">1.4.8 Follower</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二-kafka安装和启动"><span class="nav-number">2.</span> <span class="nav-text">二. kafka安装和启动</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-1-下载代码"><span class="nav-number">2.1.</span> <span class="nav-text">Step 1: 下载代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-2-启动服务"><span class="nav-number">2.2.</span> <span class="nav-text">Step 2: 启动服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-3-创建一个主题-topic"><span class="nav-number">2.3.</span> <span class="nav-text">Step 3: 创建一个主题(topic)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-4-发送消息"><span class="nav-number">2.4.</span> <span class="nav-text">Step 4: 发送消息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-5-消费消息"><span class="nav-number">2.5.</span> <span class="nav-text">Step 5: 消费消息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-6-设置多个broker集群"><span class="nav-number">2.6.</span> <span class="nav-text">Step 6: 设置多个broker集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-7-使用-Kafka-Connect-来-导入-导出-数据"><span class="nav-number">2.7.</span> <span class="nav-text">Step 7: 使用 Kafka Connect 来 导入/导出 数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-8-使用Kafka-Stream来处理数据"><span class="nav-number">2.8.</span> <span class="nav-text">Step 8: 使用Kafka Stream来处理数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三-Kafka核心API与设计理念详解"><span class="nav-number">3.</span> <span class="nav-text">三. Kafka核心API与设计理念详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Kafka的架构"><span class="nav-number">3.1.</span> <span class="nav-text">1.Kafka的架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Topics和Partition"><span class="nav-number">3.2.</span> <span class="nav-text">2.Topics和Partition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-创建Topic"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1 创建Topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-删除topic"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2 删除topic</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Producer消息路由"><span class="nav-number">3.3.</span> <span class="nav-text">3.Producer消息路由</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-写入方式"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.1 写入方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-消息路由"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.2 消息路由</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-写入流程"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3 写入流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Consumer-Group"><span class="nav-number">3.4.</span> <span class="nav-text">4.Consumer Group</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Push-vs-Pull"><span class="nav-number">3.5.</span> <span class="nav-text">5.Push vs. Pull</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Kafka-delivery-guarantee"><span class="nav-number">3.6.</span> <span class="nav-text">6.Kafka delivery guarantee</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四-Kafka的高可用"><span class="nav-number">4.</span> <span class="nav-text">四. Kafka的高可用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-高可用的由来"><span class="nav-number">4.1.</span> <span class="nav-text">1. 高可用的由来</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Kafka-HA的设计解析"><span class="nav-number">4.2.</span> <span class="nav-text">2. Kafka HA的设计解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-如何将所有Replica均匀分布到整个集群"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.1 如何将所有Replica均匀分布到整个集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Data-Replication（副本策略）"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2 Data Replication（副本策略）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-1-消息传递同步策略"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">2.2.1 消息传递同步策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-2-ACK前需要保证有多少个备份"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">2.2.2 ACK前需要保证有多少个备份</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-3-Leader-Election算法"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">2.2.3 Leader Election算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-如何处理所有Replica都不工作"><span class="nav-number">4.2.2.4.</span> <span class="nav-text">2.2.4 如何处理所有Replica都不工作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-5-选举Leader"><span class="nav-number">4.2.2.5.</span> <span class="nav-text">2.2.5 选举Leader</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-broker保存消息"><span class="nav-number">4.3.</span> <span class="nav-text">4. broker保存消息</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-存储方式"><span class="nav-number">4.3.1.</span> <span class="nav-text">4.1 存储方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-存储策略"><span class="nav-number">4.3.2.</span> <span class="nav-text">4.2 存储策略</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五-Kafka与其他常用MQ对比"><span class="nav-number">5.</span> <span class="nav-text">五. Kafka与其他常用MQ对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六-SpringBoot整合Kafka"><span class="nav-number">6.</span> <span class="nav-text">六. SpringBoot整合Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-在创建好的gradle工程中引入依赖"><span class="nav-number">6.1.</span> <span class="nav-text">1. 在创建好的gradle工程中引入依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-编写配置文件"><span class="nav-number">6.2.</span> <span class="nav-text">2. 编写配置文件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#applacation-yml"><span class="nav-number">6.2.1.</span> <span class="nav-text">applacation.yml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KafkaConfig-java"><span class="nav-number">6.2.2.</span> <span class="nav-text">KafkaConfig.java</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KafkaAdminConfig"><span class="nav-number">6.2.3.</span> <span class="nav-text">KafkaAdminConfig</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-编写Kafka通用工具类"><span class="nav-number">6.3.</span> <span class="nav-text">3. 编写Kafka通用工具类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-编写Kafka-producer"><span class="nav-number">6.4.</span> <span class="nav-text">4. 编写Kafka producer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-编写Kafka-consumer"><span class="nav-number">6.5.</span> <span class="nav-text">5. 编写Kafka consumer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-测试"><span class="nav-number">6.6.</span> <span class="nav-text">6. 测试</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#启动"><span class="nav-number">6.6.1.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-简单发送-订阅"><span class="nav-number">6.6.2.</span> <span class="nav-text">1. 简单发送-订阅</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-批量消费"><span class="nav-number">6.6.3.</span> <span class="nav-text">2. 批量消费</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-确认机制"><span class="nav-number">6.6.4.</span> <span class="nav-text">3. 确认机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-多主题订阅"><span class="nav-number">6.6.5.</span> <span class="nav-text">4. 多主题订阅</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-多主题指定分区指定偏移量订阅"><span class="nav-number">6.6.6.</span> <span class="nav-text">5. 多主题指定分区指定偏移量订阅</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七-扩展"><span class="nav-number">7.</span> <span class="nav-text">七. 扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Kafka常用命令"><span class="nav-number">7.1.</span> <span class="nav-text">1. Kafka常用命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Kafka管理工具"><span class="nav-number">7.2.</span> <span class="nav-text">2. Kafka管理工具</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Spring-Boot-Kafka参数配置"><span class="nav-number">7.3.</span> <span class="nav-text">3. Spring Boot Kafka参数配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-一些使用规范"><span class="nav-number">7.4.</span> <span class="nav-text">4. 一些使用规范</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-参考资料"><span class="nav-number">7.5.</span> <span class="nav-text">5. 参考资料</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      
      <div id="miusic163player">
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86
          src="//music.163.com/outchain/player?type=2&id=1338695683&auto=1&height=66"></iframe>
          <!-- auto为1，自动播放 -->
      </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LumpCode</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">站点总字数：</span>
    
    <span title="站点总字数">89k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    
    <span title="站点阅读时长">1:21</span>
  
</div>


  <!-- <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.7.1</div> -->



  <!-- <span class="post-meta-divider">|</span> -->



  <!-- <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div> -->




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
      
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js"></script>













  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js"></script>

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  
  

  

<script src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script>



  

<script src="//cdnjs.cloudflare.com/ajax/libs/valine/1.3.4/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: '8dbMIBjk3xnKfzq4RehDeHCU-gzGzoHsz',
    appKey: 'FjogpIe3BDlp82J7nb8bQTh7',
    placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false
  });
</script>




  


  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
    
  
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.css"/>

  
  
    
  
  <script src="//cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.js"></script>
  

  <script src="/js/src/algolia-search.js?v=6.7.0"></script>



  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
      }
      else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  

  

  

  
  
  
    
  
  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-bookmark@1/bookmark.min.js"></script>
  <script>
  
    bookmark.scrollToMark('auto', "#更多");
  
  </script>


  

  


  
  <script type="text/javascript"
  color="0,0,255" opacity='0.7' zIndex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
</body>
</html>

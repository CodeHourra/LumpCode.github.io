<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>温良恭俭让</title>
  
  <subtitle>Stay hungry Stay foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liugan96.top/"/>
  <updated>2020-07-07T13:31:09.585Z</updated>
  <id>http://liugan96.top/</id>
  
  <author>
    <name>LumpCode</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>高性能消息中间件-Kafka</title>
    <link href="http://liugan96.top/2020/07/07/%E9%AB%98%E6%80%A7%E8%83%BD%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-kafka/"/>
    <id>http://liugan96.top/2020/07/07/高性能消息中间件-kafka/</id>
    <published>2020-07-07T13:27:58.000Z</published>
    <updated>2020-07-07T13:31:09.585Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Kafka介绍"><a href="#一-Kafka介绍" class="headerlink" title="一. Kafka介绍"></a>一. Kafka介绍</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507184834852-1994140834.png" alt="img"></p><p>Kafka最先是由Linkedin公司开发，是一个分布式的、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当作MQ系统），常见可用于web/nginx日志、访问日志、消息服务等等。于2010年贡献给了Apache基金会并成为顶级开源项目。</p><ul><li>主要应用场景：日志收集系统、消息系统</li><li>设计目标<ul><li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。</li><li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。</li><li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。</li><li>同时支持离线数据处理和实时数据处理。</li><li>Scale out:支持在线水平扩展</li></ul></li></ul><h3 id="1-2-消息系统介绍"><a href="#1-2-消息系统介绍" class="headerlink" title="1.2 消息系统介绍"></a>1.2 消息系统介绍</h3><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需要关系数据，不需要关心数据之间是如何传递的。</p><p>分布式消息传递基于可靠的消息队列，在客户端和消息系统之间异步传递消息。</p><p>消息的传递模式：</p><ul><li><p>点对点传递</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190326476-771565746.png" alt="img"></p></li><li><p>发布-订阅模式（大部分消息系统选用此种方式）</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190443404-1266011458.png" alt="img"></p></li></ul><h3 id="1-3-Kafka的优点"><a href="#1-3-Kafka的优点" class="headerlink" title="1.3 Kafka的优点"></a>1.3 Kafka的优点</h3><ul><li>解耦</li><li>我们在项目开始的时候预测将来可能出现的需求是十分困难的。但是消息系统不用关心其内部处理过程，只需要遵循其约定好的消息的接口。这就允许你方便的扩展或修改其两边的处理过程，只需要保证其双方均遵循同样的接口约束。</li><li>冗余（副本）<ul><li>有些情况下处理消息数据的过程或许会失败，除非消息数据被持久化了，否则就会丢失。消息队列通常会把消息数据持久化直到确保他们已经被完全处理之后，通过这一方式避免消息丢失的风险。许多消息队列采取“插入-获取-删除”的策略模式，把一个消息删除之前需要你的系统明确的指定改条消息已经被处理完毕，从而保证了你的数据被安全的保存到你使用完毕。</li></ul></li><li>扩展性<ul><li>消息队列解耦了我们的处理过程，所以我们消息的入队和处理速率是很容易的，只需要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。</li></ul></li><li>灵活性&amp;峰值处理能力<ul><li>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</li></ul></li><li>可恢复性<ul><li>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</li></ul></li><li>顺序保证<ul><li>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。</li></ul></li><li><p>缓冲</p><ul><li>在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</li></ul></li><li><p>异步通信</p><ul><li>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</li></ul></li></ul><h3 id="1-4-Kafka中的术语解释"><a href="#1-4-Kafka中的术语解释" class="headerlink" title="1.4 Kafka中的术语解释"></a>1.4 Kafka中的术语解释</h3><p>在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507190731172-1317551019.png" alt="img"></p><p>上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。</p><p>如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。</p><h4 id="1-4-1-broker"><a href="#1-4-1-broker" class="headerlink" title="1.4.1 broker"></a>1.4.1 broker</h4><p>Kafka 集群包含一个或多个服务器，服务器节点称为broker。</p><p>broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。</p><p>如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。</p><p>如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。</p><h4 id="1-4-2-Topic"><a href="#1-4-2-Topic" class="headerlink" title="1.4.2 Topic"></a>1.4.2 Topic</h4><p>每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p><p>类似于数据库的表名</p><h4 id="1-4-3-Partition"><a href="#1-4-3-Partition" class="headerlink" title="1.4.3 Partition"></a>1.4.3 Partition</h4><p>topic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。</p><h4 id="1-4-4-Producer"><a href="#1-4-4-Producer" class="headerlink" title="1.4.4 Producer"></a>1.4.4 Producer</h4><p>生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息<strong>追加</strong>到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。</p><h4 id="1-4-5-Consumer"><a href="#1-4-5-Consumer" class="headerlink" title="1.4.5 Consumer"></a>1.4.5 Consumer</h4><p>消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。</p><h4 id="1-4-6-Consumer-Group"><a href="#1-4-6-Consumer-Group" class="headerlink" title="1.4.6 Consumer Group"></a>1.4.6 Consumer Group</h4><p>每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p><h4 id="1-4-7-Leader"><a href="#1-4-7-Leader" class="headerlink" title="1.4.7 Leader"></a>1.4.7 Leader</h4><p>每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。</p><h4 id="1-4-8-Follower"><a href="#1-4-8-Follower" class="headerlink" title="1.4.8 Follower"></a>1.4.8 Follower</h4><p>Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。</p><h2 id="二-kafka安装和启动"><a href="#二-kafka安装和启动" class="headerlink" title="二. kafka安装和启动"></a>二. kafka安装和启动</h2><h3 id="Step-1-下载代码"><a href="#Step-1-下载代码" class="headerlink" title="Step 1: 下载代码"></a>Step 1: 下载代码</h3><p>下载<code>2.5.0</code>版本并且解压它。</p><h3 id="Step-2-启动服务"><a href="#Step-2-启动服务" class="headerlink" title="Step 2: 启动服务"></a>Step 2: 启动服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; tar -xzf kafka_2.12-2.5.0.tgz </span><br><span class="line">&gt; cd kafka_2.12-2.3.0</span><br></pre></td></tr></table></figure><p>2.12：  scala版本，2.5.0：kafka版本</p><p>运行kafka需要使用Zookeeper，所以你需要先启动Zookeeper，如果你没有Zookeeper，你可以使用kafka自带打包和配置好的Zookeeper。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/zookeeper-server-start.sh config/zookeeper.properties</span><br><span class="line">[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>现在启动kafka服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)</span><br><span class="line">[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="Step-3-创建一个主题-topic"><a href="#Step-3-创建一个主题-topic" class="headerlink" title="Step 3: 创建一个主题(topic)"></a>Step 3: 创建一个主题(topic)</h3><p>创建一个名为“test”的Topic，只有一个分区和一个备份：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure><p>创建好之后，可以通过运行以下命令，查看已创建的topic信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">test</span><br></pre></td></tr></table></figure><p>或者，除了手工创建topic外，你也可以配置你的broker，当发布一个不存在的topic时自动创建topic。</p><h3 id="Step-4-发送消息"><a href="#Step-4-发送消息" class="headerlink" title="Step 4: 发送消息"></a>Step 4: 发送消息</h3><p>Kafka提供了一个命令行的工具，可以从输入文件或者命令行中读取消息并发送给Kafka集群。每一行是一条消息。<br>运行producer（生产者）,然后在控制台输入几条消息到服务器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line">This is a message</span><br><span class="line">This is another message</span><br></pre></td></tr></table></figure><h3 id="Step-5-消费消息"><a href="#Step-5-消费消息" class="headerlink" title="Step 5: 消费消息"></a>Step 5: 消费消息</h3><p>Kafka也提供了一个消费消息的命令行工具，将存储的信息输出出来。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line">This is a message</span><br><span class="line">This is another message</span><br></pre></td></tr></table></figure><p>如果你有2台不同的终端上运行上述命令，那么当你在运行生产者时，消费者就能消费到生产者发送的消息。</p><h3 id="Step-6-设置多个broker集群"><a href="#Step-6-设置多个broker集群" class="headerlink" title="Step 6: 设置多个broker集群"></a>Step 6: 设置多个broker集群</h3><p>到目前，我们只是单一的运行一个broker，没什么意思。对于Kafka，一个broker仅仅只是一个集群的大小，所有让我们多设几个broker。</p><p>首先为每个broker创建一个配置文件: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; cp config/server.properties config/server-1.properties </span><br><span class="line">&gt; cp config/server.properties config/server-2.properties</span><br></pre></td></tr></table></figure><p>现在编辑这些新建的文件，设置以下属性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config/server-1.properties: </span><br><span class="line">    broker.id=1 </span><br><span class="line">    listeners=PLAINTEXT://:9093 </span><br><span class="line">    log.dir=/tmp/kafka-logs-1</span><br><span class="line"></span><br><span class="line">config/server-2.properties: </span><br><span class="line">    broker.id=2 </span><br><span class="line">    listeners=PLAINTEXT://:9094 </span><br><span class="line">    log.dir=/tmp/kafka-logs-2</span><br></pre></td></tr></table></figure><p>复制</p><p><code>broker.id</code>是集群中每个节点的唯一且永久的名称，我们修改端口和日志目录是因为我们现在在同一台机器上运行，我们要防止broker在同一端口上注册和覆盖对方的数据。</p><p>我们已经运行了zookeeper和刚才的一个kafka节点，所有我们只需要在启动2个新的kafka节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-server-start.sh config/server-1.properties &amp;</span><br><span class="line">... </span><br><span class="line">&gt; bin/kafka-server-start.sh config/server-2.properties &amp;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>现在，我们创建一个新topic，把备份设置为：3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure><p>好了，现在我们已经有了一个集群了，我们怎么知道每个集群在做什么呢？运行命令“describe topics”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:</span><br><span class="line">Topic: my-replicated-topic    Partition: 0    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0</span><br></pre></td></tr></table></figure><p>输出解释：第一行是所有分区的摘要，其次，每一行提供一个分区信息，因为我们只有一个分区，所以只有一行。</p><ul><li>“leader”：该节点负责该分区的所有的读和写，每个节点的<code>leader</code>都是随机选择的。</li><li>“replicas”：备份的节点列表，无论该节点是否是leader或者目前是否还活着，只是显示。</li><li>“isr”：“同步备份”的节点列表，也就是活着的节点并且正在同步leader。</li></ul><p>我们运行这个命令，看看一开始我们创建的那个节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test</span><br><span class="line">Topic:test    PartitionCount:1    ReplicationFactor:1    Configs:</span><br><span class="line">Topic: test    Partition: 0    Leader: 0    Replicas: 0    Isr: 0</span><br></pre></td></tr></table></figure><p>这并不奇怪，刚才创建的主题没有Replicas，并且在服务器“0”上，我们创建它的时候，集群中只有一个服务器，所以是“0”。</p><p>让我们来发布一些信息在新的topic上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic</span><br><span class="line"> ...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>现在，消费这些消息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic</span><br><span class="line"> ...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>我们要测试集群的容错，kill掉leader，Broker1作为当前的leader，也就是kill掉Broker1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; ps | grep server-1.properties</span><br><span class="line">7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java... </span><br><span class="line">&gt; kill -9 7564</span><br></pre></td></tr></table></figure><p>在Windows上使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; wmic process where &quot;caption = &apos;java.exe&apos; and commandline like &apos;%server-1.properties%&apos;&quot; get processid</span><br><span class="line">ProcessId</span><br><span class="line">6016</span><br><span class="line">&gt; taskkill /pid 6016 /f</span><br></pre></td></tr></table></figure><p>备份节点之一成为新的leader，而broker1已经不在同步备份集合里了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:</span><br><span class="line">Topic: my-replicated-topic    Partition: 0    Leader: 2    Replicas: 1,2,0    Isr: 2,0</span><br></pre></td></tr></table></figure><p>但是，消息仍然没丢：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><h3 id="Step-7-使用-Kafka-Connect-来-导入-导出-数据"><a href="#Step-7-使用-Kafka-Connect-来-导入-导出-数据" class="headerlink" title="Step 7: 使用 Kafka Connect 来 导入/导出 数据"></a>Step 7: 使用 Kafka Connect 来 导入/导出 数据</h3><p>从控制台写入和写回数据是一个方便的开始，但你可能想要从其他来源导入或导出数据到其他系统。对于大多数系统，可以使用kafka Connect，而不需要编写自定义集成代码。</p><p><code>Kafka Connect</code>是导入和导出数据的一个工具。它是一个可扩展的工具，运行连接器，实现与自定义的逻辑的外部系统交互。在这个快速入门里，我们将看到如何运行Kafka Connect用简单的连接器从文件导入数据到Kafka主题，再从Kafka主题导出数据到文件。</p><p>首先，我们首先创建一些“种子”数据用来测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo -e &quot;foo\nbar&quot; &gt; test.txt</span><br></pre></td></tr></table></figure><p>windows上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; echo foo&gt; test.txt</span><br><span class="line">&gt; echo bar&gt;&gt; test.txt</span><br></pre></td></tr></table></figure><p>接下来，我们开始2个连接器运行在独立的模式，这意味着它们运行在一个单一的，本地的，专用的进程。我们提供3个配置文件作为参数。首先是Kafka Connect处理的配置，包含常见的配置，例如要连接的Kafka broker和数据的序列化格式。其余的配置文件都指定了要创建的连接器。包括连接器唯一名称，和要实例化的连接器类。以及连接器所需的任何其他配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</span><br></pre></td></tr></table></figure><p>kafka附带了这些示例的配置文件，并且使用了刚才我们搭建的本地集群配置并创建了2个连接器：第一个是源连接器，从输入文件中读取并发布到Kafka主题中，第二个是接收连接器，从kafka主题读取消息输出到外部文件。</p><p>在启动过程中，你会看到一些日志消息，包括一些连接器实例化的说明。一旦kafka Connect进程已经开始，导入连接器应该读取从</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.txt</span><br></pre></td></tr></table></figure><p>和写入到topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connect-test</span><br></pre></td></tr></table></figure><p>,导出连接器从主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connect-test</span><br></pre></td></tr></table></figure><p>读取消息写入到文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.sink.txt</span><br></pre></td></tr></table></figure><p>. 我们可以通过验证输出文件的内容来验证数据数据已经全部导出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">more test.sink.txt</span><br><span class="line"> foo</span><br><span class="line"> bar</span><br></pre></td></tr></table></figure><p>注意，导入的数据也已经在Kafka主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connect-test</span><br></pre></td></tr></table></figure><p>里,所以我们可以使用该命令查看这个主题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning</span><br><span class="line"> &#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false&#125;,&quot;payload&quot;:&quot;foo&quot;&#125;</span><br><span class="line">&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false&#125;,&quot;payload&quot;:&quot;bar&quot;&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>连接器继续处理数据，因此我们可以添加数据到文件并通过管道移动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;Another line&quot; &gt;&gt; test.txt</span><br></pre></td></tr></table></figure><p>你应该会看到出现在消费者控台输出一行信息并导出到文件。</p><h3 id="Step-8-使用Kafka-Stream来处理数据"><a href="#Step-8-使用Kafka-Stream来处理数据" class="headerlink" title="Step 8: 使用Kafka Stream来处理数据"></a>Step 8: 使用Kafka Stream来处理数据</h3><p>Kafka Stream是kafka的客户端库，用于实时流处理和分析存储在kafka broker的数据，这个快速入门示例将演示如何运行一个流应用程序。一个WordCountDemo的例子（为了方便阅读，使用的是java8 lambda表达式）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">KTable wordCounts = textLines</span><br><span class="line">    // Split each text line, by whitespace, into words.</span><br><span class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(&quot;W+&quot;)))</span><br><span class="line"></span><br><span class="line">    // Ensure the words are available as record keys for the next aggregate operation.</span><br><span class="line">    .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value))</span><br><span class="line"></span><br><span class="line">    // Count the occurrences of each word (record key) and store the results into a table named &quot;Counts&quot;.</span><br><span class="line">    .countByKey(&quot;Counts&quot;)</span><br></pre></td></tr></table></figure><p>它实现了wordcount算法，从输入的文本计算出一个词出现的次数。然而，不像其他的WordCount的例子，你可能会看到，在有限的数据之前，执行的演示应用程序的行为略有不同，因为它的目的是在一个无限的操作，数据流。类似的有界变量，它是一种动态算法，跟踪和更新的单词计数。然而，由于它必须假设潜在的无界输入数据，它会定期输出其当前状态和结果，同时继续处理更多的数据，因为它不知道什么时候它处理过的“所有”的输入数据。</p><p>现在准备输入数据到kafka的topic中，随后kafka Stream应用处理这个topic的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; echo -e &quot;all streams lead to kafka\nhello kafka streams\njoin kafka summit&quot; &gt; file-input.txt</span><br></pre></td></tr></table></figure><p>接下来，使用控制台的producer 将输入的数据发送到指定的topic（streams-file-input）中，（在实践中，stream数据可能会持续流入，其中kafka的应用将启动并运行）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --create \</span><br><span class="line">            --zookeeper localhost:2181 \</span><br><span class="line">            --replication-factor 1 \</span><br><span class="line">            --partitions 1 \</span><br><span class="line">            --topic streams-file-input</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list localhost:9092 --topic streams-file-input</span><br></pre></td></tr></table></figure><p>现在，我们运行 WordCount 处理输入的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo</span><br></pre></td></tr></table></figure><p>不会有任何的STDOUT输出，除了日志，结果不断地写回另一个topic（streams-wordcount-output），demo运行几秒，然后，不像典型的流处理应用程序，自动终止。</p><p>现在我们检查WordCountDemo应用，从输出的topic读取。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./bin/kafka-console-consumer --zookeeper localhost:2181 </span><br><span class="line">            --topic streams-wordcount-output </span><br><span class="line">            --from-beginning </span><br><span class="line">            --formatter kafka.tools.DefaultMessageFormatter </span><br><span class="line">            --property print.key=true </span><br><span class="line">            --property print.key=true </span><br><span class="line">            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer </span><br><span class="line">            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</span><br></pre></td></tr></table></figure><p>输出数据打印到控台（你可以使用Ctrl-C停止）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">all     1</span><br><span class="line">streams 1</span><br><span class="line">lead    1</span><br><span class="line">to      1</span><br><span class="line">kafka   1</span><br><span class="line">hello   1</span><br><span class="line">kafka   2</span><br><span class="line">streams 2</span><br><span class="line">join    1</span><br><span class="line">kafka   3</span><br><span class="line">summit  1</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>第一列是message的key，第二列是message的value，要注意，输出的实际是一个连续的更新流，其中每条数据（即：原始输出的每行）是一个单词的最新的count，又叫记录键“kafka”。对于同一个key有多个记录，每个记录之后是前一个的更新。</p><h2 id="三-Kafka核心API与设计理念详解"><a href="#三-Kafka核心API与设计理念详解" class="headerlink" title="三. Kafka核心API与设计理念详解"></a>三. Kafka核心API与设计理念详解</h2><h3 id="1-Kafka的架构"><a href="#1-Kafka的架构" class="headerlink" title="1.Kafka的架构"></a>1.Kafka的架构</h3><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507192145249-1414897650.png" alt="img"></p><p>如上图所示，该图是一个典型的Kafka集群中包含若干Producer，若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p><h3 id="2-Topics和Partition"><a href="#2-Topics和Partition" class="headerlink" title="2.Topics和Partition"></a>2.Topics和Partition</h3><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507192840409-1435311830.png" alt="img"></p><p>对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配置<code>KAFKA_HOME/config/server.properties</code>，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># The minimum age of a log file to be eligible for deletion 符合删除条件的日志文件的最小生存时间</span><br><span class="line">log.retention.hours=168</span><br><span class="line"># The maximum size of a log segment file. When this size is reached a new log segment will be created.日志段文件的最大大小。达到此大小后，将创建一个新的日志段。</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"># The interval at which log segments are checked to see if they can be deleted according to the retention policies 检查日志段以了解是否可以根据保留策略将其删除的时间间隔</span><br><span class="line">log.retention.check.interval.ms=300000 # 300s </span><br><span class="line"># If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction. 如果设置了log.cleaner.enable = true，则将启用清理器，然后可以标记单个日志以进行日志压缩。</span><br><span class="line">log.cleaner.enable=false</span><br></pre></td></tr></table></figure><p>因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。</p><h4 id="2-1-创建Topic"><a href="#2-1-创建Topic" class="headerlink" title="2.1 创建Topic"></a>2.1 创建Topic</h4><p>创建 topic 的序列图如下所示：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200343317-1340406332.png" alt="img"></p><p>流程说明：</p><blockquote><p>1、 controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。<br>2、 controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：<br>     2.1、 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR<br>     2.2、 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state<br>3、 controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。</p></blockquote><h4 id="2-2-删除topic"><a href="#2-2-删除topic" class="headerlink" title="2.2 删除topic"></a>2.2 删除topic</h4><p>删除 topic 的序列图如下所示：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200533571-310409492.png" alt="img"></p><p>流程说明：</p><blockquote><p>1、 controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。<br>2、 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。</p></blockquote><h3 id="3-Producer消息路由"><a href="#3-Producer消息路由" class="headerlink" title="3.Producer消息路由"></a>3.Producer消息路由</h3><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在<code>KAFKA_HOME/config/server.properties</code>中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。</p><p>在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。</p><blockquote><p>Math.abs(“routerKey or groupName”.hashCode()) % ParititionNum</p></blockquote><h4 id="3-1-写入方式"><a href="#3-1-写入方式" class="headerlink" title="3.1 写入方式"></a>3.1 写入方式</h4><p>producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。</p><h4 id="3-2-消息路由"><a href="#3-2-消息路由" class="headerlink" title="3.2 消息路由"></a>3.2 消息路由</h4><p>producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：</p><blockquote><p>1、 指定了 patition，则直接使用；<br>2、 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition<br>3、 patition 和 key 都未指定，使用轮询选出一个 patition。</p></blockquote><h4 id="3-3-写入流程"><a href="#3-3-写入流程" class="headerlink" title="3.3 写入流程"></a>3.3 写入流程</h4><p>producer 写入消息序列图如下所示：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507200019142-182025107.png" alt="img"></p><p>流程说明：</p><blockquote><p>1、 producer 先从 zookeeper 的 “/brokers/…/state” 节点找到该 partition 的 leader<br>2、 producer 将消息发送给该 leader<br>3、 leader 将消息写入本地 log<br>4、 followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK<br>5、 leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</p></blockquote><h3 id="4-Consumer-Group"><a href="#4-Consumer-Group" class="headerlink" title="4.Consumer Group"></a>4.Consumer Group</h3><p>使用Consumer high level API的时候,同一个Topic的一条消息只能被同一个Consumer Group中的一个Consumer消费,但是多个Consumer Group则可以同时消费这一信息.</p><blockquote><ol><li>保证消息消费的顺序性</li></ol><p>传统消息中间件存在一个queue中， c1， c2， c3三个消费者从queue中取得的顺序在消息中间件看起来是顺序一致的，但是三个消费者实际处理时可能不一定是顺序的，尤其是多个消费者之间存在业务严格明确依赖的情况下， 比如c1用户下单后，c2再优惠，c3再送积分，此时传统的消息中间件是没有一个很好发办法去处理的。Kafka就可以解决这个问题。</p><p>Kafka解决这个问题的办法：</p><ol><li>首先规定了一个分区只能有一个消费者，我们可以把有业务依赖性的消息往一个分区中发送</li><li>多个分区可以提高并发</li></ol><p>既可以满足并发，也可以满足消息的一致性</p></blockquote><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507193553697-2141118410.png" alt="img"></p><p>这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。</p><h3 id="5-Push-vs-Pull"><a href="#5-Push-vs-Pull" class="headerlink" title="5.Push vs. Pull"></a>5.Push vs. Pull</h3><p>作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。</p><p>push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。</p><p>对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p><h3 id="6-Kafka-delivery-guarantee"><a href="#6-Kafka-delivery-guarantee" class="headerlink" title="6.Kafka delivery guarantee"></a>6.Kafka delivery guarantee</h3><p>有这么几种可能的delivery guarantee：</p><blockquote><p>At most once 　　消息可能会丢，但绝不会重复传输</p><p>At least one 　　 消息绝不会丢，但可能会重复传输</p><p>Exactly once 　　 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。</p></blockquote><p>当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。</p><p>接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。</p><p><strong>Kafka默认保证At least once</strong>，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。</p><h2 id="四-Kafka的高可用"><a href="#四-Kafka的高可用" class="headerlink" title="四. Kafka的高可用"></a>四. Kafka的高可用</h2><h3 id="1-高可用的由来"><a href="#1-高可用的由来" class="headerlink" title="1. 高可用的由来"></a>1. 高可用的由来</h3><p>​        在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。</p><p>　　如果Producer使用同步模式则Producer会在尝试重新发送message.send.max.retries（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。</p><p>　　如果Producer使用异步模式，则Producer会尝试重新发送message.send.max.retries（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。同时，Kafka的Producer并未对异步模式提供callback接口。</p><p>　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。</p><p>　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。</p><p>　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。</p><h3 id="2-Kafka-HA的设计解析"><a href="#2-Kafka-HA的设计解析" class="headerlink" title="2. Kafka HA的设计解析"></a>2. Kafka HA的设计解析</h3><h4 id="2-1-如何将所有Replica均匀分布到整个集群"><a href="#2-1-如何将所有Replica均匀分布到整个集群" class="headerlink" title="2.1 如何将所有Replica均匀分布到整个集群"></a>2.1 如何将所有Replica均匀分布到整个集群</h4><p>为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。</p><p>Kafka分配Replica的算法如下：</p><p>1.将所有Broker（假设共n个Broker）和待分配的Partition排序</p><p>2.将第i个Partition分配到第（i mod n）个Broker上</p><p>3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</p><h4 id="2-2-Data-Replication（副本策略）"><a href="#2-2-Data-Replication（副本策略）" class="headerlink" title="2.2 Data Replication（副本策略）"></a>2.2 Data Replication（副本策略）</h4><p>Kafka的高可靠性的保障来源于其健壮的副本（replication）策略。</p><h5 id="2-2-1-消息传递同步策略"><a href="#2-2-1-消息传递同步策略" class="headerlink" title="2.2.1 消息传递同步策略"></a>2.2.1 消息传递同步策略</h5><p>Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少，Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。</p><p>为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。</p><p>Consumer读消息也是从Leader读取，只有被commit过的消息才会暴露给Consumer。</p><p>Kafka Replication的数据流如下图所示：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/1228818-20180507194612622-1788087919.png" alt="img"></p><h5 id="2-2-2-ACK前需要保证有多少个备份"><a href="#2-2-2-ACK前需要保证有多少个备份" class="headerlink" title="2.2.2 ACK前需要保证有多少个备份"></a>2.2.2 ACK前需要保证有多少个备份</h5><p>对于Kafka而言，定义一个Broker是否“活着”包含两个条件：</p><ul><li>一是它必须维护与ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。</li><li>二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。</li></ul><p>Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在<code>KAFKA_HOME/config/server.properties</code>中通过<code>replica.lag.max.messages</code>配置，其默认值是4000）或者<code>Follower</code>超过一定时间（该值可在<code>KAFKA_HOME/config/server.properties</code>中通过<code>replica.lag.time.max.ms</code>来配置，其默认值是10000）未向<code>Leader</code>发送<code>fetch</code>请求。</p><p>Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。</p><p>需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。</p><h5 id="2-2-3-Leader-Election算法"><a href="#2-2-3-Leader-Election算法" class="headerlink" title="2.2.3 Leader Election算法"></a>2.2.3 Leader Election算法</h5><p>Leader选举本质上是一个分布式锁，有两种方式实现基于ZooKeeper的分布式锁：</p><ul><li>节点名称唯一性：多个客户端创建一个节点，只有成功创建节点的客户端才能获得锁</li><li>临时顺序节点：所有客户端在某个目录下创建自己的临时顺序节点，只有序号最小的才获得锁</li></ul><p>一种非常常用的选举leader的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在ZooKeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种方式。</p><p>Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。</p><p>虽然Majority Vote与ISR相比有不需等待最慢的Broker这一优势，但是Kafka作者认为Kafka可以通过Producer选择是否被commit阻塞来改善这一问题，并且节省下来的Replica和磁盘使得ISR模式仍然值得。</p><h5 id="2-2-4-如何处理所有Replica都不工作"><a href="#2-2-4-如何处理所有Replica都不工作" class="headerlink" title="2.2.4 如何处理所有Replica都不工作"></a>2.2.4 如何处理所有Replica都不工作</h5><p>在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：</p><p>1.等待ISR中的任一个Replica“活”过来，并且选它作为Leader</p><p>2.选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader</p><p>这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。</p><h5 id="2-2-5-选举Leader"><a href="#2-2-5-选举Leader" class="headerlink" title="2.2.5 选举Leader"></a>2.2.5 选举Leader</h5><p>最简单最直观的方案是，所有Follower都在ZooKeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（ZooKeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。</p><p>但是该方法会有3个问题：</p><p>1.split-brain(脑裂) 这是由ZooKeeper的特性引起的，虽然ZooKeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致</p><p>2.herd effect(惊群) 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</p><p>3.ZooKeeper负载过重 每个Replica都要为此在ZooKeeper上注册一个Watch，当集群规模增加到几千个Partition时ZooKeeper负载会过重。</p><p>Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比ZooKeeper Queue的方式更高效）通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。</p><h3 id="4-broker保存消息"><a href="#4-broker保存消息" class="headerlink" title="4. broker保存消息"></a>4. broker保存消息</h3><h4 id="4-1-存储方式"><a href="#4-1-存储方式" class="headerlink" title="4.1 存储方式"></a>4.1 存储方式</h4><p>物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630212816733.png" alt="image-20200630212816733"></p><h4 id="4-2-存储策略"><a href="#4-2-存储策略" class="headerlink" title="4.2 存储策略"></a>4.2 存储策略</h4><p>无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：</p><blockquote><p>1、 基于时间：log.retention.hours=168<br>2、 基于大小：log.retention.bytes=1073741824</p></blockquote><h2 id="五-Kafka与其他常用MQ对比"><a href="#五-Kafka与其他常用MQ对比" class="headerlink" title="五. Kafka与其他常用MQ对比"></a>五. Kafka与其他常用MQ对比</h2><table><thead><tr><th></th><th>ActiveMQ</th><th>RabbitMQ</th><th>Kafka</th></tr></thead><tbody><tr><td>所属社区/公司</td><td>Apache</td><td>Mozikka Public License</td><td>Apache/LinkendIn</td></tr><tr><td>开发语言</td><td>Java</td><td>Erlang</td><td>Java</td></tr><tr><td>支持协议</td><td>OpenWire、STOMP、REST、XMPP、AMQP</td><td>AMQP</td><td>仿AMQP</td></tr><tr><td>事务</td><td>支持</td><td>支持（性能会下降）</td><td>支持</td></tr><tr><td>集群</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>负载均衡</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>动态扩容</td><td>不支持</td><td>不支持</td><td>支持（zk）</td></tr><tr><td>单机吞吐量TPS</td><td>万级</td><td>万级</td><td>十万级</td></tr><tr><td>顺序消息</td><td>不支持</td><td>不支持</td><td>支持</td></tr><tr><td>消息确认</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>消息回溯</td><td>不支持</td><td>不支持</td><td>支持指定分区offset位置的回溯</td></tr><tr><td>消息重试</td><td>不支持</td><td>不支持，但是可以利用消息确认机制实现</td><td>不支持，但是可以利用kafka支持指定分区offset位置的回溯，可以实现消息重试。</td></tr><tr><td>并发度</td><td>高</td><td>极高</td><td>高</td></tr></tbody></table><h2 id="六-SpringBoot整合Kafka"><a href="#六-SpringBoot整合Kafka" class="headerlink" title="六. SpringBoot整合Kafka"></a>六. SpringBoot整合Kafka</h2><h3 id="1-在创建好的gradle工程中引入依赖"><a href="#1-在创建好的gradle工程中引入依赖" class="headerlink" title="1. 在创建好的gradle工程中引入依赖"></a>1. 在创建好的gradle工程中引入依赖</h3><p>Gadle版本: 6.0</p><p>JDK: 1.8</p><p>IDEA: 2020.1</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dependencies</span> &#123;</span><br><span class="line">...</span><br><span class="line">    <span class="keyword">compile</span> <span class="keyword">group</span>: <span class="string">'org.springframework.boot'</span>, name: <span class="string">'spring-boot-starter-web'</span>, version:<span class="string">'2.2.0.RELEASE'</span></span><br><span class="line">    <span class="keyword">compile</span> <span class="keyword">group</span>: <span class="string">'org.springframework.kafka'</span>, name: <span class="string">'spring-kafka'</span>, version:<span class="string">'2.3.1.RELEASE'</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-编写配置文件"><a href="#2-编写配置文件" class="headerlink" title="2. 编写配置文件"></a>2. 编写配置文件</h3><p>配置文件有两种， 第一中是使用<code>application.yml</code>文件配置， 第二中是使用SpringBoot Java配置类来配置，两种配置如下</p><h4 id="applacation-yml"><a href="#applacation-yml" class="headerlink" title="applacation.yml"></a><code>applacation.yml</code></h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line"><span class="attr">  jackson:</span></span><br><span class="line"><span class="attr">    date-format:</span> <span class="string">yyyy-MM-dd</span> <span class="attr">HH:mm:ss</span>  <span class="comment">#日期序列化格式</span></span><br><span class="line"><span class="attr">  kafka:</span></span><br><span class="line"><span class="attr">    bootstrap-servers:</span> <span class="number">172.22</span><span class="number">.24</span><span class="number">.200</span><span class="string">:9092,</span> <span class="number">172.22</span><span class="number">.24</span><span class="number">.200</span><span class="string">:9093,</span> <span class="number">172.22</span><span class="number">.24</span><span class="number">.200</span><span class="string">:9094</span> <span class="comment"># 集群地址， 任意配一台可用地址即可</span></span><br><span class="line"><span class="attr">    producer:</span> <span class="comment"># 生产者配置</span></span><br><span class="line"><span class="attr">      retries:</span> <span class="number">0</span> <span class="comment"># 重试次数</span></span><br><span class="line"><span class="attr">      batch-size:</span> <span class="number">16384</span> <span class="comment"># 一次最多发送数据量</span></span><br><span class="line"><span class="attr">      buffer-memory:</span> <span class="number">33554432</span> <span class="comment"># 32M批处理缓冲区</span></span><br><span class="line"><span class="attr">      key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span> <span class="comment"># 序列化</span></span><br><span class="line"><span class="attr">      value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="attr">      linger:</span> <span class="comment"># 发送延迟</span></span><br><span class="line"><span class="attr">        ms:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">      acks:</span> <span class="string">"1"</span> <span class="comment"># 消息确认</span></span><br><span class="line"><span class="attr">    consumer:</span> <span class="comment"># 消费者配置</span></span><br><span class="line"><span class="attr">      group-id:</span> <span class="number">0</span> <span class="comment"># group-id</span></span><br><span class="line"><span class="attr">      enable-auto-commit:</span> <span class="literal">false</span> <span class="comment"># 是否开启自动提交</span></span><br><span class="line"><span class="attr">      auto-commit-interval:</span> <span class="number">100</span> <span class="comment"># consumer自动向zookeeper提交offset的频率</span></span><br><span class="line"><span class="attr">      properties:</span> <span class="comment"># 消费超时时间，大小不能超过session.timeout.ms，默认：3000</span></span><br><span class="line"><span class="attr">        session:</span></span><br><span class="line"><span class="attr">          timeout:</span></span><br><span class="line"><span class="attr">            ms:</span> <span class="number">15000</span></span><br><span class="line"><span class="attr">      key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span> <span class="comment"># 反序列化</span></span><br><span class="line"><span class="attr">      value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span> </span><br><span class="line"><span class="attr">      fetch-max-wait:</span> <span class="number">300000</span> <span class="comment"># 配置consumer最多等待response多久</span></span><br><span class="line"><span class="attr">      max-poll-records:</span> <span class="number">50</span> <span class="comment"># max.poll.records条数据需要在session.timeout.ms这个时间内处理完</span></span><br></pre></td></tr></table></figure><h4 id="KafkaConfig-java"><a href="#KafkaConfig-java" class="headerlink" title="KafkaConfig.java"></a><code>KafkaConfig.java</code></h4><p>注意: 配置写在yml和java类中均可, 但是某些工厂类如<code>KafkaTemplate</code>则需要在代码中使用<code>@Bean</code>注解交给<code>Spring</code>创建</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.config;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.EnableKafka;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.KafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.*;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.listener.ContainerProperties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Classname</span> KafkaConfig</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span></span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2020-06-29 17:12</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@EnableKafka</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.kafka.bootstrap-servers&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String hosts;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KafkaConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"kafka config init  -------------&gt;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * producer configuration</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> Map&lt;String, Object&gt; producerConfigs</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">producerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);</span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">0</span>);</span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1000</span>);</span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"1"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerFactory&lt;String, String&gt; <span class="title">producerFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * consumer configuration</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> Map&lt;String, Object&gt; consumerConfigs</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">consumerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">10</span>);</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"0"</span>);</span><br><span class="line">        <span class="comment">//自动控制提交offset</span></span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//提交延迟毫秒数</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">100</span>);</span><br><span class="line">        <span class="comment">//执行超时时间</span></span><br><span class="line">        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="string">"15000"</span>);</span><br><span class="line">        <span class="comment">// 每间隔max.poll.interval.ms我们就调用一次poll</span></span><br><span class="line">        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, <span class="string">"300000"</span>);</span><br><span class="line">        <span class="comment">// 一次poll最多返回的记录数</span></span><br><span class="line">        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, <span class="string">"50"</span>);</span><br><span class="line">        <span class="comment">//开始消费位置 earliest/latest/none</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"latest"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerFactory&lt;String, String&gt; <span class="title">consumerFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * kafka template configuration</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> KafkaTemplate&lt;String, String&gt; <span class="title">kafkaTemplate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> KafkaTemplate&lt;&gt;(producerFactory());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批量消费</span></span><br><span class="line"><span class="comment">     * MANUAL   当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> KafkaListenerContainerFactory&lt;?&gt; batchFactory(ConsumerFactory consumerFactory) &#123;</span><br><span class="line">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =</span><br><span class="line">                <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">        factory.setConsumerFactory(consumerFactory);</span><br><span class="line">        <span class="comment">// topic有5个分区，为了加快消费将并发设置为5，也就是有5个KafkaMessageListenerContainer</span></span><br><span class="line">        factory.setConcurrency(<span class="number">5</span>);</span><br><span class="line">        <span class="comment">// 设置拉取时间</span></span><br><span class="line">        factory.getContainerProperties().setPollTimeout(<span class="number">1500</span>);</span><br><span class="line">        <span class="comment">// 开启批量消费</span></span><br><span class="line">        factory.setBatchListener(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//配置手动提交offset</span></span><br><span class="line">  </span><br><span class="line">        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);</span><br><span class="line">        <span class="keyword">return</span> factory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="KafkaAdminConfig"><a href="#KafkaAdminConfig" class="headerlink" title="KafkaAdminConfig"></a><code>KafkaAdminConfig</code></h4><p>该文件可用来创建<code>topic</code>的相关操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.config;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.constant.MyTopic;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.AdminClientConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.NewTopic;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.EnableKafka;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaAdmin;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka admin config</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@EnableKafka</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaAdminConfig</span> </span>&#123;</span><br><span class="line">    <span class="comment">// yml配置文件中的变量</span></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.kafka.bootstrap-servers&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String hosts;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> KafkaAdmin <span class="title">admin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; configs = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">1</span>);</span><br><span class="line">        configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> KafkaAdmin(configs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic1</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">// 第一个是参数是topic名字，第二个参数是分区个数</span></span><br><span class="line">        <span class="comment">// 第三个是topic的复制因子个数</span></span><br><span class="line">        <span class="comment">// -----------------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;当broker个数为1个时会创建topic失败，</span></span><br><span class="line">        <span class="comment">//提示：replication factor: 2 larger than available brokers: 1</span></span><br><span class="line">        <span class="comment">//只有在集群中才能使用kafka的备份功能</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC1, <span class="number">5</span>, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic2</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC2, <span class="number">5</span>, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic3</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC3, <span class="number">5</span>, (<span class="keyword">short</span>) <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> NewTopic <span class="title">topic4</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NewTopic(MyTopic.TOPIC4, <span class="number">3</span>, (<span class="keyword">short</span>) <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-编写Kafka通用工具类"><a href="#3-编写Kafka通用工具类" class="headerlink" title="3. 编写Kafka通用工具类"></a>3. 编写Kafka通用工具类</h3><ol><li>可以先创建一个工具类接口, 然后再去实现这个接口</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javafx.util.Pair;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.TopicListing;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.springframework.util.concurrent.ListenableFuture;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020/6/20 0020</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Time</span>: 11:37</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: KafkaService</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg      数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Boolean <span class="title">sendDataToTopic</span><span class="params">(String topicName, String msg)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic和key中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAndKey(String topicName, String key, String msg);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic的中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic     topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition 分区名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       指定的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAppointPartition(String topic, Integer partition, String key, String msg);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 校验topic是否已经存在于kafka中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否存在的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Boolean <span class="title">isExistTopic</span><span class="params">(String topicName)</span></span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建指定的topic</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName         topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicPartition    话题创建的分区</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> replicationFactor 话题创建的副本， 不能大于broker的数量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否创建成功</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Boolean <span class="title">createTopic</span><span class="params">(String topicName, Integer topicPartition, <span class="keyword">short</span> replicationFactor)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除话题</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicNames 话题名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 删除结果</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    List&lt;Pair&lt;String, Boolean&gt;&gt; deleteTopic(String[] topicNames);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取所有的topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> topic集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">Map&lt;String, TopicListing&gt; <span class="title">getTopics</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>实现类</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.service.impl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.config.KafkaConfig;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.service.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> javafx.util.Pair;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.admin.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.KafkaFuture;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaAdmin;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"><span class="keyword">import</span> org.springframework.util.concurrent.ListenableFuture;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020/6/20 0020</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Time</span>: 20:20</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Kafka封装操作类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtilsImpl</span> <span class="keyword">implements</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaConfig kafkaConfig;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaAdmin kafkaAdmin;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTimeout</span><span class="params">(<span class="keyword">int</span> timeout)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.timeout = timeout;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> timeout = <span class="number">6000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">sendDataToTopic</span><span class="params">(String topicName, String msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; result = kafkaTemplate.send(topicName, msg);</span><br><span class="line">            result.get();</span><br><span class="line">            <span class="keyword">return</span> !result.completable().isCompletedExceptionally();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.info(<span class="string">"发送普通消息失败, topic=&#123;&#125;, msg=&#123;&#125;, failure Message=&#123;&#125;"</span>, topicName, msg, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic和key中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAndKey(String topicName, String key, String msg) &#123;</span><br><span class="line">        <span class="keyword">return</span> kafkaTemplate.send(topicName, key, msg);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送数据到指定的topic的中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic     topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition 分区名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key       指定的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAppointPartition(String topic, Integer partition, String key, String msg) &#123;</span><br><span class="line">        <span class="keyword">return</span> kafkaTemplate.send(topic, partition, key, msg);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 校验topic是否已经存在于kafka中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否存在的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">isExistTopic</span><span class="params">(String topicName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            ListTopicsOptions listTopicsOptions = <span class="keyword">new</span> ListTopicsOptions();</span><br><span class="line">            listTopicsOptions.listInternal(<span class="keyword">true</span>);</span><br><span class="line">            ListTopicsResult listTopicsResult = adminClient.listTopics(listTopicsOptions);</span><br><span class="line">            Boolean flag = listTopicsResult.names().get().contains(<span class="string">"topicName"</span>);</span><br><span class="line">            <span class="keyword">return</span> flag;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.info(<span class="string">"校验topic: &#123;&#125; 是否已经存在于kafka中异常 &#123;&#125;"</span>, topicName, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建指定的topic</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicName         topic的名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicPartition    话题创建的分区</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> replicationFactor 话题创建的副本， 不能大于broker的数量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 是否创建成功</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">createTopic</span><span class="params">(String topicName, Integer topicPartition, <span class="keyword">short</span> replicationFactor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            Boolean existTopic = isExistTopic(topicName);</span><br><span class="line">            <span class="keyword">if</span> (existTopic) &#123;</span><br><span class="line">                <span class="keyword">return</span> existTopic;</span><br><span class="line">            &#125;</span><br><span class="line">            NewTopic newTopic = <span class="keyword">new</span> NewTopic(topicName, topicPartition, replicationFactor);</span><br><span class="line">            List&lt;NewTopic&gt; newTopics = Collections.singletonList(newTopic);</span><br><span class="line">            adminClient.createTopics(newTopics);</span><br><span class="line">            <span class="keyword">return</span> isExistTopic(topicName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"创建话题&#123;&#125;失败, Cause by: &#123;&#125;"</span>, topicName, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定topic(如果broker那没有设置允许删除topic的话，此调用会持续等待最终超时返回)</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topicNames 待删除的topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 删除是否成功</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Pair&lt;String, Boolean&gt;&gt; deleteTopic(String[] topicNames) &#123;</span><br><span class="line">        List&lt;Pair&lt;String, Boolean&gt;&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            DeleteTopicsOptions options = <span class="keyword">new</span> DeleteTopicsOptions();</span><br><span class="line">            options.timeoutMs(timeout);</span><br><span class="line">            DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(Arrays.asList(topicNames), options);</span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;String, KafkaFuture&lt;Void&gt;&gt; e : deleteTopicsResult.values().entrySet()) &#123;</span><br><span class="line">                String topic = e.getKey();</span><br><span class="line">                KafkaFuture&lt;Void&gt; future = e.getValue();</span><br><span class="line">                future.get();</span><br><span class="line">                result.add(<span class="keyword">new</span> Pair&lt;&gt;(topic, !future.isCompletedExceptionally()));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"删除话题&#123;&#125;失败, Cause by: &#123;&#125;"</span>, String.join(<span class="string">","</span>, topicNames), e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取所有的topic</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> topic集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, TopicListing&gt; <span class="title">getTopics</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ListTopicsOptions options = <span class="keyword">new</span> ListTopicsOptions();</span><br><span class="line">        <span class="comment">//设置超时时间</span></span><br><span class="line">        options.timeoutMs(timeout);</span><br><span class="line">        <span class="comment">//不列出kafka内部topic</span></span><br><span class="line">        options.listInternal(<span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">try</span> (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123;</span><br><span class="line">            ListTopicsResult listTopicsResult = adminClient.listTopics(options);</span><br><span class="line">            <span class="keyword">return</span> listTopicsResult.namesToListings().get();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"查询话题失败失败, Cause by: &#123;&#125;"</span>, e.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注: @Slf4j注解是集成了lombok后可以方便打印日志使用的</p><h3 id="4-编写Kafka-producer"><a href="#4-编写Kafka-producer" class="headerlink" title="4. 编写Kafka producer"></a>4. 编写Kafka producer</h3><p>通用工具类编写好后, 可以独立出专门用来发送消息的<code>producer</code>类, 代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.constant.MyTopic;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.entity.KafkaMessage;</span><br><span class="line"><span class="keyword">import</span> com.gsafety.springbootkafka.service.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"><span class="keyword">import</span> org.springframework.util.concurrent.ListenableFuture;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Objects;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CompletableFuture;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Classname</span> KafkaProducer</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> 生产者</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2020-06-30 11:00</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducer</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaUtils kafkaUtils;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, String key, KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">        String msg = JSON.toJSONString(kafkaMessage);</span><br><span class="line">        kafkaUtils.sendDataToTopicAndKey(topic, key, msg).addCallback(success -&gt; &#123;</span><br><span class="line">            <span class="comment">// 消息发送到的topic</span></span><br><span class="line">            String successTopic = Objects.requireNonNull(success).getRecordMetadata().topic();</span><br><span class="line">            <span class="comment">// 消息发送到的分区</span></span><br><span class="line">            <span class="keyword">int</span> partition = success.getRecordMetadata().partition();</span><br><span class="line">            <span class="comment">// 消息在分区内的offset</span></span><br><span class="line">            <span class="keyword">long</span> offset = success.getRecordMetadata().offset();</span><br><span class="line">            log.info(<span class="string">"发送普通消息, topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, msg);</span><br><span class="line">        &#125;, failure -&gt; &#123;</span><br><span class="line">            log.info(<span class="string">"发送普通消息失败, topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;, failure Message=&#123;&#125;"</span>, topic, key, msg, failure.getMessage());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">send</span><span class="params">(String topic, Integer partition, String key, KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 或者 JSON.toJSONString(kafkaMessage, SerializerFeature.WriteDateUseDateFormat);</span></span><br><span class="line">        String msg = JSON.toJSONStringWithDateFormat(kafkaMessage, <span class="string">"yyyy-MM-dd HH:mm:ss"</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendResultListenableFuture = kafkaUtils.sendDataToTopicAppointPartition(topic, partition, key, msg);</span><br><span class="line">            log.info(<span class="string">"发送普通消息，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, msg);</span><br><span class="line">            CompletableFuture&lt;SendResult&lt;String, String&gt;&gt; completable = sendResultListenableFuture.completable();</span><br><span class="line">            completable.get();</span><br><span class="line">            <span class="keyword">return</span> !completable.isCompletedExceptionally();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"发送普通消息失败，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, msg);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, Integer partition, String key, String kafkaMessage)</span> </span>&#123;</span><br><span class="line">        kafkaUtils.sendDataToTopicAppointPartition(topic, partition, key, kafkaMessage);</span><br><span class="line">        log.info(<span class="string">"发送普通消息，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;"</span>, topic, key, kafkaMessage);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>首先将<code>KafkaUtils</code>自动注入。</li><li>然后就可以使用<code>KafkaUtils</code>中提供的<code>API</code>按照自己的需求进行二次封装，实现自己想要的逻辑处理。</li><li>发送消息后，可以通过其<code>addCallback</code>方法来处理发送成功或者失败后的逻辑，或者接收<code>ListenableFuture</code>类型的返回值并且使用<code>try-catch</code>来作逻辑判断，上述两种方式在代码中均有体现。</li></ol><h3 id="5-编写Kafka-consumer"><a href="#5-编写Kafka-consumer" class="headerlink" title="5. 编写Kafka consumer"></a>5. 编写Kafka consumer</h3><p><code>consumer</code>主要利用<code>SpringBoot</code> 提供的<code>@KafkaListener</code>注解来实现的。下面先来简单介绍一下<code>@KafkaListener</code>注解的相关内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="meta">@Target</span>(&#123; ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE &#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@MessageMapping</span></span><br><span class="line"><span class="meta">@Documented</span></span><br><span class="line"><span class="meta">@Repeatable</span>(KafkaListeners.class)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> KafkaListener &#123;</span><br><span class="line"><span class="comment">// 消费者的id（唯一），当GroupId没有被配置的时候，默认id为GroupId，支持SpEL表达式#&#123;&#125;</span></span><br><span class="line"><span class="function">String <span class="title">id</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line"><span class="comment">// 这里面配置的是监听容器工厂BeanName，常用于批量消费时指定消费工厂</span></span><br><span class="line"><span class="function">String <span class="title">containerFactory</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line"><span class="comment">// 需要监听的Topic，可监听多个</span></span><br><span class="line">String[] topics() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"><span class="comment">// 可配置更加详细的监听信息，必须监听某个Topic中的指定分区，或者从offset为200的偏移量开始监听</span></span><br><span class="line">TopicPartition[] topicPartitions() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"><span class="comment">// 监听异常处理器，配置BeanName</span></span><br><span class="line"><span class="function">String <span class="title">errorHandler</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line"><span class="comment">// 消费组ID</span></span><br><span class="line"><span class="function">String <span class="title">groupId</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line"><span class="comment">// id是否为GroupId</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">idIsGroup</span><span class="params">()</span> <span class="keyword">default</span> <span class="keyword">true</span></span>;</span><br><span class="line"><span class="comment">// 消费者Id前缀</span></span><br><span class="line"><span class="function">String <span class="title">clientIdPrefix</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line"><span class="comment">// 真实监听容器的BeanName，需要在 BeanName前加 "__"</span></span><br><span class="line"><span class="function">String <span class="title">beanRef</span><span class="params">()</span> <span class="keyword">default</span> "__listener"</span>;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现<code>consumer</code>类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.gsafety.springbootkafka.consumer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.PartitionOffset;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.Acknowledgment;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Optional;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lg</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Classname</span> KafkaConsumer</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> 消费者</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019-11-06 17:01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）;</span></span><br><span class="line"><span class="comment">     * （1）使用一个全新的"group.id"（就是之前没有被任何消费者使用过）;</span></span><br><span class="line"><span class="comment">     * （2）指定"auto.offset.reset"参数的值为earliest;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> content 消息内容</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-1"</span>, topics = <span class="string">"topic4"</span>, groupId = <span class="string">"group2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage2</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"消费者topic4-1监听消息,消息内容=[&#123;&#125;]"</span>, content);s</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-2"</span>, topics = <span class="string">"topic4"</span>, groupId = <span class="string">"group2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage3</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"消费者topic4-2监听消息,消息内容=[&#123;&#125;]"</span>, content);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批量消费</span></span><br><span class="line"><span class="comment">     * containerFactory: 需要声明消费工厂名</span></span><br><span class="line"><span class="comment">     * batchFactory： 在KafakaConfig中配置的消费者工厂类</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ack 消息确认对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> records 消息内容</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-3"</span>, topics = <span class="string">"topic3"</span>, groupId = <span class="string">"group1"</span>,</span><br><span class="line">            containerFactory = <span class="string">"batchFactory"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"client-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;"</span>, Thread.currentThread().getId(), records.size());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">                Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value());</span><br><span class="line">                <span class="keyword">if</span> (kafkaMessage.isPresent()) &#123;</span><br><span class="line">                    Object message = record.value();</span><br><span class="line">                    String topic = record.topic();</span><br><span class="line">                    <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">                    log.info(<span class="string">"client-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]"</span>, topic, offset, message);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 手动提交，设置offset</span></span><br><span class="line">            ack.acknowledge();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">"client-3监听异常&#123;&#125;"</span>, e.getMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * id是消费者监听容器</span></span><br><span class="line"><span class="comment">     * 配置topic和分区：监听两个topic，分别为topic1、topic2，topic1只接收分区0，3的消息，</span></span><br><span class="line"><span class="comment">     * topic2接收分区0和分区1的消息，但是分区1的消费者初始位置为5</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> record 消费内容</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-4"</span>, clientIdPrefix = <span class="string">"my"</span>,</span><br><span class="line">            topicPartitions =</span><br><span class="line">                    &#123;<span class="meta">@TopicPartition</span>(topic = <span class="string">"topic1"</span>, partitions = &#123;<span class="string">"0"</span>, <span class="string">"3"</span>&#125;),</span><br><span class="line">                            <span class="meta">@TopicPartition</span>(topic = <span class="string">"topic2"</span>, partitions = <span class="string">"0"</span>,</span><br><span class="line">                                    partitionOffsets = <span class="meta">@PartitionOffset</span>(partition = <span class="string">"1"</span>, initialOffset = <span class="string">"4"</span>))</span><br><span class="line">                    &#125;)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"topic1消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"client-5"</span>, topics = &#123;<span class="string">"topic1"</span>, <span class="string">"topic2"</span>&#125;)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen2</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"topic1,topic2 多主题消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，我们的生产者和消费者就已经都编写好了，至于要简单的编写一下单元测试或者<code>Controller</code>实现<code>RESTful API</code>就可以开始验证和简单的使用<code>Kafka</code>了。</p><h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">30</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">20.332</span>  INFO <span class="number">20320</span> --- [  topic-<span class="number">3</span>-<span class="number">2</span>-C-<span class="number">1</span>] o.s.k.l.KafkaMessageListenerContainer    : group1: partitions assigned: [topic3-<span class="number">4</span>]</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">30</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">20.336</span>  INFO <span class="number">20320</span> --- [  topic-<span class="number">3</span>-<span class="number">1</span>-C-<span class="number">1</span>] o.s.k.l.KafkaMessageListenerContainer    : group1: partitions assigned: [topic3-<span class="number">3</span>, topic3-<span class="number">2</span>]</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">30</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">20.336</span>  INFO <span class="number">20320</span> --- [  topic-<span class="number">3</span>-<span class="number">0</span>-C-<span class="number">1</span>] o.s.k.l.KafkaMessageListenerContainer    : group1: partitions assigned: [topic3-<span class="number">1</span>, topic3-<span class="number">0</span>]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>在消费工厂或者配置中设置并发量，小于或等于Topic的分区数<br><code>factory.setConcurrency(3);</code></p><p>我们设置concurrency为3，也就是将会启动3条线程进行监听，我们创建的topic则是有5个partition，意味着将有2条线程分配到2个partition和1条线程分配到1个partition。我们可以看到这段日志的最后3行，这就是每条线程分配到的partition.<br>注意：设置的并发量不能大于partition的数量，如果需要提高吞吐量，可以通过增加partition的数量达到快速提升吞吐量的效果。</p><h4 id="1-简单发送-订阅"><a href="#1-简单发送-订阅" class="headerlink" title="1. 简单发送-订阅"></a>1. 简单发送-订阅</h4><p>发送：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送数据到指定的topic的中</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic     topic名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partition 分区名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> key       指定的key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> msg       消息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 发送状态</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, String key, KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">    String msg = JSON.toJSONString(kafkaMessage);</span><br><span class="line">    kafkaUtils.sendDataToTopicAndKey(topic, key, msg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>订阅：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）;</span></span><br><span class="line"><span class="comment"> * （1）使用一个全新的"group.id"（就是之前没有被任何消费者使用过）;</span></span><br><span class="line"><span class="comment"> * （2）指定"auto.offset.reset"参数的值为earliest;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> content 消息内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic4-1"</span>, topics = <span class="string">"topic4"</span>, groupId = <span class="string">"group2"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage2</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"消费者topic4-1监听消息,消息内容=[&#123;&#125;]"</span>, content);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>控制台结果：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630161218434.png" alt="image-20200630161218434"></p><h4 id="2-批量消费"><a href="#2-批量消费" class="headerlink" title="2. 批量消费"></a>2. 批量消费</h4><ol><li><p>重新创建一份新的消费者配置，配置为一次拉取5条消息</p></li><li><p>创建一个监听容器工厂，设置其为批量消费并设置并发量为5，这个并发量根据分区数决定，必须小于等于分区数，否则会有线程一直处于空闲状态</p></li><li><p>创建一个分区数为5的Topic</p></li><li><p>创建监听方法，设置消费<code>id</code>为<code>batch</code>，<code>clientID</code>前缀为<code>batch</code>，监听<code>topic3</code>，使用<code>batchFactory</code>工厂创建该监听容器</p></li></ol><p>该方法在<code>KafkaConfig.java</code>中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; <span class="title">batchFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =</span><br><span class="line">            <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">    factory.setConsumerFactory(<span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()));</span><br><span class="line">    <span class="comment">//设置并发量，小于或等于Topic的分区数</span></span><br><span class="line">    factory.setConcurrency(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 开启批量消费</span></span><br><span class="line">    factory.setBatchListener(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> factory;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>生产者代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@ApiOperation</span>(value = <span class="string">"向Topic3中发送消息"</span>, notes=<span class="string">"向Topic3中发送消息"</span>)</span><br><span class="line"><span class="meta">@PostMapping</span>(path = <span class="string">"/topic3"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> ResponseEntity&lt;Boolean&gt; <span class="title">sendTopic3</span><span class="params">(@RequestBody KafkaMessage kafkaMessage)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        flag = kafkaProducer.send(MyTopic.TOPIC3, <span class="number">2</span>, <span class="string">"topic.*"</span>, kafkaMessage);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ResponseEntity.ok(flag);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>消费者代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * containerFactory: 需要声明消费工厂名</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> records 消息内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic-3"</span>, topics = <span class="string">"topic3"</span>, groupId = <span class="string">"group1"</span>,</span><br><span class="line">        containerFactory = <span class="string">"batchFactory"</span>, clientIdPrefix = <span class="string">"batch"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;"</span>, Thread.currentThread().getId(), records.size());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">            Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value());</span><br><span class="line">            <span class="keyword">if</span> (kafkaMessage.isPresent()) &#123;</span><br><span class="line">                Object message = record.value();</span><br><span class="line">                String topic = record.topic();</span><br><span class="line">                <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">                log.info(<span class="string">"topic-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]"</span>, topic, offset, message);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">"topic-3监听异常&#123;&#125;"</span>, e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>控制台结果：max.poll.records设置为5（一次poll最多返回的记录数）</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630165743413.png" alt="image-20200630165743413"></p><h4 id="3-确认机制"><a href="#3-确认机制" class="headerlink" title="3. 确认机制"></a>3. 确认机制</h4><p>与<a href="">2. 批量消费</a>的代码相似，使用Kafka的Ack机制比较简单，只需简单的三步即可：</p><ol><li>设置ENABLE_AUTO_COMMIT_CONFIG=false，禁止自动提交</li><li>设置AckMode=MANUAL_IMMEDIATE</li><li>监听方法加入Acknowledgment ack 参数</li></ol><p>Kafka是通过最新保存偏移量进行消息消费的，而且确认消费的消息并不会立刻删除，所以我们可以重复的消费未被删除的数据，当第一条消息未被确认，而第二条消息被确认的时候，Kafka会保存第二条消息的偏移量，也就是说第一条消息再也不会被监听器所获取，除非是根据第一条消息的偏移量手动获取。</p><p>拒绝消息只要在监听方法中不调用ack.acknowledge()即可</p><p>配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; <span class="title">batchFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =</span><br><span class="line">            <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">    factory.setConsumerFactory(<span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()));</span><br><span class="line">    <span class="comment">//设置并发量，小于或等于Topic的分区数</span></span><br><span class="line">    factory.setConcurrency(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 开启批量消费</span></span><br><span class="line">    factory.setBatchListener(<span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//配置手动提交offset</span></span><br><span class="line">factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);</span><br><span class="line">    <span class="keyword">return</span> factory;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * consumer configuration</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Map&lt;String, Object&gt; consumerConfigs</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">consumerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">10</span>);</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//自动控制提交offset,注意此处设置自动提交为false的意思时offset从由kafka自动提交转为由Spring自动提交了，实现真正的手动提交还需要在消费工厂类中配合factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);或者kafka-listener-ack-mode: manual 参数配置， 才能实现真正的手动提交</span></span><br><span class="line">    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> props;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>消费者代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量消费</span></span><br><span class="line"><span class="comment"> * containerFactory: 需要声明消费工厂名</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> records 消息内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic-3"</span>, topics = <span class="string">"topic3"</span>, groupId = <span class="string">"group1"</span>,</span><br><span class="line">        containerFactory = <span class="string">"batchFactory"</span>, clientIdPrefix = <span class="string">"batch"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMessage</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;"</span>, Thread.currentThread().getId(), records.size());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">            Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value());</span><br><span class="line">            <span class="keyword">if</span> (kafkaMessage.isPresent()) &#123;</span><br><span class="line">                Object message = record.value();</span><br><span class="line">                String topic = record.topic();</span><br><span class="line">                <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">                log.info(<span class="string">"topic-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]"</span>, topic, offset, message);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 手动提交，设置offset, 确认消息被消费</span></span><br><span class="line">        ack.acknowledge();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">"topic-3监听异常&#123;&#125;"</span>, e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编写测试方法，运行后可以方法监听方法能收到消息，紧接着注释ack.acknowledge()方法，重新测试，同样你会发现监听容器能接收到消息，这个时候如果你重启项目还是可以看到未被确认的那几条消息。</p><h4 id="4-多主题订阅"><a href="#4-多主题订阅" class="headerlink" title="4. 多主题订阅"></a>4. 多主题订阅</h4><p>消费者：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic1-2"</span>, topics = &#123;<span class="string">"topic1"</span>, <span class="string">"topic2"</span>&#125;)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen2</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic1, topic2 多主题消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174523010.png" alt="image-20200630174523010"></p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174450976.png" alt="image-20200630174450976"></p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174734251.png" alt="image-20200630174734251"></p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630174708339.png" alt="image-20200630174708339"></p><h4 id="5-多主题指定分区指定偏移量订阅"><a href="#5-多主题指定分区指定偏移量订阅" class="headerlink" title="5. 多主题指定分区指定偏移量订阅"></a>5. 多主题指定分区指定偏移量订阅</h4><p>消费者：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * id是消费者监听容器</span></span><br><span class="line"><span class="comment"> * 配置topic和分区：监听两个topic，分别为topic1、topic2，topic1只接收分区0，3的消息，</span></span><br><span class="line"><span class="comment"> * topic2接收分区0和分区1的消息，但是分区1的消费者初始位置为5</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> record 消费内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener</span>(id = <span class="string">"topic1-topic2"</span>, clientIdPrefix = <span class="string">"my"</span>,</span><br><span class="line">        topicPartitions =</span><br><span class="line">                &#123;</span><br><span class="line">                        <span class="meta">@TopicPartition</span>(topic = <span class="string">"topic1"</span>, partitions = &#123;<span class="string">"0"</span>, <span class="string">"3"</span>&#125;),</span><br><span class="line">                        <span class="meta">@TopicPartition</span>(topic = <span class="string">"topic2"</span>, partitions = <span class="string">"0"</span>,</span><br><span class="line">                                partitionOffsets = <span class="meta">@PartitionOffset</span>(partition = <span class="string">"1"</span>, initialOffset = <span class="string">"4"</span>))</span><br><span class="line">                &#125;)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"topic1-topic2消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;"</span>, record.topic(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>控制台：</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630185143846.png" alt="image-20200630185143846"></p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630185123707.png" alt="image-20200630185123707"></p><p>向topic1的分区2中发送消息，<code>topic1-topic2</code>未监听到，因为我们只监听了topic1的 0 3分区，topic2的 0 1分区。</p><p>自定义<code>offset</code>：<code>auto.offset.reset=&quot;earliest&quot;</code>时会从设置的<code>initialOffset</code>开始消费</p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630190157507.png" alt="image-20200630190157507"></p><p><img src="//liugan96.top/2020/07/07/高性能消息中间件-kafka/image-20200630191425762.png" alt="image-20200630191425762"></p><h2 id="七-扩展"><a href="#七-扩展" class="headerlink" title="七. 扩展"></a>七. 扩展</h2><h3 id="1-Kafka常用命令"><a href="#1-Kafka常用命令" class="headerlink" title="1. Kafka常用命令"></a>1. Kafka常用命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动kafka服务，集群则在不同服务器上各自执行此命令</span></span><br><span class="line">./bin/kafka-server-start.sh config/server.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建主题（4个分区，2个副本）</span></span><br><span class="line">./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看服务器中所有的topic</span></span><br><span class="line">./kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看集群描述</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果说明：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这是输出的解释。第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。由于我们只有一个分 区用于此主题，因此只有一行。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> “leader”是负责给定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。（因为在kafka中 如果有多个副本的话，就会存在leader和follower的关系，表示当前这个副本为leader所在的broker是哪一个） </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> “replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。（所有副本列表0,1,2） </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> “isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。（可用的列表数）</span></span><br><span class="line">./kafka-topics.sh --describe --zookeeper localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看某个topic详情</span></span><br><span class="line">./kafka-topics.sh --topic &lt;topic-name&gt; --describe --zookeeper localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除某个topic</span></span><br><span class="line">./kafka-topics.sh --delete --zookeeper localhost:2181 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示某个消费组的消费详情（0.10.1.0版本+）</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group &lt;group-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 新消费者列表查询（支持0.10版本+）</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动生产者 发送消息</span></span><br><span class="line">./kafka-console-producer.sh --broker-list localhost:9092 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动消费者 接收消息</span></span><br><span class="line"> ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic &lt;topic-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 高级点的用法</span></span><br><span class="line">./kafka-simple-consumer-shell.sh --brist localhost:9092 --topic test --partition 0 --offset 1234  --max-messages 10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看消费组详情</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group &lt;group-name&gt; --describe</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询topic的最大（小）offset 最大：--time -1 最小： --time -2</span></span><br><span class="line">./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic &lt;topic-name&gt; --time -1</span><br></pre></td></tr></table></figure><p>zookeeper:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 连接 zookeeper</span></span><br><span class="line">./zookeeper-shell.sh 127.0.0.1:2181</span><br><span class="line"></span><br><span class="line">ls /</span><br><span class="line">[zookeeper, brokers]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看broker的id</span></span><br><span class="line">ls /brokers/ids</span><br><span class="line">[3, 2, 0]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看消息</span></span><br><span class="line">ls /brokers/topics</span><br><span class="line">[new, __consumer_offsets, test]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看删除的消息</span></span><br><span class="line">ls /admin/delete_topics</span><br><span class="line">[account]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看broker的信息</span></span><br><span class="line">get /brokers/ids/0</span><br><span class="line">&#123;"jmx_port":-1,"timestamp":"1516184048700","endpoints":["PLAINTEXT://hs01:9092"],"host":"hs01","version":2,"port":9092&#125;</span><br><span class="line">cZxid = 0x29000048b2</span><br><span class="line">ctime = Wed Jan 17 18:14:08 CST 2018</span><br><span class="line">mZxid = 0x29000048b2</span><br><span class="line">mtime = Wed Jan 17 18:14:08 CST 2018</span><br><span class="line">pZxid = 0x29000048b2</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x261033b8886032a</span><br><span class="line">dataLength = 119</span><br><span class="line">numChildren = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删除消息</span></span><br><span class="line">rmr /brokers/topics/test</span><br><span class="line">rmr /admin/delete_topics/test</span><br></pre></td></tr></table></figure><h3 id="2-Kafka管理工具"><a href="#2-Kafka管理工具" class="headerlink" title="2. Kafka管理工具"></a>2. Kafka管理工具</h3><ul><li><p>KafkaOffsetMonitor：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。</p></li><li><p>Kafka Web Console：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。</p></li><li><p>Kafka Manager：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。</p></li></ul><h3 id="3-Spring-Boot-Kafka参数配置"><a href="#3-Spring-Boot-Kafka参数配置" class="headerlink" title="3. Spring Boot Kafka参数配置"></a>3. Spring Boot Kafka参数配置</h3><ul><li><p>生产者</p><ul><li>重要配置</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 高优先级配置</span><br><span class="line"># 以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接</span><br><span class="line">spring.kafka.producer.bootstrap-servers=TopKafka1:9092,TopKafka2:9092,TopKafka3:9092</span><br><span class="line"> </span><br><span class="line"># 设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。</span><br><span class="line">spring.kafka.producer.retries=0</span><br><span class="line"> </span><br><span class="line"># 每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求，</span><br><span class="line"># 这有助于提升客户端和服务端之间的性能，此配置控制默认批量大小（以字节为单位），默认值为16384</span><br><span class="line">spring.kafka.producer.batch-size=16384</span><br><span class="line"> </span><br><span class="line"># producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。这项设置将和producer能够使用的总内存相关，但并不是一个硬性的限制，因为不是producer使用的所有内存都是用于缓存。一些额外的内存会用于压缩（如果引入压缩机制），同样还有一些用于维护请求。</span><br><span class="line">spring.kafka.producer.buffer-memory=33554432</span><br><span class="line"> </span><br><span class="line"># key的Serializer类，实现了org.apache.kafka.common.serialization.Serializer接口</span><br><span class="line">spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line"> </span><br><span class="line"># 值的Serializer类，实现了org.apache.kafka.common.serialization.Serializer接口</span><br><span class="line">spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line"> </span><br><span class="line"># procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：</span><br><span class="line"># acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。</span><br><span class="line"># acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。</span><br><span class="line"># acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。</span><br><span class="line"># 可以设置的值为：all, -1, 0, 1</span><br><span class="line">spring.kafka.producer.acks=-1</span><br><span class="line"> </span><br><span class="line"># 当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪</span><br><span class="line">spring.kafka.producer.client-id=1</span><br><span class="line"> </span><br><span class="line"># producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。压缩最好用于批量处理，批量处理消息越多，压缩性能越好</span><br><span class="line">spring.kafka.producer.compression-type=none</span><br></pre></td></tr></table></figure><ul><li>其他配置</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># 中优先级配置</span><br><span class="line"># 以毫秒为单位的时间，是在我们强制更新metadata的时间间隔。即使我们没有看到任何partition leadership改变。默认值：5 * 60 * 1000 = 300000</span><br><span class="line">spring.kafka.producer.properties.metadata.max.age.ms=300000</span><br><span class="line"> </span><br><span class="line"># producer组将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。通常来说，这只有在记录产生速度大于发送速度的时候才能发生。然而，在某些条件下，客户端将希望降低请求的数量，甚至降低到中等负载一下。这项设置将通过增加小的延迟来完成–即，不是立即发送一条记录，producer将会等待给定的延迟时间以允许其他消息记录发送，这些消息记录可以批量处理。这可以认为是TCP种Nagle的算法类似。这项设置设定了批量处理的更高的延迟边界：一旦我们获得某个partition的batch.size，他将会立即发送而不顾这项设置，然而如果我们获得消息字节数比这项设置要小的多，我们需要“linger”特定的时间以获取更多的消息。 这个设置默认为0，即没有延迟。设定linger.ms=5，例如，将会减少请求数目，但是同时会增加5ms的延迟。</span><br><span class="line">spring.kafka.producer.properties.linger.ms=0</span><br><span class="line"> </span><br><span class="line"># 发送数据时的缓存空间大小，默认：128 * 1024 = 131072</span><br><span class="line">spring.kafka.producer.properties.send.buffer.bytes=131072</span><br><span class="line"> </span><br><span class="line"># socket的接收缓存空间大小,当阅读数据时使用，默认：32 * 1024 = 32768</span><br><span class="line">spring.kafka.producer.properties.receive.buffer.bytes=32768</span><br><span class="line"> </span><br><span class="line"># 请求的最大字节数。这也是对最大记录尺寸的有效覆盖。注意：server具有自己对消息记录尺寸的覆盖，这些尺寸和这个设置不同。此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。默认：1 * 1024 * 1024 = 1048576</span><br><span class="line">spring.kafka.producer.properties.max.request.size=1048576</span><br><span class="line"> </span><br><span class="line"># 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连，默认值：50</span><br><span class="line">spring.kafka.producer.properties.reconnect.backoff.ms=50</span><br><span class="line"> </span><br><span class="line"># producer客户端连接一个kafka服务（broker）失败重连的总时间，每次连接失败，重连时间都会指数级增加，每次增加的时间会存在20%的随机抖动，以避免连接风暴。默认：1000</span><br><span class="line"># spring.kafka.producer.properties.reconnect.backoff.max.ms=1000</span><br><span class="line"> </span><br><span class="line"># 控制block的时长,当buffer空间不够或者metadata丢失时产生block，默认：60 * 1000 = 60000</span><br><span class="line">spring.kafka.producer.properties.max.block.ms=60000</span><br><span class="line"> </span><br><span class="line"># 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中，默认：100</span><br><span class="line">spring.kafka.producer.properties.retry.backoff.ms=100</span><br><span class="line"> </span><br><span class="line"># metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口退出后，我们会擦除并重写最老的窗口，默认：30000</span><br><span class="line">spring.kafka.producer.properties.metrics.sample.window.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用于维护metrics的样本数，默认：2</span><br><span class="line">spring.kafka.producer.properties.metrics.num.samples=2</span><br><span class="line"> </span><br><span class="line"># 用于metrics的最高纪录等级。</span><br><span class="line"># spring.kafka.producer.properties.metrics.recording.level=Sensor.RecordingLevel.INFO.toString()</span><br><span class="line"> </span><br><span class="line"># 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计</span><br><span class="line">#spring.kafka.producer.properties.metric.reporters=Collections.emptyList()</span><br><span class="line"> </span><br><span class="line"># kafka可以在一个connection中发送多个请求，叫作一个flight,这样可以减少开销，但是如果产生错误，可能会造成数据的发送顺序改变,默认是5 (修改）</span><br><span class="line">spring.kafka.producer.properties.max.in.flight.requests.per.connection=5</span><br><span class="line"> </span><br><span class="line"># 关闭连接空闲时间，默认：9 * 60 * 1000 = 540000</span><br><span class="line">spring.kafka.producer.properties.connections.max.idle.ms=540000</span><br><span class="line"> </span><br><span class="line"># 分区类，默认：org.apache.kafka.clients.producer.internals.DefaultPartitioner</span><br><span class="line">spring.kafka.producer.properties.partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner</span><br><span class="line"> </span><br><span class="line"># 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认：30 * 1000 = 30000</span><br><span class="line">spring.kafka.producer.properties.request.timeout.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用户自定义interceptor。</span><br><span class="line">#spring.kafka.producer.properties.interceptor.classes=none</span><br><span class="line"> </span><br><span class="line"># 是否使用幂等性。如果设置为true，表示producer将确保每一条消息都恰好有一份备份；如果设置为false，则表示producer因发送数据到broker失败重试使，可能往数据流中写入多分重试的消息。</span><br><span class="line">#spring.kafka.producer.properties.enable.idempotence=false</span><br><span class="line"> </span><br><span class="line"># 在主动中止正在进行的事务之前，事务协调器将等待生产者的事务状态更新的最长时间（以ms为单位）。</span><br><span class="line">#spring.kafka.producer.properties.transaction.timeout.ms=60000</span><br><span class="line"> </span><br><span class="line"># 用于事务传递的TransactionalId。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的TransactionalId的事务已经完成。 如果没有提供TransactionalId，则生产者被限制为幂等传递。请注意，如果配置了TransactionalId，则必须启用enable.idempotence。默认值为空，这意味着无法使用事务。</span><br><span class="line">#spring.kafka.producer.properties.transactional.id=null</span><br><span class="line"></span><br><span class="line"># 连接风暴</span><br><span class="line"></span><br><span class="line">#应用启动的时候，经常可能发生各应用服务器的连接数异常飙升的情况。假设连接数的设置为：min值3,max值10，正常的业务使用连接数在5个左右，当重启应用时，各应用连接数可能会飙升到10个，瞬间甚至还有可能部分应用会报取不到连接。启动完成后接下来的时间内，连接开始慢慢返回到业务的正常值。这就是所谓的连接风暴。</span><br></pre></td></tr></table></figure></li><li><p>消费者</p><ul><li>重要配置</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># 以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接</span><br><span class="line">spring.kafka.consumer.bootstrap-servers=TopKafka1:9092,TopKafka2:9092,TopKafka3:9092</span><br><span class="line"> </span><br><span class="line"># 用来唯一标识consumer进程所在组的字符串，如果设置同样的group id，表示这些processes都是属于同一个consumer group，默认：&quot;&quot;</span><br><span class="line">spring.kafka.consumer.group-id=TyyLoveZyy</span><br><span class="line"> </span><br><span class="line"># max.poll.records条数据需要在session.timeout.ms这个时间内处理完，默认：500</span><br><span class="line">spring.kafka.consumer.max-poll-records=500</span><br><span class="line"> </span><br><span class="line"># 消费超时时间，大小不能超过session.timeout.ms，默认：3000</span><br><span class="line">spring.kafka.consumer.heartbeat-interval=3000</span><br><span class="line"> </span><br><span class="line"># 如果为真，consumer所fetch的消息的offset将会自动的同步到zookeeper。这项提交的offset将在进程挂掉时，由新的consumer使用，默认：true</span><br><span class="line">spring.kafka.consumer.enable-auto-commit=true</span><br><span class="line"> </span><br><span class="line"># consumer自动向zookeeper提交offset的频率，默认：5000</span><br><span class="line">spring.kafka.consumer.auto-commit-interval=5000</span><br><span class="line"> </span><br><span class="line"># 没有初始化的offset时，可以设置以下三种情况：（默认：latest）</span><br><span class="line"># earliest</span><br><span class="line"># 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费</span><br><span class="line"># latest</span><br><span class="line"># 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</span><br><span class="line"># none</span><br><span class="line"># topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</span><br><span class="line">spring.kafka.consumer.auto-offset-reset=earliest</span><br><span class="line"> </span><br><span class="line"># 每次fetch请求时，server应该返回的最小字节数。如果没有足够的数据返回，请求会等待，直到足够的数据才会返回。默认：1</span><br><span class="line">spring.kafka.consumer.fetch-min-size=1</span><br><span class="line"> </span><br><span class="line"># Fetch请求发给broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长。这个配置就是来配置consumer最多等待response多久。</span><br><span class="line">spring.kafka.consumer.fetch-max-wait=500</span><br><span class="line"> </span><br><span class="line"># 消费者进程的标识。如果设置一个人为可读的值，跟踪问题会比较方便。。默认：&quot;&quot;</span><br><span class="line">spring.kafka.consumer.client-id=1</span><br><span class="line"> </span><br><span class="line"># key的反序列化类。实现了org.apache.kafka.common.serialization.Deserializer接口</span><br><span class="line">spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"> </span><br><span class="line"># 值的反序列化类。实现了org.apache.kafka.common.serialization.Deserializer接口</span><br><span class="line">spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer</span><br></pre></td></tr></table></figure><ul><li>其他配置</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># consumer是通过拉取的方式向服务端拉取数据，当超过指定时间间隔max.poll.interval.ms没有向服务端发送poll()请求，而心跳heartbeat线程仍然在继续，会认为该consumer锁死，就会将该consumer退出group，并进行再分配。默认：300000</span><br><span class="line">spring.kafka.consumer.properties.max.poll.interval.ms=300000</span><br><span class="line"> </span><br><span class="line"># 会话的超时限制。如果consumer在这段时间内没有发送心跳信息，则它会被认为挂掉了，并且reblance将会产生，必须在[group.min.session.timeout.ms, group.max.session.timeout.ms]范围内。默认：10000</span><br><span class="line">spring.kafka.consumer.properties.session.timeout.ms=10000</span><br><span class="line"> </span><br><span class="line"># 在“range”和“roundrobin”策略之间选择一种作为分配partitions给consumer 数据流的策略； 循环的partition分配器分配所有可用的partitions以及所有可用consumer 线程。它会将partition循环的分配到consumer线程上。如果所有consumer实例的订阅都是确定的，则partitions的划分是确定的分布。循环分配策略只有在以下条件满足时才可以：（1）每个topic在每个consumer实例上都有同样数量的数据流。（2）订阅的topic的集合对于consumer group中每个consumer实例来说都是确定的。</span><br><span class="line">spring.kafka.consumer.properties.partition.assignment.strategy=range</span><br><span class="line"> </span><br><span class="line"># 一次fetch请求，从一个broker中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。默认：50 * 1024 * 1024 = 52428800</span><br><span class="line">spring.kafka.consumer.properties.fetch.max.bytes=52428800</span><br><span class="line"> </span><br><span class="line"># Metadata数据的刷新间隔。即便没有任何的partition订阅关系变更也能执行。默认：5 * 60 * 1000 = 300000</span><br><span class="line">spring.kafka.consumer.properties.metadata.max.age.ms=300000</span><br><span class="line"> </span><br><span class="line"># 一次fetch请求，从一个partition中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。broker、topic都会对producer发给它的message size做限制。所以在配置这值时，可以参考broker的message.max.bytes 和 topic的max.message.bytes的配置。默认：1 * 1024 * 1024 = 1048576</span><br><span class="line">spring.kafka.consumer.properties.max.partition.fetch.bytes=1048576</span><br><span class="line"> </span><br><span class="line"># 最大发送的TCP大小。默认：128 * 1024 = 131072，如果设置为 -1 则为操作系统默认大小</span><br><span class="line">spring.kafka.consumer.properties.send.buffer.bytes=131072</span><br><span class="line"> </span><br><span class="line"># 消费者接受缓冲区的大小。这个值在创建Socket连接时会用到。取值范围是：[-1, Integer.MAX]。默认值是：65536 （64 KB），如果值设置为-1，则会使用操作系统默认的值。默认：64 * 1024 = 65536</span><br><span class="line">spring.kafka.consumer.properties.receive.buffer.bytes=65536</span><br><span class="line"> </span><br><span class="line"># 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连，默认：50</span><br><span class="line">spring.kafka.consumer.properties.reconnect.backoff.ms=50</span><br><span class="line"> </span><br><span class="line"># producer客户端连接一个kafka服务（broker）失败重连的总时间，每次连接失败，重连时间都会指数级增加，每次增加的时间会存在20%的随机抖动，以避免连接风暴。默认：1000</span><br><span class="line">spring.kafka.consumer.properties.reconnect.backoff.max.ms=1000</span><br><span class="line"> </span><br><span class="line"># 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中，默认：100</span><br><span class="line">spring.kafka.consumer.properties.retry.backoff.ms=100</span><br><span class="line"> </span><br><span class="line"># metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口退出后，我们会擦除并重写最老的窗口，默认：30000</span><br><span class="line">spring.kafka.consumer.properties.metrics.sample.window.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用于维护metrics的样本数，默认：2</span><br><span class="line">spring.kafka.consumer.properties.metrics.num.samples=2</span><br><span class="line"> </span><br><span class="line"># 用于metrics的最高纪录等级。默认：Sensor.RecordingLevel.INFO.toString()</span><br><span class="line">#spring.kafka.consumer.properties.metrics.recording.level=Sensor.RecordingLevel.INFO.toString()</span><br><span class="line"> </span><br><span class="line"># 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计。默认：Collections.emptyList()</span><br><span class="line">#spring.kafka.consumer.properties.metric.reporters=Collections.emptyList()</span><br><span class="line"> </span><br><span class="line"># 自动检查所消耗记录的CRC32。这可以确保没有线上或磁盘损坏的消息发生。此检查会增加一些开销，因此在寻求极高性能的情况下可能会被禁用。默认：true</span><br><span class="line">spring.kafka.consumer.properties.check.crcs=true</span><br><span class="line"> </span><br><span class="line"># 连接空闲超时时间。因为consumer只与broker有连接（coordinator也是一个broker），所以这个配置的是consumer到broker之间的。默认：9 * 60 * 1000 = 540000</span><br><span class="line">spring.kafka.consumer.properties.connections.max.idle.ms=540000</span><br><span class="line"> </span><br><span class="line"># 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认：30000</span><br><span class="line">spring.kafka.consumer.properties.request.timeout.ms=30000</span><br><span class="line"> </span><br><span class="line"># 用于阻止的KafkaConsumer API的默认超时时间。KIP还为这样的阻塞API添加了重载，以支持指定每个阻塞API使用的特定超时，而不是使用default.api.timeout.ms设置的默认超时。特别是，添加了一个新的轮询（持续时间）API，它不会阻止动态分区分配。旧的poll（long）API已被弃用，将在以后的版本中删除。还为其他KafkaConsumer方法添加了重载，例如partitionsFor，listTopics，offsetsForTimes，beginningOffsets，endOffsets和close，它们接收持续时间。默认：60 * 1000 = 60000</span><br><span class="line">spring.kafka.consumer.properties.default.api.timeout.ms=60000</span><br><span class="line"> </span><br><span class="line"># 用户自定义interceptor。默认：Collections.emptyList()</span><br><span class="line">#spring.kafka.consumer.properties.interceptor.classes=Collections.emptyList()</span><br><span class="line"> </span><br><span class="line"># 是否将内部topics的消息暴露给consumer。默认：true</span><br><span class="line">spring.kafka.consumer.properties.exclude.internal.topics=true</span><br><span class="line"> </span><br><span class="line"># 默认：true</span><br><span class="line">spring.kafka.consumer.properties.internal.leave.group.on.close=true</span><br><span class="line"> </span><br><span class="line"># 默认：IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT)</span><br><span class="line">#spring.kafka.consumer.properties.isolation.level=IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT)</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-一些使用规范"><a href="#4-一些使用规范" class="headerlink" title="4. 一些使用规范"></a>4. 一些使用规范</h3><p>一个真实的公司要求的使用规范实例</p><ul><li><p>Producer 部分参数设定:</p><ol><li><p>acks 设置为 “all” 即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失.</p></li><li><p>retries = MAX 无限重试，直到你意识到出现了问题.</p></li><li><p>使用 callback 来处理消息失败发送逻辑.</p></li><li><p>min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用.</p></li><li><p>其他一些超时参数: reconnect.backoff.ms, retry.backoff.ms , linger.ms 结合 batch.size 等.</p></li></ol></li><li><p>Consumer 部分参数设定:</p><ol><li><p>auto.offset.reset 设置为 “earliest” 避免 offset 丢失时跳过未消费的消息. 目前消息存储不统一, 部分使用 zookeeper, 部分使用 kafka topic.</p></li><li><p>enable.auto.commit=false  关闭自动提交位移, 在消息被完整处理之后再手动提交位移.</p></li><li><p>consumer 的并发受 partition 的限制. 如果消息处理量比较大的情况请提前与运维联系, 增加 partition 数量应对消费端并发. 默认topic partition 为6-8个.partition 也不是越多越好. 首先会增加 file 和 memory, 其次会延长选举时间, 并且会延长 offset 的查询时间.  partition可以扩容但无法缩减.</p></li></ol></li><li><p>极限情况的数据丢失现象.</p><ol><li><p>即使将 ack 设置为 “all” 也会在一定情况下丢失消息. 因为 kafka 的高性能特性, 消息在写入 kafka 时并没有落盘 而是写入了 OS buffer 中. 使用 OS 的脏页刷新策略周期性落盘, 就算落盘 仍然会有 raid buffer. 前者机器宕机数据丢失, 后者机器跳电数据丢失.</p></li><li><p>对数据可靠性较高的场景建议 offset 手动提交. 自动提交当遇到业务系统上线被关闭时, 消息读取并且 offset 已经提交, 但是数据没有存储或者仍没来得及消费时, 消息状态在内存中无法保留, 重启应用会跳过消息 致使消息丢失.</p></li></ol></li></ul><h3 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h3><p><a href="https://www.docs.spring.io/spring-kafka/docs/2.2.0.RELEASE" target="_blank" rel="noopener">Spring-Kafka doc</a></p><p><a href="https://www.orchome.com/kafka/index" target="_blank" rel="noopener">Kafka中文网</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一-Kafka介绍&quot;&gt;&lt;a href=&quot;#一-Kafka介绍&quot; class=&quot;headerlink&quot; title=&quot;一. Kafka介绍&quot;&gt;&lt;/a&gt;一. Kafka介绍&lt;/h2&gt;&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;header
      
    
    </summary>
    
    
      <category term="Kafka 消息队列" scheme="http://liugan96.top/tags/kafka-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>记一次使用node.js的child_process模块来调用其他子进程学习</title>
    <link href="http://liugan96.top/2019/08/23/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8node-js%E7%9A%84child-process%E6%A8%A1%E5%9D%97%E6%9D%A5%E5%90%AF%E5%8A%A8%E5%85%B6%E4%BB%96%E5%AD%90%E8%BF%9B%E7%A8%8B%E5%AD%A6%E4%B9%A0/"/>
    <id>http://liugan96.top/2019/08/23/记一次使用node-js的child-process模块来启动其他子进程学习/</id>
    <published>2019-08-23T14:55:53.000Z</published>
    <updated>2019-08-24T01:56:17.209Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><blockquote><p>由于最近项目中遇到了一个问题,因公司项目是需要<code>electron</code>环境来运行,同时还依赖其他独立应用程序,所以出现了有一个场景: 就是在<code>electron</code>项目在被强制结束进程的情况下,独立程序并未被关闭,导致出现某些异常,所以现在需要在<code>electron</code>主进程重新启动时,关闭并重启依赖的其他独立应用程序(进程).</p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol><li>首先会去寻找 <code>electron</code> 的官方文档,寻找是否本身就拥有调用和关闭其他进程的 <code>API</code> 来满足我们的需求.当然,经过一番寻找发现这条路是行不通的,官方并未提供这类 <code>API</code>.</li><li>我们接着可以发现官方文档有如下介绍: <code>Electron是由Github开发，用HTML，CSS和JavaScript来构建跨平台桌面应用程序的一个开源库。 Electron通过将Chromium和Node.js合并到同一个运行时环境中，并将其打包为Mac，Windows和Linux系统下的应用来实现这一目的.</code>所以我们可以想到可以利用<code>node.js</code>相关能力来操作第三方程序来作为子进程.</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ol><li>第一步,查看官方文档,如下:</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">`child_process`</span> 模块提供了衍生子进程的能力（以一种与 popen(<span class="number">3</span>) 类似但不相同的方式）。 此功能主要由 <span class="string">`child_process.spawn()`</span> 函数提供：</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> &#123; spawn &#125; = <span class="built_in">require</span>(<span class="string">'child_process'</span>);</span><br><span class="line"><span class="keyword">const</span> ls = spawn(<span class="string">'ls'</span>, [<span class="string">'-lh'</span>, <span class="string">'/usr'</span>]);</span><br><span class="line"></span><br><span class="line">ls.stdout.on(<span class="string">'data'</span>, (data) =&gt; &#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">`stdout: <span class="subst">$&#123;data&#125;</span>`</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">ls.stderr.on(<span class="string">'data'</span>, (data) =&gt; &#123;</span><br><span class="line">  <span class="built_in">console</span>.error(<span class="string">`stderr: <span class="subst">$&#123;data&#125;</span>`</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">ls.on(<span class="string">'close'</span>, (code) =&gt; &#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">`子进程退出，使用退出码 <span class="subst">$&#123;code&#125;</span>`</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">默认情况下， stdin、 stdout 和 stderr 的管道会在父 Node.js 进程和衍生的子进程之间建立。 这些管道具有有限的（且平台特定的）容量。 如果子进程写入 stdout 时超出该限制且没有捕获输出，则子进程将会阻塞并等待管道缓冲区接受更多的数据。 这与 shell 中的管道的行为相同。 如果不消费输出，则使用 &#123; <span class="attr">stdio</span>: <span class="string">'ignore'</span> &#125; 选项。</span><br></pre></td></tr></table></figure><p>看官方文档及实例可知,我们可以使用<code>child_process</code>模块的<code>child_process.spawn()</code>函数来调用指定的命令,并并捕获 <code>stdout</code>、 <code>stderr</code>、以及退出码.</p><p>无论在<code>windows</code>下还是在<code>linux</code>下,我们知道都可以通过执行相应的命令来启动其他应用程序,按照这个思路,我们可以利用<code>child_process.spawn()</code>函数来执行我们想要的命令,从而达到我们想要实现的效果.如果是想执行多条命令,例如首先查找该应用是否启动,若启动的话,先结束该进程,然后再启动它,我们可以编写一个批处理文件,同样可以使用<code>child_process.spawn()</code>函数来执行.官方对于使用<code>.bat、.cad</code>文件也给出了示例:</p><blockquote><h2 id="在-Windows-上衍生-bat-和-cmd-文件中英对照提交修改"><a href="#在-Windows-上衍生-bat-和-cmd-文件中英对照提交修改" class="headerlink" title="在 Windows 上衍生 .bat 和 .cmd 文件中英对照提交修改"></a>在 Windows 上衍生 .bat 和 .cmd 文件中英对照提交修改</h2><p>child_process.exec() 和 child_process.execFile() 之间区别的重要性可能因平台而异。 在 Unix 类型的操作系统（Unix、Linux、macOS）上，child_process.execFile() 可以更高效，因为默认情况下它不会衍生 shell。 但是在 Windows 上， .bat 和 .cmd 文件在没有终端的情况下不能自行执行，因此无法使用 child_process.execFile() 启动。 当在 Windows 上运行时，要调用 .bat 和 .cmd 文件，可以使用设置了 shell 选项的 child_process.spawn()、或 child_process.exec()、或衍生 cmd.exe 并将 .bat 或 .cmd 文件作为参数传入（也就是 shell 选项和 child_process.exec() 所做的）。 在任何情况下，如果脚本的文件名包含空格，则需要加上引号。</p></blockquote><p>好的,到此我们就开始正式编码.</p><blockquote><p>child_process.exec() 和 child_process.execFile() 之间区别的重要性可能因平台而异。 在 Unix 类型的操作系统（Unix、Linux、macOS）上，child_process.execFile() 可以更高效，因为默认情况下它不会衍生 shell。 但是在 Windows 上， .bat 和 .cmd 文件在没有终端的情况下不能自行执行，因此无法使用 child_process.execFile() 启动。 当在 Windows 上运行时，要调用 .bat 和 .cmd 文件，可以使用设置了 shell 选项的 child_process.spawn()、或 child_process.exec()、或衍生 cmd.exe 并将 .bat 或 .cmd 文件作为参数传入（也就是 shell 选项和 child_process.exec() 所做的）。 在任何情况下，如果脚本的文件名包含空格，则需要加上引号。</p></blockquote><p>因为<code>child_process.exec()</code>底层也使用了<code>spawn()</code>函数,所以我们直接选择使用<code>child_process.spawn()</code>来完成我们的需求</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">// 首先我们编写一个我们需要的.bat文件,代码如下,保存为startApp.bat</span><br><span class="line"><span class="built_in">echo</span> create softphone windows service</span><br><span class="line"></span><br><span class="line"><span class="comment">@rem 进入当前文件夹</span></span><br><span class="line"><span class="built_in">cd</span> /d <span class="variable">%cd%</span></span><br><span class="line"></span><br><span class="line"># 以下命令可根据你想要启动的应用程序所在位置的不同而调整</span><br><span class="line"><span class="built_in">cd</span> ../</span><br><span class="line"></span><br><span class="line"># 获取当前所在根目录的路径</span><br><span class="line"><span class="built_in">set</span> rootPath=<span class="variable">%cd%</span></span><br><span class="line"></span><br><span class="line"># 设置启动程序的路径</span><br><span class="line"><span class="built_in">set</span> spPath=\xxxx\xxxx.exe</span><br><span class="line"><span class="built_in">set</span> rdPath = \xxxx\xxxx.exe</span><br><span class="line"></span><br><span class="line"># 拼接完整路径地址</span><br><span class="line"><span class="built_in">set</span> test1Path=<span class="variable">%rootPath%</span><span class="variable">%spPath%</span></span><br><span class="line"><span class="built_in">set</span> test2Path=<span class="variable">%rootPath%</span><span class="variable">%rdPath%</span></span><br><span class="line"></span><br><span class="line"># 查找该进程是否启动,若启动则结束该进程</span><br><span class="line">tasklist | <span class="built_in">find</span> /i "xxxx.exe" &amp;&amp; <span class="built_in">taskkill</span> /F /im xxxx.exe</span><br><span class="line">tasklist | <span class="built_in">find</span> /i "test.exe" &amp;&amp; <span class="built_in">taskkill</span> /F /im test.exe</span><br><span class="line"></span><br><span class="line"># 启动进程 '-a': 启动进程的参数</span><br><span class="line"><span class="built_in">start</span> <span class="variable">%test1Path%</span> -a</span><br><span class="line"><span class="built_in">start</span> <span class="variable">%test2Path%</span> -a</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="built_in">start</span> service</span><br><span class="line"></span><br><span class="line"><span class="keyword">exit</span></span><br></pre></td></tr></table></figure><p>接下来使用<code>child_process.spawn()</code>函数来调用该批处理文件.<br>首先展示一下目录结构.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">|-src</span><br><span class="line">  |-common</span><br><span class="line">    |-startApp.bat</span><br><span class="line">    |-test.js</span><br><span class="line">  |- main.js</span><br><span class="line">该目录结构是在一个标准的 electron工程下的目录结构,调用.bat文件时要注意的一点就是文件与代码所处路径的问题.</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在common下新建一个test.js文件</span></span><br><span class="line"><span class="comment">// 下面三个模块时为了在windows下一些输出错误信息中文字符会乱码的问题</span></span><br><span class="line"><span class="keyword">var</span> iconv = <span class="built_in">require</span>(<span class="string">'iconv-lite'</span>);</span><br><span class="line"><span class="keyword">var</span> encoding = <span class="string">'cp936'</span>;</span><br><span class="line"><span class="keyword">var</span> binaryEncoding = <span class="string">'binary'</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">restartApp</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="comment">// 引入spawn函数</span></span><br><span class="line">    <span class="keyword">const</span> &#123; spawn &#125; = <span class="built_in">require</span>(<span class="string">'child_process'</span>);</span><br><span class="line">    <span class="keyword">const</span> path=<span class="built_in">require</span>(<span class="string">'path'</span>);</span><br><span class="line">    <span class="comment">// 使用cmd命令 `start restartApp`: 执行restartApp.bat</span></span><br><span class="line">    <span class="keyword">const</span> bat = spawn(</span><br><span class="line">        [<span class="string">'/c'</span>, <span class="string">'start restartApp'</span>],</span><br><span class="line">        &#123;</span><br><span class="line">            cwd: path.join(__dirname,<span class="string">"../common"</span>), <span class="comment">//运行子进程的目录,此处千万要写对路径,否则会报'xxxx'不是不是内部或外部命令，也不是可运行的程序或批处理文件的提示</span></span><br><span class="line">            detached: <span class="literal">true</span>, <span class="comment">//让父进程退出后，子进程能独立运行</span></span><br><span class="line">            shell: process.platform === <span class="string">'win32'</span>,</span><br><span class="line">            windowsHide: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br><span class="line">    bat.stdout.on(<span class="string">'data'</span>, (data) =&gt; &#123;</span><br><span class="line">        <span class="comment">// 标准输出</span></span><br><span class="line">        <span class="built_in">console</span>.log(data.toString());</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    bat.stderr.on(<span class="string">'data'</span>, (data) =&gt; &#123;</span><br><span class="line">        <span class="comment">// 输出错误信息</span></span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'stderr: '</span> + iconv.decode(<span class="keyword">new</span> Buffer(data, binaryEncoding), encoding), iconv.decode(<span class="keyword">new</span> Buffer(data, binaryEncoding), encoding));</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    bat.on(<span class="string">'exit'</span>, (code) =&gt; &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">`子进程退出，退出码 <span class="subst">$&#123;code&#125;</span>`</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line">closeService = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> &#123; exec &#125; = <span class="built_in">require</span>(<span class="string">'child_process'</span>);</span><br><span class="line">    exec(<span class="string">'tasklist | find /i "xxxx.exe" &amp;&amp; taskkill /F /im xxxx.exe'</span>, (error, stdout, stderr) =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span>(error !== <span class="literal">null</span>)&#123;</span><br><span class="line">            <span class="built_in">console</span>.info(<span class="string">'stderr: '</span> + error);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">console</span>.info(<span class="string">"成功"</span>)</span><br><span class="line">        <span class="built_in">console</span>.info(<span class="string">'stdout: '</span> + stdout);</span><br><span class="line">        <span class="built_in">console</span>.info(<span class="string">'stderr: '</span> + stderr);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">    restartApp,</span><br><span class="line">    closeService</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在<code>electron</code>项目里的<code>main.js</code>中调用</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">const</span> &#123; app &#125; = <span class="built_in">require</span>(<span class="string">'electron'</span>);</span><br><span class="line"><span class="keyword">const</span> &#123; restartApp &#125; = <span class="built_in">require</span>(<span class="string">'./test'</span>);</span><br><span class="line">...</span><br><span class="line">app.once(<span class="string">'ready'</span>, () =&gt; &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 调用刚刚的方法,在electron启动时,来重新启动其他进程</span></span><br><span class="line">  restartApp();</span><br><span class="line">  ...</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">app.on(<span class="string">'window-all-closed'</span>, () =&gt; &#123;</span><br><span class="line">  <span class="keyword">if</span> (process.platform !== <span class="string">'darwin'</span>) &#123;</span><br><span class="line">    <span class="comment">// electron关闭时,来关闭其他进程</span></span><br><span class="line">    closeService();</span><br><span class="line">    app.quit();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>综上,就是一次使用在寻找如何在electron中创建其他子进程的全过程,在有一个需求之后我们首先应该去分析,为什么会产生这个需求,我们的目的是什么,有哪些熟知的手段去解决,然后可以去查看官方文档,一步一部的去尝试,就能解决问题.当然<code>child_process</code>模块中还有许多方法,可以实现类似的效果,还可以同步的创建子进程,监听相关事件然后响应,等待各位去探索.作者水平有限,文中有所不足之处还望大家不吝赐教.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://electronjs.org/docs/tutorial/about" target="_blank" rel="noopener">Electron 文档</a></p><p><a href="http://nodejs.cn/api/child_process.html#child_process_child_process_spawn_command_args_options" target="_blank" rel="noopener">Node.js 中文网 v10.16.3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;由于最近项目中遇到了一个问题,因公司项目是需要&lt;code&gt;electron&lt;/code&gt;环境来运行,同时还依赖其他
      
    
    </summary>
    
    
      <category term="node.js" scheme="http://liugan96.top/tags/node-js/"/>
    
      <category term="electron" scheme="http://liugan96.top/tags/electron/"/>
    
  </entry>
  
  <entry>
    <title>springboot+maven+mybatis整合时遇到一些需要注意的问题</title>
    <link href="http://liugan96.top/2019/01/13/springboot-maven-mybatis%E6%95%B4%E5%90%88%E6%97%B6%E9%81%87%E5%88%B0%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://liugan96.top/2019/01/13/springboot-maven-mybatis整合时遇到一些需要注意的问题/</id>
    <published>2019-01-13T03:28:29.000Z</published>
    <updated>2019-01-20T01:59:53.066Z</updated>
    
    <content type="html"><![CDATA[<p>昨天在整合<code>springboot+maven+mybatis</code>遇到了一些问题，也花了一些时间去解决，其主要问题都是些配置的问题，由于版本的变动，相应的一些包的版本和配置文件的写法也会有些许不同，如果依旧按照之前的写法，可能编译通过，但是运行时就会报错，下面就简要记录一下遇到的一些坑。</p><ol><li><code>mybatis-generator-core</code>包无法下载<img src="//liugan96.top/2019/01/13/springboot-maven-mybatis整合时遇到一些需要注意的问题/mybatis插件.png" alt="mybatis插件"></li></ol><p>有的同学可能在此处会遇到该依赖无法下载下来，此时只需要将该依赖移到<code>&lt;build&gt;&lt;/build&gt;</code>标签外的<code>&lt;dependencies&gt;&lt;/dependencies&gt;</code>标签内就可以把<code>jar</code>包下载下来，再移回原处即可。只要有<code>jar</code>包就不会报错了，出现这个问题的原因尚不明确，笔者推测可能是maven的依赖传递存在的一些问题。</p><ol start="2"><li>如图，找不到<code>xml</code>中的<code>sql</code>语句<img src="//liugan96.top/2019/01/13/springboot-maven-mybatis整合时遇到一些需要注意的问题/无效的statement.png" alt="无效的statement"></li></ol><p>该问题不影响<code>springboot</code>服务的正常启动，但是在访问该方法去执行<code>mybatis</code>生成的查询方法时则会出现，网上关于此类似的问题解决的方案主要分为三种：</p><p>​    <strong>第一种：语法错误</strong></p><p>Java DAO层接口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public void delete(@Param(&quot;id&quot;)String id);</span><br></pre></td></tr></table></figure><p>Java 对应的mapper.xml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;</span><br><span class="line">&lt;mapper namespace=&quot;xxx.xxx.xxx.Mapper&quot;&gt;</span><br><span class="line">    &lt;!-- 删除数据 --&gt;</span><br><span class="line">    &lt;delete id=&quot;delete&quot; parameterType=&quot;java.lang.String&quot;&gt;</span><br><span class="line">        DELETE FROM xxx WHERE id=#&#123;id&#125;</span><br><span class="line">    &lt;/delete&gt;</span><br><span class="line">&lt;/mapper&gt;</span><br></pre></td></tr></table></figure><p>检查：1. 接口中方法名（delete）与xml文件中 id=”delete”是否一致</p><p>　　　2. xml文件中的 namespace=”xxx.xxx.xxx.Mapper” 中的路径是否与接口文件路径一致</p><p>　　　3.parameterType类型 与 resultType类型是否准确；resultMap与resultType是不一样的。</p><p>​    <strong>第二种：编译错误</strong></p><p>　　定位到项目路径下：<code>target\classes\</code> 中报错路径下，寻找对应的<code>xml</code>文件是否存在。</p><p>（1）若不存在对应的<code>xml</code>文件，则需要在<code>pom.xml</code>中加入以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;resources&gt;</span><br><span class="line">         &lt;resource&gt;</span><br><span class="line">             &lt;directory&gt;src/main/java&lt;/directory&gt;</span><br><span class="line">             &lt;excludes&gt;</span><br><span class="line">                 &lt;exclude&gt;**/*.java&lt;/exclude&gt;</span><br><span class="line">             &lt;/excludes&gt;</span><br><span class="line">         &lt;/resource&gt;</span><br><span class="line">         &lt;resource&gt;</span><br><span class="line">             &lt;directory&gt;src/main/resources&lt;/directory&gt;</span><br><span class="line">             &lt;includes&gt;</span><br><span class="line">                 &lt;include&gt;**/*.*&lt;/include&gt;</span><br><span class="line">             &lt;/includes&gt;</span><br><span class="line">        &lt;/resource&gt;</span><br><span class="line">    &lt;/resources&gt;</span><br><span class="line">&lt;/build&gt;</span><br></pre></td></tr></table></figure><p>　　删除classes文件夹中文件，重新编译，出现了对应的<code>xml</code>文件即可。</p><p><img src="//liugan96.top/2019/01/13/springboot-maven-mybatis整合时遇到一些需要注意的问题/生成的mapperd-xml.png" alt=""></p><p>（2）若存在xml文件，则打开xml文件，检查其中报错部分是否与源文件一致，不一致，则</p><p>　　先清除classes文件夹中文件，执行命令：mvn clean 清理内容，重新编译后即可。</p><p>​    <strong>第三种：配置错误</strong></p><p>　　在配置文件中指定扫描包时，配置路径有问题。例如：spring配置文件中<code>”basePackage”</code>属性包名的指定一定要具体到接口所在包，而不要写父级甚至更高级别的包 ，否则可能出现问题；<code>cn.dao</code>与<code>cn.*</code>也可能导致错误；注解扫描时，可能没有扫描到包等。</p><ol start="3"><li><p><code>mysql</code>版本及驱动包问题</p><p>我安装的<code>MySQL</code>版本为<code>8.0.13</code>，对应的<code>mysql-connector-java</code>包的版本应该为<code>8.0.11</code>,<code>MySQL5.7</code>之前的版本可以使用<code>5.1.41</code>，同时对应的<code>MySQL8</code>以上的数据库连接<code>url及driver</code>和之前的有所区别，当运行时出现有关数据连接方面的错误可以先检查一下数据库连接<code>url</code>，在<code>MySQL8</code>中<code>url</code>要指定<code>ssl,编码格式，时区</code>等。先将<code>MySQL8的url及driver写法展示如下</code>：</p><p><code>application.yml</code>中：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line"><span class="attr">  datasource:</span></span><br><span class="line"><span class="attr">    data:</span> <span class="string">miaosha</span></span><br><span class="line"><span class="attr">    url:</span> <span class="attr">jdbc:mysql://localhost:3306/miaosha?serverTimezone=UTC&amp;acharacterEncoding=utf-8&amp;useSSL=false&amp;serverTimezone=GMT&amp;rewriteBatchedStatements=true</span></span><br><span class="line"><span class="attr">    username:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">    password:</span> <span class="string">******</span></span><br><span class="line">    <span class="comment">#使用druid数据源</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">com.alibaba.druid.pool.DruidDataSource</span></span><br><span class="line">    <span class="comment"># mysql8以上推荐的驱动包</span></span><br><span class="line"><span class="attr">    driver-class-name:</span> <span class="string">com.mysql.cj.jdbc.Driver</span></span><br></pre></td></tr></table></figure></li></ol><p>   <code>mybatis-generator.xml</code>中：</p>   <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">context</span> <span class="attr">id</span>=<span class="string">"DB2Tables"</span> <span class="attr">targetRuntime</span>=<span class="string">"MyBatis3"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--数据库连接地址账号密码--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">jdbcConnection</span> <span class="attr">driverClass</span>=<span class="string">"com.mysql.cj.jdbc.Driver"</span></span></span><br><span class="line"><span class="tag">                        <span class="attr">connectionURL</span>=<span class="string">"jdbc:mysql://localhost:3306/miaosha?serverTimezone=UTC&amp;amp;characterEncoding=utf-8&amp;amp;useSSL=false&amp;amp;serverTimezone=GMT&amp;amp;rewriteBatchedStatements=true"</span></span></span><br><span class="line"><span class="tag">                        <span class="attr">userId</span>=<span class="string">"root"</span></span></span><br><span class="line"><span class="tag">                        <span class="attr">password</span>=<span class="string">"******"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">jdbcConnection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">context</span>&gt;</span></span><br></pre></td></tr></table></figure><p>   注：<code>&amp;amp;</code>为<code>&amp;</code>转义，在<code>Idea</code>中编写<code>XML</code>文件直接使用<code>&amp;</code>貌似会报错</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;昨天在整合&lt;code&gt;springboot+maven+mybatis&lt;/code&gt;遇到了一些问题，也花了一些时间去解决，其主要问题都是些配置的问题，由于版本的变动，相应的一些包的版本和配置文件的写法也会有些许不同，如果依旧按照之前的写法，可能编译通过，但是运行时就会报错，
      
    
    </summary>
    
    
      <category term="Springboot" scheme="http://liugan96.top/tags/springboot/"/>
    
      <category term="Maven" scheme="http://liugan96.top/tags/maven/"/>
    
      <category term="Mybatis" scheme="http://liugan96.top/tags/mybatis/"/>
    
      <category term="MySQL" scheme="http://liugan96.top/tags/mysql/"/>
    
      <category term="Java" scheme="http://liugan96.top/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://liugan96.top/2019/01/12/hello-world/"/>
    <id>http://liugan96.top/2019/01/12/hello-world/</id>
    <published>2019-01-11T16:01:34.435Z</published>
    <updated>2019-01-05T07:36:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Docker入门简易教程</title>
    <link href="http://liugan96.top/2019/01/05/docker%E5%85%A5%E9%97%A8%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/"/>
    <id>http://liugan96.top/2019/01/05/docker入门简易教程/</id>
    <published>2019-01-05T14:14:26.000Z</published>
    <updated>2020-07-07T13:33:27.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-Docker简介"><a href="#一-Docker简介" class="headerlink" title="一. Docker简介"></a>一. Docker简介</h2><p>关于<code>Docker</code>的介绍网上已经有了不少,简单来说,<code>Docker</code>是一种容器技术,但是是经过改进的容器技术,其改进的地方在于<code>Docker</code>容器引入了镜像,使的容器的创建和迁移变得方便快捷不少</p><blockquote><p>Docker是一个开源的引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括<code>VMs</code>（虚拟机）、<a href="http://www.whatis.com.cn/word_5275.htm" target="_blank" rel="noopener">bare metal</a>、<code>OpenStack</code> 集群和其他的基础应用平台。</p></blockquote><h4 id="Docker通常用于如下场景："><a href="#Docker通常用于如下场景：" class="headerlink" title="Docker通常用于如下场景："></a>Docker通常用于如下场景：</h4><ul><li>web应用的自动化打包和发布；</li><li>自动化测试和持续集成、发布；</li><li>在服务型环境中部署和调整数据库或其他的后台应用；</li><li>从头编译或者扩展现有的<code>OpenShift</code>或<code>Cloud Foundry</code>平台来搭建自己的<code>PaaS</code>环境。</li></ul><h4 id="Docker明显的特点"><a href="#Docker明显的特点" class="headerlink" title="Docker明显的特点:"></a>Docker明显的特点:</h4><ul><li>轻量,内存占用小,密度高</li><li>快速,启动时间为毫秒级</li><li>隔离,沙盒技术类似于虚拟机</li></ul><h4 id="Docker技术的基础："><a href="#Docker技术的基础：" class="headerlink" title="Docker技术的基础："></a>Docker技术的基础：</h4><ul><li>namespace，容器隔离的基础，保证A容器看不到B容器. 6个名空间：User,Mnt,Network,UTS,IPC,Pid</li><li>cgroups，容器资源统计和隔离。主要用到的cgroups子系统：cpu,blkio,device,freezer,memory</li><li>unionfs，典型：aufs/overlayfs，分层镜像实现的基础</li></ul><h4 id="Docker组件："><a href="#Docker组件：" class="headerlink" title="Docker组件："></a>Docker组件：</h4><ul><li><strong>docker Client</strong>客户端————&gt;向docker服务器进程发起请求，如:创建、停止、销毁容器等操作</li><li><strong>docker Server</strong>服务器进程—–&gt;处理所有docker的请求，管理所有容器</li><li><strong>docker Registry</strong>镜像仓库——&gt;镜像存放的中央仓库，可看作是存放二进制的scm</li></ul><h4 id="Docker的安装"><a href="#Docker的安装" class="headerlink" title="Docker的安装:"></a>Docker的安装:</h4><p>安装在此处就不赘述, 因为其安装比较简单,且支持目前主流操作系统,相关Mac到Windows到Linux发行版的具体安装说明可以参考官方文档: <a href="https://docs.docker.com/installation/" target="_blank" rel="noopener">docker安装</a></p><h2 id="二-准备"><a href="#二-准备" class="headerlink" title="二.准备"></a>二.准备</h2><blockquote><p>Docker系统有两个程序: Docker服务端和Docker客户端.Docker服务端主要是作为管理所有容器的一个服务进程,管理着所有的容器.Docker客户端则相当于Docker服务端的远程控制器,可以用来控制Docker的服务端进程,Docker服务端和客户端通常情况下都运行在一台机器上。</p></blockquote><p>本次的目标是安装Docker成功后检查Docker版本，以便确认Docker服务在运行并且可以通过客户端连接。</p><p>提示： 同其他<code>Linux</code>命令相似可以通过<code>docker</code>命令来查看所有的参数</p><p>通过<code>docker version</code>命令可以查看<code>docker</code>的版本</p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/dockerVersion.png" alt="dockerVersion"></p><h2 id="三-寻找可用的Docker镜像"><a href="#三-寻找可用的Docker镜像" class="headerlink" title="三.寻找可用的Docker镜像"></a>三.寻找可用的Docker镜像</h2><p>对于新手来讲，使用<code>Docker</code>最简单快捷的方式就是从现有容器镜像入手了，那么该如何去寻找现有的镜像呢，<code>Docker</code>官网专门有一个页面用来存储所有的可用镜像。（<a href="http://index.docker.io/" target="_blank" rel="noopener">网址</a> ）。我们可以通过浏览这个网页来查找我们所需要的镜像或者是通过命令行工具来检索。</p><p>本节的目标是学会通过命令行工具来检索镜像。</p><p>命令行的用法为<code>docker search 镜像名字</code></p><p>示例：</p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/dockerSearch.png" alt="docker search 镜像"></p><h2 id="四-下载Docker镜像"><a href="#四-下载Docker镜像" class="headerlink" title="四.下载Docker镜像"></a>四.下载Docker镜像</h2><p>在第三小节中我们检索到了名为<code>tutorial</code>的镜像，我们可以使用如下命令来下载该镜像(<code>docker</code>命令和<code>git</code>命令有些类似的地方)</p><p>命令： <code>docker pull learn/tutorial</code></p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/pullImage1.png" alt="1"></p><p>当执行上述命令时可能会出现镜像已存在的情况，那么此时我们想删除之前的镜像该如何操作呢？如图所示：<img src="//liugan96.top/2019/01/05/docker入门简易教程/pullImage2.png" alt="2"></p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/pullImage3.png" alt="3"></p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/pullImage4.png" alt="4"></p><p>总结： </p><p>当想删除镜像时，必须保证该镜像没有容器在使用，否则需要停止并删除该容器，才能进一步删除镜像。</p><ol><li>停止所有的<code>container</code>，这样才能够删除其中的<code>images</code>：<code>docker stop $(docker ps -a -q)</code></li></ol><p>如果想要删除所有<code>container</code>的话再加一个指令：<code>docker rm $(docker ps -a -q)</code></p><ol start="2"><li><p>查看当前有些什么images: <code>docker images</code></p></li><li><p>删除images，通过image的id来指定删除谁:<code>docker rmi &lt;image id&gt;</code></p></li></ol><p>想要删除<code>untagged images</code>，也就是那些<code>id</code>为<code>&lt;None&gt;</code>的<code>image</code>的话可以用</p><p><code>docker rmi $(docker images | grep &quot;^&lt;none&gt;&quot; | awk &quot;{print $3}&quot;)</code></p><p>要删除全部image的话<code>docker rmi $(docker images -q)</code></p><h2 id="五-在Docker容器中运行“hello-docker”"><a href="#五-在Docker容器中运行“hello-docker”" class="headerlink" title="五. 在Docker容器中运行“hello docker”"></a>五. 在Docker容器中运行“hello docker”</h2><p>通过<code>docker run</code>命令可以启动某一个镜像并运行一个命令</p><p>在此之前我们可能会问，什么是<code>docker</code>容器呢，和<code>docker</code>又有什么区别呢？</p><blockquote><p><code>docker</code>容器可以理解为在沙盒中运行的进程。这个沙盒包含了该进程运行的所有的必须的资源包括文件系统、系统类库、shell 环境等等。但这个沙盒默认是不会运行任何程序的。你需要在沙盒中运行一个进程来启动某一个容器。这个进程是该容器的唯一进程，所以当该进程结束的时候，容器也会完全的停止。 而<code>docker</code>则是一种容器技术·</p></blockquote><p>现在我们想在刚刚下载的镜像中输出”hello docker”。为了达到这个目的，我们需要在这个容器中运行”echo”命令，输出”hello docker”。 </p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/hellodocker.png" alt="4"></p><h2 id="六-如何在容器中安装新程序"><a href="#六-如何在容器中安装新程序" class="headerlink" title="六. 如何在容器中安装新程序"></a>六. 如何在容器中安装新程序</h2><p>下一步我们要做的事情是在容器里面安装一个简单的程序(ping)。你可以使用<code>apt-get</code>命令来安装<code>ping</code>程序： <code>apt-get install -y ping</code>。</p><p>备注：<code>apt-get</code> 命令执行完毕之后，容器就会停止，但对容器的改动不会丢失。</p><p>现在我们想在<code>learn/tutorial</code>镜像里面安装<code>ping</code>程序。</p><p>在执行<code>apt-get</code>命令的时候，要带上<code>-y</code>参数。如果不指定-y参数的话，<code>apt-get</code>命令会进入交互模式，需要用户输入命令来进行确认，但在<code>docker</code>环境中是无法响应这种交互的。</p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/安装新程序.png" alt="4"></p><h2 id="七-保存对容器的修改"><a href="#七-保存对容器的修改" class="headerlink" title="七. 保存对容器的修改"></a>七. 保存对容器的修改</h2><p>当你对某一个容器做了修改之后（通过在容器中运行某一个命令），可以把对容器的修改保存下来，这样下次可以从保存后的最新状态运行该容器。docker中保存状态的过程称之为committing，它保存的新旧状态之间的区别，从而产生一个新的版本。</p><p>首先使用<strong>docker ps -l</strong>命令获得安装完ping命令之后容器的id。然后把这个镜像保存为learn/ping。</p><ol><li><p>运行docker commit，可以查看该命令的参数列表。</p></li><li><p>你需要指定要提交保存容器的ID。(译者按：通过docker ps -l 命令获得)</p></li><li><p>无需拷贝完整的id，通常来讲最开始的三至四个字母即可区分。（备注：非常类似git里面的版本号)</p></li></ol><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/保存修改的容器.png" alt="4"></p><h2 id="八-运行新的镜像"><a href="#八-运行新的镜像" class="headerlink" title="八. 运行新的镜像"></a>八. 运行新的镜像</h2><p><code>ok</code>，到现在为止，你已经建立了一个完整的、自成体系的<code>docker</code>环境，并且安装了<code>ping</code>命令在里面。它可以在任何支持<code>docker</code>环境的系统中运行啦！下面就让我们来体验一下吧！</p><p>在新的镜像中运行ping <a href="http://www.baidu.com（当然也可以www.google.com，" target="_blank" rel="noopener">www.baidu.com（当然也可以www.google.com，</a> but……）命令。</p><p>一定要使用新的镜像名<strong>learn/ping</strong>来运行ping命令。(注：最开始下载的<code>learn/tutorial</code>镜像中是没有ping命令的)</p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/run.png" alt="4"></p><h2 id="九-检查运行中的镜像"><a href="#九-检查运行中的镜像" class="headerlink" title="九. 检查运行中的镜像"></a>九. 检查运行中的镜像</h2><p>现在你已经运行了一个docker容器，让我们来看下正在运行的容器。</p><p>使用docker ps命令可以查看所有正在运行中的容器列表，使用docker inspect命令我们可以查看更详细的关于某一个容器的信息。</p><p>查找某一个运行中容器的id，然后使用docker inspect命令查看容器的信息。</p><p>可以使用镜像id的前面部分，不需要完整的id。</p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/dockerps.png" alt="docker ps"></p><p><img src="//liugan96.top/2019/01/05/docker入门简易教程/dockerinspect.png" alt="docker inspect"></p><h2 id="十-发布自己的docker镜像"><a href="#十-发布自己的docker镜像" class="headerlink" title="十. 发布自己的docker镜像"></a>十. 发布自己的docker镜像</h2><p>现在我们已经验证了新镜像可以正常工作，我们可以选择将其发布到官方的索引网站。还记得我们最开始下载的learn/tutorial镜像吧，我们也可以把我们自己编译的镜像发布到索引页面，一方面可以自己重用，另一方面也可以分享给其他人使用。</p><p><code>docker push</code>命令可以将某一个镜像发布到官方网站。 </p><p>注：push之前需要我们登陆<code>docker repository</code>的账号，这里就不再赘述了。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.docker.org.cn" target="_blank" rel="noopener">docker中文网</a></p><p><a href="https://blog.csphere.cn/archives/22" target="_blank" rel="noopener">一小时Docker教程</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一-Docker简介&quot;&gt;&lt;a href=&quot;#一-Docker简介&quot; class=&quot;headerlink&quot; title=&quot;一. Docker简介&quot;&gt;&lt;/a&gt;一. Docker简介&lt;/h2&gt;&lt;p&gt;关于&lt;code&gt;Docker&lt;/code&gt;的介绍网上已经有了不少,简单来
      
    
    </summary>
    
    
      <category term="Docker" scheme="http://liugan96.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>有关Hexo博客图片插件使用报错问题</title>
    <link href="http://liugan96.top/2018/10/05/%E6%9C%89%E5%85%B3Hexo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/"/>
    <id>http://liugan96.top/2018/10/05/有关Hexo博客图片插件使用报错问题/</id>
    <published>2018-10-05T01:09:03.000Z</published>
    <updated>2018-10-05T01:24:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>在我们搭建好Hexo博客平台后,我们通常都会尝试写一篇博客去测试是否搭建成功,但是在博客里通常带有一些图片,在没有经过配置的情况下,图片是不会被正常显示的,下面就来简单介绍一下如何来配置让图片能够正常显示.</p><p><code>hexo</code>默认无法自动处理文章插入本地图片，需要通过扩展插件支持。</p><ol><li>首先找到配置<code>_config.yml</code>文件中的<code>post_asset_folder:false</code>这个选项设置为<code>true</code>,启动 Asset 文件夹.当您设置<code>post_asset_folder</code>为<code>true</code>参数后，在建立文件时，Hexo 会自动建立一个与文章同名的文件夹，您可以把与该文章相关的所有资源都放到那个文件夹，如此一来，您便可以更方便的使用资源。</li><li>在hexo的目录下执行<code>npm install https://github.com/CodeFalling/hexo-asset-image --save</code>（需要等待一段时间,安装插件）</li><li>完成安装后,运行<code>hexo n &quot;xxxx&quot;</code>来生成md博文时会发现_posts目录下面会多出一个和文章名字一样的文件夹。图片就可以放在文件夹下面。结构如下：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">本地图片测试</span><br><span class="line">├── test1.jpg</span><br><span class="line">├── test2.jpg</span><br><span class="line">└── test3.jpg</span><br><span class="line">本地图片测试.md</span><br></pre></td></tr></table></figure><p>这样的目录结构（目录名和文章名一致），只要使用标准的md语法来引入<code>![logo](本地图片测试/test1.jpg)</code> 就可以插入图片。其中<code>[]</code>里面不写文字则没有图片标题。<br>生成的结构为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public/2016/3/9/本地图片测试</span><br><span class="line">├── test1.jpg</span><br><span class="line">├── index.html</span><br><span class="line">├── test2.jpg</span><br><span class="line">└── test3.jpg</span><br></pre></td></tr></table></figure><p>同时，生成的 html 是</p><p><code>&lt;img src=&quot;/2016/3/9/本地图片测试/test1.jpg&quot; alt=&quot;test1&quot;&gt;</code></p><p>而不是愚蠢的</p><p><code>&lt;img src=&quot;本地图片测试/test1.jpg&quot; alt=&quot;test1&quot;&gt;</code></p><p>Issue:</p><p>由于<code>hexo3</code>版本后对很多插件支持有问题，<code>hexo-asset-image</code>插件在处理<code>data.permalink</code>链接时出现路径错误，把年月去掉了，导致最后生成的路径为<code>%d/xxx/xxx</code>需要对其做兼容处理。通过判断当前版本是否等于3的版本做不同的路径分割。</p><p>在代码中加入：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> version = <span class="built_in">String</span>(hexo.version).split(<span class="string">'.'</span>);</span><br><span class="line">修改date.permalink处理：</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> link = data.permalink;  </span><br><span class="line"><span class="keyword">if</span>(version.length &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">var</span> beginPos = getPosition(link, <span class="string">'/'</span>, <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">var</span> beginPos = getPosition(link, <span class="string">'/'</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br><span class="line">重新生成静态文件即可正确显示。</span><br></pre></td></tr></table></figure><p>可直接安装已经修改过得插件<code>npm install https://github.com/7ym0n/hexo-asset-image --save</code></p><p>参考链接：<a href="https://www.jianshu.com/p/3db6a61d3782" target="_blank" rel="noopener">https://www.jianshu.com/p/3db6a61d3782</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在我们搭建好Hexo博客平台后,我们通常都会尝试写一篇博客去测试是否搭建成功,但是在博客里通常带有一些图片,在没有经过配置的情况下,图片是不会被正常显示的,下面就来简单介绍一下如何来配置让图片能够正常显示.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hexo&lt;/code&gt;默认无法自动处理文
      
    
    </summary>
    
    
      <category term="小问题" scheme="http://liugan96.top/tags/%E5%B0%8F%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Angular与其他前端框架的对比</title>
    <link href="http://liugan96.top/2018/10/04/Angular%E4%B8%8E%E5%85%B6%E4%BB%96%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AF%B9%E6%AF%94/"/>
    <id>http://liugan96.top/2018/10/04/Angular与其他前端框架的对比/</id>
    <published>2018-10-04T14:31:15.000Z</published>
    <updated>2018-10-05T00:48:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="与React对比"><a href="#与React对比" class="headerlink" title="与React对比"></a>与React对比</h2><ul><li>速度:react会先更新虚拟dom,再和实际dom进行对比 angular与之不相上下</li><li>FLUX架构:组件化 数据单向更新</li><li>服务器端渲染 预渲染 seo优化</li></ul><h2 id="与vue对比"><a href="#与vue对比" class="headerlink" title="与vue对比"></a>与vue对比</h2><ul><li>个人主导(angular为google开发,号召力资金充足,vue相对来说有些不足)</li><li>只关注web(angular还可以开发客户端应用)</li><li>服务器端渲染(vue通过第三方库,angular有官方库)</li></ul><h2 id="angular程序架构"><a href="#angular程序架构" class="headerlink" title="angular程序架构"></a>angular程序架构</h2><p><img src="//liugan96.top/2018/10/04/Angular与其他前端框架的对比/angularApp.png" alt="angular程序架构"></p><p><strong>组件</strong>:是Angular应用的基本构建块,你可以把一个组件理解为一段带有业务逻辑和数据的html</p><p><strong>服务</strong>:用来封装可重用的业务逻辑</p><p><strong>指令</strong>:允许你向html中添加自定义行为</p><p><strong>模块</strong>:用来将应用中不同的部分组织成一个Angular框架可以理解的单元</p><p><strong>组件、服务、模块是为了完成某些功能的,模块是为了打包、分发这些功能的.</strong></p><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><p><strong>组件(Component)必备三要素</strong>:</p><ul><li>装饰器( @Component() ):</li><li>模版(Template)</li><li>控制器(Controller)</li><li>控制器←→控制器进行数据绑定</li></ul><p>:插值表达式</p><h3 id="启动angular应用"><a href="#启动angular应用" class="headerlink" title="启动angular应用"></a>启动angular应用</h3><ol><li>启动时加载了哪个页面?<br><code>angular.json中有一项index属性，该属性对应的src目录下的indexhtml指向的是root src目录下的index.html,main属性则指向src下的main.ts文件</code></li><li>启动时加载了哪些脚本?<br><code>main.ts</code></li><li>这些脚本做了什么事?<br><code>负责引导angular应用的启动,引入相关的库及文件,来帮助angular应用程序启动</code></li></ol><h3 id="如何引用第三方类库-2-5-开发准备"><a href="#如何引用第三方类库-2-5-开发准备" class="headerlink" title="如何引用第三方类库(2-5)开发准备"></a>如何引用第三方类库(2-5)开发准备</h3><ol><li><code>npm install</code> 安装到本地库 <code>package.json -&gt; dependencies</code></li><li>引入到项目中 <code>angular6</code> 后在<code>angular.json</code>中在<code>styles/script</code>中分别引入,之前的版本在<code>angular-cli.json</code>中</li><li><code>npm install @types/jquery -D</code> 安装<code>jquery</code>的<code>ts</code>类型描述文件,<code>npm i @types/bootstrap -D</code>安装<code>bootstrap</code>的<code>ts</code>类型描述文件</li></ol><h3 id="组件开发"><a href="#组件开发" class="headerlink" title="组件开发"></a>组件开发</h3><ol><li>App组件</li><li>导航栏组件</li><li>页脚组件</li><li>搜索表单组件</li><li>轮播图组件</li><li>商品展示组件</li><li>星级评价组件 *ngFor:指令</li></ol><p>命令自动生成组件 <code>ng g component xxx</code> 在当前项目下生成组件</p><h3 id="商品展示组件"><a href="#商品展示组件" class="headerlink" title="商品展示组件"></a>商品展示组件</h3><p>重点:<strong>angular始终是根据后台数据的变化来生成页面的,而不是去操纵dom元素</strong></p><p>bootstrap元素居中:<br>在bootstrap中实现元素居中的方法主要有这几种：</p><ol><li>加类.text-center（子元素居中）</li><li>加类.center-block（自身居中）</li><li>利用bootstrap中列偏移的概念。例如：col-md-offset-2(外边距向右偏移两列)</li></ol><h3 id="数据绑定"><a href="#数据绑定" class="headerlink" title="数据绑定"></a>数据绑定</h3><ol><li>通过插值表达式  直接将对象的属性展示出来</li><li>属性绑定:<code>&lt;img [src]=&quot;imgUrl&quot; alt=&quot;商品图片&quot;&gt;</code> 通过[]将标签属性绑定控制器的属性</li><li>样式绑定: <code>[class.glyphicon-star-empty]=&quot;star&quot;</code>  前面的class表示的是我后面要绑定的是一个class样式这个样式是由star的值决定的,star为true时会多出这么个样式,反之则不会</li><li>属性传递:</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;与React对比&quot;&gt;&lt;a href=&quot;#与React对比&quot; class=&quot;headerlink&quot; title=&quot;与React对比&quot;&gt;&lt;/a&gt;与React对比&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;速度:react会先更新虚拟dom,再和实际dom进行对比 angular与之不
      
    
    </summary>
    
    
      <category term="Angular" scheme="http://liugan96.top/tags/angular/"/>
    
  </entry>
  
  <entry>
    <title>路由中传递参数</title>
    <link href="http://liugan96.top/2018/10/04/%E5%A6%82%E4%BD%95%E5%9C%A8%E8%B7%AF%E7%94%B1%E4%B8%AD%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0/"/>
    <id>http://liugan96.top/2018/10/04/如何在路由中传递参数/</id>
    <published>2018-10-04T13:53:16.000Z</published>
    <updated>2018-10-04T14:03:34.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>修改路由配置中的path属性,使其可以携带参数 <code>path: &#39;product/:id&#39;</code> path的参数后加上 <code>/:id</code> 名为id变量</li></ol><figure class="highlight ts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> routes: Routes = [</span><br><span class="line">  &#123;path: <span class="string">''</span>, redirectTo: <span class="string">'/home'</span>, pathMatch: <span class="string">'full'</span>&#125;,</span><br><span class="line">  &#123;path: <span class="string">'chat'</span>, component: ChatComponent, outlet: <span class="string">'aux'</span>&#125;,</span><br><span class="line">  &#123;path: <span class="string">'home'</span>, component: HomeComponent&#125;,</span><br><span class="line"></span><br><span class="line">  &#123;path: <span class="string">'product/:id'</span>, component: ProductComponent,</span><br><span class="line">    children: [</span><br><span class="line">      &#123;path: <span class="string">''</span>, component: ProductDescComponent&#125;,</span><br><span class="line">      &#123;path: <span class="string">'seller/:id'</span>, component: SellerINfoComponent&#125;</span><br><span class="line">  ],canActivate: [LoginGuard],canDeactivate: [UnsavedGuard],</span><br><span class="line">  resolve: &#123;</span><br><span class="line">    product: ProductResolve</span><br><span class="line">  &#125;</span><br><span class="line">&#125;,</span><br><span class="line">  &#123;path: <span class="string">'**'</span>, component: Code404Component&#125;</span><br><span class="line">];</span><br></pre></td></tr></table></figure><ol start="2"><li>修改路由链接的参数来传递参数</li></ol><p><code>&lt;a [routerLink]=&quot;[&#39;/product&#39;, 2]&quot; &gt;商品详情页&lt;/a&gt;</code></p><ol start="3"><li>在控制器中修改代码</li></ol><figure class="highlight ts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ngOnInit() &#123;</span><br><span class="line">   <span class="comment">// 参数订阅 订阅后声明一个匿名函数传进来之后从参数中取出ID赋值给本地的product</span></span><br><span class="line">   <span class="comment">// subscribe rxjs的语法</span></span><br><span class="line">   <span class="keyword">this</span>.routeInfo.params.subscribe(<span class="function">(<span class="params">params: Params</span>) =&gt;</span> <span class="keyword">this</span>.productId = params[<span class="string">'id'</span>]);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;修改路由配置中的path属性,使其可以携带参数 &lt;code&gt;path: &amp;#39;product/:id&amp;#39;&lt;/code&gt; path的参数后加上 &lt;code&gt;/:id&lt;/code&gt; 名为id变量&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;high
      
    
    </summary>
    
    
      <category term="Angular6" scheme="http://liugan96.top/tags/angular6/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 在layout 为 draft 的情况使用记录</title>
    <link href="http://liugan96.top/2018/10/04/Hexo%20%E5%9C%A8layout%20%E4%B8%BA%20draft%20%E7%9A%84%E6%83%85%E5%86%B5%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <id>http://liugan96.top/2018/10/04/Hexo 在layout 为 draft 的情况使用记录/</id>
    <published>2018-10-04T10:24:16.000Z</published>
    <updated>2019-01-05T14:02:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>layout 为 draft 的情况<br>通过以上的学习，我们大概了解了一般情况下生成一篇博客的流程以及各个内容设置对页面效果的影响。那么我们现在学习一下  <code>hexo new draft myDraftBlog</code> 的情况。在没有执行这个命令之前，你可以去看下你的<code>source</code>文件夹，如果你是按照我之前的文章一步步进行的话，你会发现其中只有<code>_post</code>一个文件夹。</p><p>当我们执行上面的命令之后，类似<code>post</code>一样，我们同样会发现在根目录下<code>source/_drafts</code>文件夹中会出现一个<code>myDraftBlog.md</code>的文件。也就是说，<code>_draft</code>文件夹和里面的内容是你执行上述命令之后生成的。同样的，你开始编辑<code>myDraftBlog.md</code>文件，然后保存。</p><p>以此执行<code>hexo g</code> 和 <code>hexo s</code> 命令，打开4000端口之后你会发现，你完全看不到你刚才编辑的<code>myDraftBlog</code>这篇博客。这是为什么呢？因为<code>layout</code> 为 <code>draft</code>的时候，其实这个md文件是草稿状态，也就是说，这篇文章仅仅是作为你的草稿而不是正式稿，所以不会发表在博客主页上。草稿就是需要你不断完善的文章，知道有一天你觉得这篇文章可以正式发表了，那么如何才能将草稿发表成为正式稿件呢？官方文档其实有非常详细的说明：<br><img src="http://oakland.github.io/2016/05/02/hexo-%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84post/1462185994268.png" alt="Drafts"><br>上面的官方文档会告诉你，通过<code>publish</code>命令可以将<code>draft</code>移动到<code>_post</code>文件夹下，形成正式的博客。也就是说，当你认为你的草稿已经完善到可以发表的状态时，执行<code>hexo publish draft myDraftBlog.md</code>，你就会发现，<code>source/_draft</code>文件夹下的<code>myDraftBlog.md</code>文件消失了，而在<code>_post</code>文件夹下你会找到<code>myDraftBlog.md</code>文件。这个时候再按照生成<code>post</code>文件的方式，依次执行<code>hexo g hexo d</code> 就可以将这篇草稿正式转为发表在网上的博客了。</p><p>因此，<code>draft</code>都是草稿，需要和<code>publish</code>命令配合使用。</p><p>当然，根据上面的官网说明，如果我们一定要查看我们的草稿，我们可以使用<code>hexo g --draft</code>， <code>hexo s --draft</code>命令来在本地预览我们的草稿效果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;layout 为 draft 的情况&lt;br&gt;通过以上的学习，我们大概了解了一般情况下生成一篇博客的流程以及各个内容设置对页面效果的影响。那么我们现在学习一下  &lt;code&gt;hexo new draft myDraftBlog&lt;/code&gt; 的情况。在没有执行这个命令之前，你
      
    
    </summary>
    
    
      <category term="Hexo" scheme="http://liugan96.top/tags/hexo/"/>
    
  </entry>
  
</feed>

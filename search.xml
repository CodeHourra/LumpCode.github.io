<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[高性能消息中间件-Kafka]]></title>
    <url>%2F2020%2F07%2F07%2F%E9%AB%98%E6%80%A7%E8%83%BD%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-kafka%2F</url>
    <content type="text"><![CDATA[一. Kafka介绍1. 概述 Kafka最先是由Linkedin公司开发，是一个分布式的、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当作MQ系统），常见可用于web/nginx日志、访问日志、消息服务等等。于2010年贡献给了Apache基金会并成为顶级开源项目。 主要应用场景：日志收集系统、消息系统 设计目标 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。 支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 Scale out:支持在线水平扩展 1.2 消息系统介绍一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需要关系数据，不需要关心数据之间是如何传递的。 分布式消息传递基于可靠的消息队列，在客户端和消息系统之间异步传递消息。 消息的传递模式： 点对点传递 发布-订阅模式（大部分消息系统选用此种方式） 1.3 Kafka的优点 解耦 我们在项目开始的时候预测将来可能出现的需求是十分困难的。但是消息系统不用关心其内部处理过程，只需要遵循其约定好的消息的接口。这就允许你方便的扩展或修改其两边的处理过程，只需要保证其双方均遵循同样的接口约束。 冗余（副本） 有些情况下处理消息数据的过程或许会失败，除非消息数据被持久化了，否则就会丢失。消息队列通常会把消息数据持久化直到确保他们已经被完全处理之后，通过这一方式避免消息丢失的风险。许多消息队列采取“插入-获取-删除”的策略模式，把一个消息删除之前需要你的系统明确的指定改条消息已经被处理完毕，从而保证了你的数据被安全的保存到你使用完毕。 扩展性 消息队列解耦了我们的处理过程，所以我们消息的入队和处理速率是很容易的，只需要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。 灵活性&amp;峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。 缓冲 在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 1.4 Kafka中的术语解释在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系： 上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。 如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。 1.4.1 brokerKafka 集群包含一个或多个服务器，服务器节点称为broker。 broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。 如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。 如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。 1.4.2 Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） 类似于数据库的表名 1.4.3 Partitiontopic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。 1.4.4 Producer生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息追加到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。 1.4.5 Consumer消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。 1.4.6 Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。 1.4.7 Leader每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。 1.4.8 FollowerFollower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。 二. kafka安装和启动Step 1: 下载代码下载2.5.0版本并且解压它。 Step 2: 启动服务12&gt; tar -xzf kafka_2.12-2.5.0.tgz &gt; cd kafka_2.12-2.3.0 2.12： scala版本，2.5.0：kafka版本 运行kafka需要使用Zookeeper，所以你需要先启动Zookeeper，如果你没有Zookeeper，你可以使用kafka自带打包和配置好的Zookeeper。 123&gt; bin/zookeeper-server-start.sh config/zookeeper.properties[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)... 现在启动kafka服务 1234&gt; bin/kafka-server-start.sh config/server.properties &amp;[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)... Step 3: 创建一个主题(topic)创建一个名为“test”的Topic，只有一个分区和一个备份： 1&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 创建好之后，可以通过运行以下命令，查看已创建的topic信息： 12&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181test 或者，除了手工创建topic外，你也可以配置你的broker，当发布一个不存在的topic时自动创建topic。 Step 4: 发送消息Kafka提供了一个命令行的工具，可以从输入文件或者命令行中读取消息并发送给Kafka集群。每一行是一条消息。运行producer（生产者）,然后在控制台输入几条消息到服务器。 123&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testThis is a messageThis is another message Step 5: 消费消息Kafka也提供了一个消费消息的命令行工具，将存储的信息输出出来。 123&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningThis is a messageThis is another message 如果你有2台不同的终端上运行上述命令，那么当你在运行生产者时，消费者就能消费到生产者发送的消息。 Step 6: 设置多个broker集群到目前，我们只是单一的运行一个broker，没什么意思。对于Kafka，一个broker仅仅只是一个集群的大小，所有让我们多设几个broker。 首先为每个broker创建一个配置文件: 12&gt; cp config/server.properties config/server-1.properties &gt; cp config/server.properties config/server-2.properties 现在编辑这些新建的文件，设置以下属性： 123456789config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2 复制 broker.id是集群中每个节点的唯一且永久的名称，我们修改端口和日志目录是因为我们现在在同一台机器上运行，我们要防止broker在同一端口上注册和覆盖对方的数据。 我们已经运行了zookeeper和刚才的一个kafka节点，所有我们只需要在启动2个新的kafka节点。 1234&gt; bin/kafka-server-start.sh config/server-1.properties &amp;... &gt; bin/kafka-server-start.sh config/server-2.properties &amp;... 现在，我们创建一个新topic，把备份设置为：3 1&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic 好了，现在我们已经有了一个集群了，我们怎么知道每个集群在做什么呢？运行命令“describe topics” 123&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs:Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0 输出解释：第一行是所有分区的摘要，其次，每一行提供一个分区信息，因为我们只有一个分区，所以只有一行。 “leader”：该节点负责该分区的所有的读和写，每个节点的leader都是随机选择的。 “replicas”：备份的节点列表，无论该节点是否是leader或者目前是否还活着，只是显示。 “isr”：“同步备份”的节点列表，也就是活着的节点并且正在同步leader。 我们运行这个命令，看看一开始我们创建的那个节点： 123&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic testTopic:test PartitionCount:1 ReplicationFactor:1 Configs:Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 这并不奇怪，刚才创建的主题没有Replicas，并且在服务器“0”上，我们创建它的时候，集群中只有一个服务器，所以是“0”。 让我们来发布一些信息在新的topic上： 12345&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic ...my test message 1my test message 2^C 现在，消费这些消息 12345&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic ...my test message 1my test message 2^C 我们要测试集群的容错，kill掉leader，Broker1作为当前的leader，也就是kill掉Broker1。 123&gt; ps | grep server-1.properties7564 ttys002 0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java... &gt; kill -9 7564 在Windows上使用： 1234&gt; wmic process where &quot;caption = &apos;java.exe&apos; and commandline like &apos;%server-1.properties%&apos;&quot; get processidProcessId6016&gt; taskkill /pid 6016 /f 备份节点之一成为新的leader，而broker1已经不在同步备份集合里了。 123&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs:Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 1,2,0 Isr: 2,0 但是，消息仍然没丢： 12345&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic...my test message 1my test message 2^C Step 7: 使用 Kafka Connect 来 导入/导出 数据从控制台写入和写回数据是一个方便的开始，但你可能想要从其他来源导入或导出数据到其他系统。对于大多数系统，可以使用kafka Connect，而不需要编写自定义集成代码。 Kafka Connect是导入和导出数据的一个工具。它是一个可扩展的工具，运行连接器，实现与自定义的逻辑的外部系统交互。在这个快速入门里，我们将看到如何运行Kafka Connect用简单的连接器从文件导入数据到Kafka主题，再从Kafka主题导出数据到文件。 首先，我们首先创建一些“种子”数据用来测试： 1echo -e &quot;foo\nbar&quot; &gt; test.txt windows上： 12&gt; echo foo&gt; test.txt&gt; echo bar&gt;&gt; test.txt 接下来，我们开始2个连接器运行在独立的模式，这意味着它们运行在一个单一的，本地的，专用的进程。我们提供3个配置文件作为参数。首先是Kafka Connect处理的配置，包含常见的配置，例如要连接的Kafka broker和数据的序列化格式。其余的配置文件都指定了要创建的连接器。包括连接器唯一名称，和要实例化的连接器类。以及连接器所需的任何其他配置。 1&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties kafka附带了这些示例的配置文件，并且使用了刚才我们搭建的本地集群配置并创建了2个连接器：第一个是源连接器，从输入文件中读取并发布到Kafka主题中，第二个是接收连接器，从kafka主题读取消息输出到外部文件。 在启动过程中，你会看到一些日志消息，包括一些连接器实例化的说明。一旦kafka Connect进程已经开始，导入连接器应该读取从 1test.txt 和写入到topic 1connect-test ,导出连接器从主题 1connect-test 读取消息写入到文件 1test.sink.txt . 我们可以通过验证输出文件的内容来验证数据数据已经全部导出： 123more test.sink.txt foo bar 注意，导入的数据也已经在Kafka主题 1connect-test 里,所以我们可以使用该命令查看这个主题： 1234bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning &#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false&#125;,&quot;payload&quot;:&quot;foo&quot;&#125;&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false&#125;,&quot;payload&quot;:&quot;bar&quot;&#125;... 连接器继续处理数据，因此我们可以添加数据到文件并通过管道移动： 1echo &quot;Another line&quot; &gt;&gt; test.txt 你应该会看到出现在消费者控台输出一行信息并导出到文件。 Step 8: 使用Kafka Stream来处理数据Kafka Stream是kafka的客户端库，用于实时流处理和分析存储在kafka broker的数据，这个快速入门示例将演示如何运行一个流应用程序。一个WordCountDemo的例子（为了方便阅读，使用的是java8 lambda表达式） 123456789KTable wordCounts = textLines // Split each text line, by whitespace, into words. .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(&quot;W+&quot;))) // Ensure the words are available as record keys for the next aggregate operation. .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value)) // Count the occurrences of each word (record key) and store the results into a table named &quot;Counts&quot;. .countByKey(&quot;Counts&quot;) 它实现了wordcount算法，从输入的文本计算出一个词出现的次数。然而，不像其他的WordCount的例子，你可能会看到，在有限的数据之前，执行的演示应用程序的行为略有不同，因为它的目的是在一个无限的操作，数据流。类似的有界变量，它是一种动态算法，跟踪和更新的单词计数。然而，由于它必须假设潜在的无界输入数据，它会定期输出其当前状态和结果，同时继续处理更多的数据，因为它不知道什么时候它处理过的“所有”的输入数据。 现在准备输入数据到kafka的topic中，随后kafka Stream应用处理这个topic的数据。 1&gt; echo -e &quot;all streams lead to kafka\nhello kafka streams\njoin kafka summit&quot; &gt; file-input.txt 接下来，使用控制台的producer 将输入的数据发送到指定的topic（streams-file-input）中，（在实践中，stream数据可能会持续流入，其中kafka的应用将启动并运行） 12345&gt; bin/kafka-topics.sh --create \ --zookeeper localhost:2181 \ --replication-factor 1 \ --partitions 1 \ --topic streams-file-input 1&gt; cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list localhost:9092 --topic streams-file-input 现在，我们运行 WordCount 处理输入的数据： 1&gt; ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo 不会有任何的STDOUT输出，除了日志，结果不断地写回另一个topic（streams-wordcount-output），demo运行几秒，然后，不像典型的流处理应用程序，自动终止。 现在我们检查WordCountDemo应用，从输出的topic读取。 12345678&gt; ./bin/kafka-console-consumer --zookeeper localhost:2181 --topic streams-wordcount-output --from-beginning --formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer 输出数据打印到控台（你可以使用Ctrl-C停止）： 123456789101112all 1streams 1lead 1to 1kafka 1hello 1kafka 2streams 2join 1kafka 3summit 1^C 第一列是message的key，第二列是message的value，要注意，输出的实际是一个连续的更新流，其中每条数据（即：原始输出的每行）是一个单词的最新的count，又叫记录键“kafka”。对于同一个key有多个记录，每个记录之后是前一个的更新。 三. Kafka核心API与设计理念详解1.Kafka的架构 如上图所示，该图是一个典型的Kafka集群中包含若干Producer，若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 2.Topics和PartitionTopic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。 对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配置KAFKA_HOME/config/server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示： 12345678# The minimum age of a log file to be eligible for deletion 符合删除条件的日志文件的最小生存时间log.retention.hours=168# The maximum size of a log segment file. When this size is reached a new log segment will be created.日志段文件的最大大小。达到此大小后，将创建一个新的日志段。log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according to the retention policies 检查日志段以了解是否可以根据保留策略将其删除的时间间隔log.retention.check.interval.ms=300000 # 300s # If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction. 如果设置了log.cleaner.enable = true，则将启用清理器，然后可以标记单个日志以进行日志压缩。log.cleaner.enable=false 因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。 2.1 创建Topic创建 topic 的序列图如下所示： 流程说明： 1、 controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。2、 controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition： 2.1、 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR 2.2、 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state3、 controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。 2.2 删除topic删除 topic 的序列图如下所示： 流程说明： 1、 controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。2、 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。 3.Producer消息路由Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在KAFKA_HOME/config/server.properties中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。 在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。 Math.abs(“routerKey or groupName”.hashCode()) % ParititionNum 3.1 写入方式producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。 3.2 消息路由producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为： 1、 指定了 patition，则直接使用；2、 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition3、 patition 和 key 都未指定，使用轮询选出一个 patition。 3.3 写入流程producer 写入消息序列图如下所示： 流程说明： 1、 producer 先从 zookeeper 的 “/brokers/…/state” 节点找到该 partition 的 leader2、 producer 将消息发送给该 leader3、 leader 将消息写入本地 log4、 followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK5、 leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK 4.Consumer Group使用Consumer high level API的时候,同一个Topic的一条消息只能被同一个Consumer Group中的一个Consumer消费,但是多个Consumer Group则可以同时消费这一信息. 保证消息消费的顺序性 传统消息中间件存在一个queue中， c1， c2， c3三个消费者从queue中取得的顺序在消息中间件看起来是顺序一致的，但是三个消费者实际处理时可能不一定是顺序的，尤其是多个消费者之间存在业务严格明确依赖的情况下， 比如c1用户下单后，c2再优惠，c3再送积分，此时传统的消息中间件是没有一个很好发办法去处理的。Kafka就可以解决这个问题。 Kafka解决这个问题的办法： 首先规定了一个分区只能有一个消费者，我们可以把有业务依赖性的消息往一个分区中发送 多个分区可以提高并发 既可以满足并发，也可以满足消息的一致性 这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。 5.Push vs. Pull作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。 push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。 对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 6.Kafka delivery guarantee有这么几种可能的delivery guarantee： At most once 消息可能会丢，但绝不会重复传输 At least one 消息绝不会丢，但可能会重复传输 Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。 当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。 接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。 Kafka默认保证At least once，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。 四. Kafka的高可用1. 高可用的由来​ 在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。 如果Producer使用同步模式则Producer会在尝试重新发送message.send.max.retries（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。 如果Producer使用异步模式，则Producer会尝试重新发送message.send.max.retries（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。同时，Kafka的Producer并未对异步模式提供callback接口。 由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。 因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。 2. Kafka HA的设计解析2.1 如何将所有Replica均匀分布到整个集群为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。 Kafka分配Replica的算法如下： 1.将所有Broker（假设共n个Broker）和待分配的Partition排序 2.将第i个Partition分配到第（i mod n）个Broker上 3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上 2.2 Data Replication（副本策略）Kafka的高可靠性的保障来源于其健壮的副本（replication）策略。 2.2.1 消息传递同步策略Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少，Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。 为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。 Consumer读消息也是从Leader读取，只有被commit过的消息才会暴露给Consumer。 Kafka Replication的数据流如下图所示： 2.2.2 ACK前需要保证有多少个备份对于Kafka而言，定义一个Broker是否“活着”包含两个条件： 一是它必须维护与ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。 二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。 Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在KAFKA_HOME/config/server.properties中通过replica.lag.max.messages配置，其默认值是4000）或者Follower超过一定时间（该值可在KAFKA_HOME/config/server.properties中通过replica.lag.time.max.ms来配置，其默认值是10000）未向Leader发送fetch请求。 Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。 需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。 2.2.3 Leader Election算法Leader选举本质上是一个分布式锁，有两种方式实现基于ZooKeeper的分布式锁： 节点名称唯一性：多个客户端创建一个节点，只有成功创建节点的客户端才能获得锁 临时顺序节点：所有客户端在某个目录下创建自己的临时顺序节点，只有序号最小的才获得锁 一种非常常用的选举leader的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在ZooKeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种方式。 Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。 虽然Majority Vote与ISR相比有不需等待最慢的Broker这一优势，但是Kafka作者认为Kafka可以通过Producer选择是否被commit阻塞来改善这一问题，并且节省下来的Replica和磁盘使得ISR模式仍然值得。 2.2.4 如何处理所有Replica都不工作在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案： 1.等待ISR中的任一个Replica“活”过来，并且选它作为Leader 2.选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader 这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 2.2.5 选举Leader最简单最直观的方案是，所有Follower都在ZooKeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（ZooKeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。 但是该方法会有3个问题： 1.split-brain(脑裂) 这是由ZooKeeper的特性引起的，虽然ZooKeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致 2.herd effect(惊群) 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整 3.ZooKeeper负载过重 每个Replica都要为此在ZooKeeper上注册一个Watch，当集群规模增加到几千个Partition时ZooKeeper负载会过重。 Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比ZooKeeper Queue的方式更高效）通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。 4. broker保存消息4.1 存储方式物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下： 4.2 存储策略无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据： 1、 基于时间：log.retention.hours=1682、 基于大小：log.retention.bytes=1073741824 五. Kafka与其他常用MQ对比 ActiveMQ RabbitMQ Kafka 所属社区/公司 Apache Mozikka Public License Apache/LinkendIn 开发语言 Java Erlang Java 支持协议 OpenWire、STOMP、REST、XMPP、AMQP AMQP 仿AMQP 事务 支持 支持（性能会下降） 支持 集群 支持 支持 支持 负载均衡 支持 支持 支持 动态扩容 不支持 不支持 支持（zk） 单机吞吐量TPS 万级 万级 十万级 顺序消息 不支持 不支持 支持 消息确认 支持 支持 支持 消息回溯 不支持 不支持 支持指定分区offset位置的回溯 消息重试 不支持 不支持，但是可以利用消息确认机制实现 不支持，但是可以利用kafka支持指定分区offset位置的回溯，可以实现消息重试。 并发度 高 极高 高 六. SpringBoot整合Kafka1. 在创建好的gradle工程中引入依赖Gadle版本: 6.0 JDK: 1.8 IDEA: 2020.1 123456dependencies &#123; ... compile group: 'org.springframework.boot', name: 'spring-boot-starter-web', version:'2.2.0.RELEASE' compile group: 'org.springframework.kafka', name: 'spring-kafka', version:'2.3.1.RELEASE' ...&#125; 2. 编写配置文件配置文件有两种， 第一中是使用application.yml文件配置， 第二中是使用SpringBoot Java配置类来配置，两种配置如下 applacation.yml1234567891011121314151617181920212223242526272829server: port: 8080spring: jackson: date-format: yyyy-MM-dd HH:mm:ss #日期序列化格式 kafka: bootstrap-servers: 172.22.24.200:9092, 172.22.24.200:9093, 172.22.24.200:9094 # 集群地址， 任意配一台可用地址即可 producer: # 生产者配置 retries: 0 # 重试次数 batch-size: 16384 # 一次最多发送数据量 buffer-memory: 33554432 # 32M批处理缓冲区 key-serializer: org.apache.kafka.common.serialization.StringSerializer # 序列化 value-serializer: org.apache.kafka.common.serialization.StringSerializer linger: # 发送延迟 ms: 1000 acks: "1" # 消息确认 consumer: # 消费者配置 group-id: 0 # group-id enable-auto-commit: false # 是否开启自动提交 auto-commit-interval: 100 # consumer自动向zookeeper提交offset的频率 properties: # 消费超时时间，大小不能超过session.timeout.ms，默认：3000 session: timeout: ms: 15000 key-deserializer: org.apache.kafka.common.serialization.StringDeserializer # 反序列化 value-deserializer: org.apache.kafka.common.serialization.StringDeserializer fetch-max-wait: 300000 # 配置consumer最多等待response多久 max-poll-records: 50 # max.poll.records条数据需要在session.timeout.ms这个时间内处理完 KafkaConfig.java注意: 配置写在yml和java类中均可, 但是某些工厂类如KafkaTemplate则需要在代码中使用@Bean注解交给Spring创建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126package com.gsafety.springbootkafka.config;import lombok.extern.slf4j.Slf4j;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.kafka.annotation.EnableKafka;import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;import org.springframework.kafka.config.KafkaListenerContainerFactory;import org.springframework.kafka.core.*;import org.springframework.kafka.listener.ContainerProperties;import java.util.HashMap;import java.util.Map;/** * @author lg * @Classname KafkaConfig * @Description * @Date 2020-06-29 17:12 */@Slf4j@EnableKafka@Configurationpublic class KafkaConfig &#123; @Value("$&#123;spring.kafka.bootstrap-servers&#125;") private String hosts; public KafkaConfig() &#123; log.info("kafka config init -------------&gt;"); &#125; /** * producer configuration * * @return Map&lt;String, Object&gt; producerConfigs */ @Bean public Map&lt;String, Object&gt; producerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(8); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, hosts); props.put(ProducerConfig.RETRIES_CONFIG, 0); props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); props.put(ProducerConfig.LINGER_MS_CONFIG, 1000); props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); props.put(ProducerConfig.ACKS_CONFIG, "1"); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); return props; &#125; @Bean public ProducerFactory&lt;String, String&gt; producerFactory() &#123; return new DefaultKafkaProducerFactory&lt;&gt;(producerConfigs()); &#125; /** * consumer configuration * * @return Map&lt;String, Object&gt; consumerConfigs */ @Bean public Map&lt;String, Object&gt; consumerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(10); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, hosts); props.put(ConsumerConfig.GROUP_ID_CONFIG, "0"); //自动控制提交offset props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); //提交延迟毫秒数 props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 100); //执行超时时间 props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "15000"); // 每间隔max.poll.interval.ms我们就调用一次poll props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, "300000"); // 一次poll最多返回的记录数 props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "50"); //开始消费位置 earliest/latest/none props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); return props; &#125; @Bean public ConsumerFactory&lt;String, String&gt; consumerFactory() &#123; return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()); &#125; /** * kafka template configuration * * @return KafkaTemplate&lt;String, String&gt; kafkaTemplate */ @Bean public KafkaTemplate&lt;String, String&gt; kafkaTemplate() &#123; return new KafkaTemplate&lt;&gt;(producerFactory()); &#125; /** * 批量消费 * MANUAL 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交 * @return */ @Bean public KafkaListenerContainerFactory&lt;?&gt; batchFactory(ConsumerFactory consumerFactory) &#123; ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(consumerFactory); // topic有5个分区，为了加快消费将并发设置为5，也就是有5个KafkaMessageListenerContainer factory.setConcurrency(5); // 设置拉取时间 factory.getContainerProperties().setPollTimeout(1500); // 开启批量消费 factory.setBatchListener(true); //配置手动提交offset factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL); return factory; &#125;&#125; KafkaAdminConfig该文件可用来创建topic的相关操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.gsafety.springbootkafka.config;import com.gsafety.springbootkafka.constant.MyTopic;import org.apache.kafka.clients.admin.AdminClientConfig;import org.apache.kafka.clients.admin.NewTopic;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.kafka.annotation.EnableKafka;import org.springframework.kafka.core.KafkaAdmin;import org.springframework.stereotype.Component;import java.util.HashMap;import java.util.Map;/** * kafka admin config */@EnableKafka@Configurationpublic class KafkaAdminConfig &#123; // yml配置文件中的变量 @Value("$&#123;spring.kafka.bootstrap-servers&#125;") private String hosts; @Bean public KafkaAdmin admin() &#123; Map&lt;String, Object&gt; configs = new HashMap&lt;&gt;(1); configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, hosts); return new KafkaAdmin(configs); &#125; @Bean public NewTopic topic1()&#123; // 第一个是参数是topic名字，第二个参数是分区个数 // 第三个是topic的复制因子个数 // -----------------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;当broker个数为1个时会创建topic失败， //提示：replication factor: 2 larger than available brokers: 1 //只有在集群中才能使用kafka的备份功能 return new NewTopic(MyTopic.TOPIC1, 5, (short) 1); &#125; @Bean public NewTopic topic2()&#123; return new NewTopic(MyTopic.TOPIC2, 5, (short) 1); &#125; @Bean public NewTopic topic3()&#123; return new NewTopic(MyTopic.TOPIC3, 5, (short) 3); &#125; @Bean public NewTopic topic4()&#123; return new NewTopic(MyTopic.TOPIC4, 3, (short) 2); &#125;&#125; 3. 编写Kafka通用工具类 可以先创建一个工具类接口, 然后再去实现这个接口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.gsafety.springbootkafka.service;import javafx.util.Pair;import org.apache.kafka.clients.admin.TopicListing;import org.springframework.kafka.support.SendResult;import org.springframework.util.concurrent.ListenableFuture;import java.util.List;import java.util.Map;/** * Created with IntelliJ IDEA. * * @author: lg * @Date: 2020/6/20 0020 * @Time: 11:37 * @Description: KafkaService */public interface KafkaUtils &#123; /** * 发送数据到指定的topic中 * * @param topicName topic名称 * @param msg 数据 * @return 发送的状态 */ Boolean sendDataToTopic(String topicName, String msg); /** * 发送数据到指定的topic和key中 * * @param topicName topic名称 * @param key key * @param msg 消息 * @return 发送状态 */ ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAndKey(String topicName, String key, String msg); /** * 发送数据到指定的topic的中 * * @param topic topic名称 * @param partition 分区名称 * @param key 指定的key * @param msg 消息 * @return 发送状态 */ ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAppointPartition(String topic, Integer partition, String key, String msg); /** * 校验topic是否已经存在于kafka中 * * @param topicName topic的名称 * @return 是否存在的状态 */ Boolean isExistTopic(String topicName); /** * 创建指定的topic * * @param topicName topic的名称 * @param topicPartition 话题创建的分区 * @param replicationFactor 话题创建的副本， 不能大于broker的数量 * @return 是否创建成功 */ Boolean createTopic(String topicName, Integer topicPartition, short replicationFactor); /** * 删除话题 * @param topicNames 话题名称 * @return 删除结果 */ List&lt;Pair&lt;String, Boolean&gt;&gt; deleteTopic(String[] topicNames); /** * 获取所有的topic * @return topic集合 */ Map&lt;String, TopicListing&gt; getTopics(); ...&#125; 实现类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182package com.gsafety.springbootkafka.service.impl;import com.gsafety.springbootkafka.config.KafkaConfig;import com.gsafety.springbootkafka.service.KafkaUtils;import javafx.util.Pair;import lombok.extern.slf4j.Slf4j;import org.apache.kafka.clients.admin.*;import org.apache.kafka.common.KafkaFuture;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.kafka.core.KafkaAdmin;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.kafka.support.SendResult;import org.springframework.stereotype.Component;import org.springframework.util.concurrent.ListenableFuture;import java.util.*;/** * Created with IntelliJ IDEA. * * @author: lg * @Date: 2020/6/20 0020 * @Time: 20:20 * @Description: Kafka封装操作类 */@Component@Slf4jpublic class KafkaUtilsImpl implements KafkaUtils &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Autowired private KafkaConfig kafkaConfig; @Autowired private KafkaAdmin kafkaAdmin; public void setTimeout(int timeout) &#123; this.timeout = timeout; &#125; private int timeout = 6000; /** * 发送数据到指定的topic中 * * @param topicName topic名称 * @param msg 数据 * @return 发送的状态 */ @Override public Boolean sendDataToTopic(String topicName, String msg) &#123; try &#123; ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; result = kafkaTemplate.send(topicName, msg); result.get(); return !result.completable().isCompletedExceptionally(); &#125; catch (Exception e) &#123; log.info("发送普通消息失败, topic=&#123;&#125;, msg=&#123;&#125;, failure Message=&#123;&#125;", topicName, msg, e.getMessage()); return false; &#125; &#125; /** * 发送数据到指定的topic和key中 * * @param topicName topic名称 * @param key key * @param msg 消息 * @return 发送状态 */ @Override public ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAndKey(String topicName, String key, String msg) &#123; return kafkaTemplate.send(topicName, key, msg); &#125; /** * 发送数据到指定的topic的中 * * @param topic topic名称 * @param partition 分区名称 * @param key 指定的key * @param msg 消息 * @return 发送状态 */ @Override public ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendDataToTopicAppointPartition(String topic, Integer partition, String key, String msg) &#123; return kafkaTemplate.send(topic, partition, key, msg); &#125; /** * 校验topic是否已经存在于kafka中 * * @param topicName topic的名称 * @return 是否存在的状态 */ @Override public Boolean isExistTopic(String topicName) &#123; try (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123; ListTopicsOptions listTopicsOptions = new ListTopicsOptions(); listTopicsOptions.listInternal(true); ListTopicsResult listTopicsResult = adminClient.listTopics(listTopicsOptions); Boolean flag = listTopicsResult.names().get().contains("topicName"); return flag; &#125; catch (Exception e) &#123; log.info("校验topic: &#123;&#125; 是否已经存在于kafka中异常 &#123;&#125;", topicName, e.getMessage()); return false; &#125; &#125; /** * 创建指定的topic * * @param topicName topic的名称 * @param topicPartition 话题创建的分区 * @param replicationFactor 话题创建的副本， 不能大于broker的数量 * @return 是否创建成功 */ @Override public Boolean createTopic(String topicName, Integer topicPartition, short replicationFactor) &#123; try (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123; Boolean existTopic = isExistTopic(topicName); if (existTopic) &#123; return existTopic; &#125; NewTopic newTopic = new NewTopic(topicName, topicPartition, replicationFactor); List&lt;NewTopic&gt; newTopics = Collections.singletonList(newTopic); adminClient.createTopics(newTopics); return isExistTopic(topicName); &#125; catch (Exception e) &#123; log.error("创建话题&#123;&#125;失败, Cause by: &#123;&#125;", topicName, e.getMessage()); return false; &#125; &#125; /** * 删除指定topic(如果broker那没有设置允许删除topic的话，此调用会持续等待最终超时返回) * * @param topicNames 待删除的topic * @return 删除是否成功 */ @Override public List&lt;Pair&lt;String, Boolean&gt;&gt; deleteTopic(String[] topicNames) &#123; List&lt;Pair&lt;String, Boolean&gt;&gt; result = new ArrayList&lt;&gt;(); try (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123; DeleteTopicsOptions options = new DeleteTopicsOptions(); options.timeoutMs(timeout); DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(Arrays.asList(topicNames), options); for (Map.Entry&lt;String, KafkaFuture&lt;Void&gt;&gt; e : deleteTopicsResult.values().entrySet()) &#123; String topic = e.getKey(); KafkaFuture&lt;Void&gt; future = e.getValue(); future.get(); result.add(new Pair&lt;&gt;(topic, !future.isCompletedExceptionally())); &#125; return result; &#125; catch (Exception e) &#123; log.error("删除话题&#123;&#125;失败, Cause by: &#123;&#125;", String.join(",", topicNames), e.getMessage()); return result; &#125; &#125; /** * 获取所有的topic * * @return topic集合 */ @Override public Map&lt;String, TopicListing&gt; getTopics() &#123; ListTopicsOptions options = new ListTopicsOptions(); //设置超时时间 options.timeoutMs(timeout); //不列出kafka内部topic options.listInternal(false); try (AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfig())) &#123; ListTopicsResult listTopicsResult = adminClient.listTopics(options); return listTopicsResult.namesToListings().get(); &#125; catch (Exception e) &#123; log.error("查询话题失败失败, Cause by: &#123;&#125;", e.getMessage()); return null; &#125; &#125;&#125; 注: @Slf4j注解是集成了lombok后可以方便打印日志使用的 4. 编写Kafka producer通用工具类编写好后, 可以独立出专门用来发送消息的producer类, 代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.gsafety.springbootkafka.producer;import com.alibaba.fastjson.JSON;import com.gsafety.springbootkafka.constant.MyTopic;import com.gsafety.springbootkafka.entity.KafkaMessage;import com.gsafety.springbootkafka.service.KafkaUtils;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.kafka.support.SendResult;import org.springframework.stereotype.Component;import org.springframework.util.concurrent.ListenableFuture;import java.util.Objects;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ExecutionException;import java.util.concurrent.atomic.AtomicInteger;/** * @author lg * @Classname KafkaProducer * @Description 生产者 * @Date 2020-06-30 11:00 */@Slf4j@Componentpublic class KafkaProducer &#123; @Autowired private KafkaUtils kafkaUtils; public void send(String topic, String key, KafkaMessage kafkaMessage) &#123; String msg = JSON.toJSONString(kafkaMessage); kafkaUtils.sendDataToTopicAndKey(topic, key, msg).addCallback(success -&gt; &#123; // 消息发送到的topic String successTopic = Objects.requireNonNull(success).getRecordMetadata().topic(); // 消息发送到的分区 int partition = success.getRecordMetadata().partition(); // 消息在分区内的offset long offset = success.getRecordMetadata().offset(); log.info("发送普通消息, topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;", topic, key, msg); &#125;, failure -&gt; &#123; log.info("发送普通消息失败, topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;, failure Message=&#123;&#125;", topic, key, msg, failure.getMessage()); &#125;); &#125; public boolean send(String topic, Integer partition, String key, KafkaMessage kafkaMessage) &#123; // 或者 JSON.toJSONString(kafkaMessage, SerializerFeature.WriteDateUseDateFormat); String msg = JSON.toJSONStringWithDateFormat(kafkaMessage, "yyyy-MM-dd HH:mm:ss"); try &#123; ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; sendResultListenableFuture = kafkaUtils.sendDataToTopicAppointPartition(topic, partition, key, msg); log.info("发送普通消息，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;", topic, key, msg); CompletableFuture&lt;SendResult&lt;String, String&gt;&gt; completable = sendResultListenableFuture.completable(); completable.get(); return !completable.isCompletedExceptionally(); &#125; catch (Exception e) &#123; log.error("发送普通消息失败，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;", topic, key, msg); return false; &#125; &#125; public void send(String topic, Integer partition, String key, String kafkaMessage) &#123; kafkaUtils.sendDataToTopicAppointPartition(topic, partition, key, kafkaMessage); log.info("发送普通消息，topic=&#123;&#125;,key=&#123;&#125;,msg=&#123;&#125;", topic, key, kafkaMessage); &#125;&#125; 首先将KafkaUtils自动注入。 然后就可以使用KafkaUtils中提供的API按照自己的需求进行二次封装，实现自己想要的逻辑处理。 发送消息后，可以通过其addCallback方法来处理发送成功或者失败后的逻辑，或者接收ListenableFuture类型的返回值并且使用try-catch来作逻辑判断，上述两种方式在代码中均有体现。 5. 编写Kafka consumerconsumer主要利用SpringBoot 提供的@KafkaListener注解来实现的。下面先来简单介绍一下@KafkaListener注解的相关内容： 123456789101112131415161718192021222324252627...@Target(&#123; ElementType.TYPE, ElementType.METHOD, ElementType.ANNOTATION_TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@MessageMapping@Documented@Repeatable(KafkaListeners.class)public @interface KafkaListener &#123; // 消费者的id（唯一），当GroupId没有被配置的时候，默认id为GroupId，支持SpEL表达式#&#123;&#125; String id() default ""; // 这里面配置的是监听容器工厂BeanName，常用于批量消费时指定消费工厂 String containerFactory() default ""; // 需要监听的Topic，可监听多个 String[] topics() default &#123;&#125;; // 可配置更加详细的监听信息，必须监听某个Topic中的指定分区，或者从offset为200的偏移量开始监听 TopicPartition[] topicPartitions() default &#123;&#125;; // 监听异常处理器，配置BeanName String errorHandler() default ""; // 消费组ID String groupId() default ""; // id是否为GroupId boolean idIsGroup() default true; // 消费者Id前缀 String clientIdPrefix() default ""; // 真实监听容器的BeanName，需要在 BeanName前加 "__" String beanRef() default "__listener"; ...&#125; 实现consumer类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package com.gsafety.springbootkafka.consumer;import lombok.extern.slf4j.Slf4j;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.kafka.annotation.PartitionOffset;import org.springframework.kafka.annotation.TopicPartition;import org.springframework.kafka.support.Acknowledgment;import org.springframework.stereotype.Component;import java.util.List;import java.util.Optional;/** * @author lg * @Classname KafkaConsumer * @Description 消费者 * @Date 2019-11-06 17:01 */@Component@Slf4jpublic class KafkaConsumer &#123; /** * 消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）; * （1）使用一个全新的"group.id"（就是之前没有被任何消费者使用过）; * （2）指定"auto.offset.reset"参数的值为earliest; * @param content 消息内容 */ @KafkaListener(id = "client-1", topics = "topic4", groupId = "group2") public void processMessage2(String content) &#123; log.info("消费者topic4-1监听消息,消息内容=[&#123;&#125;]", content);s &#125; @KafkaListener(id = "client-2", topics = "topic4", groupId = "group2") public void processMessage3(String content) &#123; log.info("消费者topic4-2监听消息,消息内容=[&#123;&#125;]", content); &#125; /** * 批量消费 * containerFactory: 需要声明消费工厂名 * batchFactory： 在KafakaConfig中配置的消费者工厂类 * * @param ack 消息确认对象 * @param records 消息内容 */ @KafkaListener(id = "client-3", topics = "topic3", groupId = "group1", containerFactory = "batchFactory") public void processMessage(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack) &#123; log.info("client-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;", Thread.currentThread().getId(), records.size()); try &#123; for (ConsumerRecord&lt;?, ?&gt; record : records) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = record.value(); String topic = record.topic(); long offset = record.offset(); log.info("client-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]", topic, offset, message); &#125; &#125; // 手动提交，设置offset ack.acknowledge(); &#125; catch (Exception e) &#123; log.error("client-3监听异常&#123;&#125;", e.getMessage(), e); &#125; &#125; /** * id是消费者监听容器 * 配置topic和分区：监听两个topic，分别为topic1、topic2，topic1只接收分区0，3的消息， * topic2接收分区0和分区1的消息，但是分区1的消费者初始位置为5 * * @param record 消费内容 */ @KafkaListener(id = "client-4", clientIdPrefix = "my", topicPartitions = &#123;@TopicPartition(topic = "topic1", partitions = &#123;"0", "3"&#125;), @TopicPartition(topic = "topic2", partitions = "0", partitionOffsets = @PartitionOffset(partition = "1", initialOffset = "4")) &#125;) public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; log.info("topic1消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;", record.topic(), record.key(), record.value()); &#125; @KafkaListener(id = "client-5", topics = &#123;"topic1", "topic2"&#125;) public void listen2(ConsumerRecord&lt;?, ?&gt; record) &#123; log.info("topic1,topic2 多主题消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;", record.topic(), record.key(), record.value()); &#125;&#125; 至此，我们的生产者和消费者就已经都编写好了，至于要简单的编写一下单元测试或者Controller实现RESTful API就可以开始验证和简单的使用Kafka了。 6. 测试启动12345...2020-06-30 15:38:20.332 INFO 20320 --- [ topic-3-2-C-1] o.s.k.l.KafkaMessageListenerContainer : group1: partitions assigned: [topic3-4]2020-06-30 15:38:20.336 INFO 20320 --- [ topic-3-1-C-1] o.s.k.l.KafkaMessageListenerContainer : group1: partitions assigned: [topic3-3, topic3-2]2020-06-30 15:38:20.336 INFO 20320 --- [ topic-3-0-C-1] o.s.k.l.KafkaMessageListenerContainer : group1: partitions assigned: [topic3-1, topic3-0]... 在消费工厂或者配置中设置并发量，小于或等于Topic的分区数factory.setConcurrency(3); 我们设置concurrency为3，也就是将会启动3条线程进行监听，我们创建的topic则是有5个partition，意味着将有2条线程分配到2个partition和1条线程分配到1个partition。我们可以看到这段日志的最后3行，这就是每条线程分配到的partition.注意：设置的并发量不能大于partition的数量，如果需要提高吞吐量，可以通过增加partition的数量达到快速提升吞吐量的效果。 1. 简单发送-订阅发送： 12345678910111213/** * 发送数据到指定的topic的中 * * @param topic topic名称 * @param partition 分区名称 * @param key 指定的key * @param msg 消息 * @return 发送状态*/public void send(String topic, String key, KafkaMessage kafkaMessage) &#123; String msg = JSON.toJSONString(kafkaMessage); kafkaUtils.sendDataToTopicAndKey(topic, key, msg);&#125; 订阅： 1234567891011/** * 消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）; * （1）使用一个全新的"group.id"（就是之前没有被任何消费者使用过）; * （2）指定"auto.offset.reset"参数的值为earliest; * * @param content 消息内容 */@KafkaListener(id = "topic4-1", topics = "topic4", groupId = "group2")public void processMessage2(String content) &#123; log.info("消费者topic4-1监听消息,消息内容=[&#123;&#125;]", content);&#125; 控制台结果： 2. 批量消费 重新创建一份新的消费者配置，配置为一次拉取5条消息 创建一个监听容器工厂，设置其为批量消费并设置并发量为5，这个并发量根据分区数决定，必须小于等于分区数，否则会有线程一直处于空闲状态 创建一个分区数为5的Topic 创建监听方法，设置消费id为batch，clientID前缀为batch，监听topic3，使用batchFactory工厂创建该监听容器 该方法在KafkaConfig.java中 1234567891011121314151617.../** * 批量消费 * @return */@Beanpublic ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; batchFactory() &#123; ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs())); //设置并发量，小于或等于Topic的分区数 factory.setConcurrency(3); // 开启批量消费 factory.setBatchListener(true); return factory;&#125;... 生产者代码： 123456789@ApiOperation(value = "向Topic3中发送消息", notes="向Topic3中发送消息")@PostMapping(path = "/topic3")public ResponseEntity&lt;Boolean&gt; sendTopic3(@RequestBody KafkaMessage kafkaMessage) &#123; boolean flag = false; for (int i = 0; i &lt; 5; i++) &#123; flag = kafkaProducer.send(MyTopic.TOPIC3, 2, "topic.*", kafkaMessage); &#125; return ResponseEntity.ok(flag);&#125; 消费者代码： 123456789101112131415161718192021222324/** * 批量消费 * containerFactory: 需要声明消费工厂名 * * @param records 消息内容 */@KafkaListener(id = "topic-3", topics = "topic3", groupId = "group1", containerFactory = "batchFactory", clientIdPrefix = "batch")public void processMessage(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records) &#123; log.info("topic-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;", Thread.currentThread().getId(), records.size()); try &#123; for (ConsumerRecord&lt;?, ?&gt; record : records) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = record.value(); String topic = record.topic(); long offset = record.offset(); log.info("topic-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]", topic, offset, message); &#125; &#125; &#125; catch (Exception e) &#123; log.error("topic-3监听异常&#123;&#125;", e.getMessage(), e); &#125;&#125; 控制台结果：max.poll.records设置为5（一次poll最多返回的记录数） 3. 确认机制与2. 批量消费的代码相似，使用Kafka的Ack机制比较简单，只需简单的三步即可： 设置ENABLE_AUTO_COMMIT_CONFIG=false，禁止自动提交 设置AckMode=MANUAL_IMMEDIATE 监听方法加入Acknowledgment ack 参数 Kafka是通过最新保存偏移量进行消息消费的，而且确认消费的消息并不会立刻删除，所以我们可以重复的消费未被删除的数据，当第一条消息未被确认，而第二条消息被确认的时候，Kafka会保存第二条消息的偏移量，也就是说第一条消息再也不会被监听器所获取，除非是根据第一条消息的偏移量手动获取。 拒绝消息只要在监听方法中不调用ack.acknowledge()即可 配置： 12345678910111213141516171819202122232425262728293031323334.../** * 批量消费 * @return */@Beanpublic ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; batchFactory() &#123; ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs())); //设置并发量，小于或等于Topic的分区数 factory.setConcurrency(3); // 开启批量消费 factory.setBatchListener(true); //配置手动提交offset factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL); return factory;&#125;/** * consumer configuration * * @return Map&lt;String, Object&gt; consumerConfigs */@Beanpublic Map&lt;String, Object&gt; consumerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(10); ... //自动控制提交offset,注意此处设置自动提交为false的意思时offset从由kafka自动提交转为由Spring自动提交了，实现真正的手动提交还需要在消费工厂类中配合factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);或者kafka-listener-ack-mode: manual 参数配置， 才能实现真正的手动提交 props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); ... return props;&#125;... 消费者代码： 1234567891011121314151617181920212223242526/** * 批量消费 * containerFactory: 需要声明消费工厂名 * * @param records 消息内容 */@KafkaListener(id = "topic-3", topics = "topic3", groupId = "group1", containerFactory = "batchFactory", clientIdPrefix = "batch")public void processMessage(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack) &#123; log.info("topic-3 开始监听消息, Thread ID: &#123;&#125;， records size: &#123;&#125;", Thread.currentThread().getId(), records.size()); try &#123; for (ConsumerRecord&lt;?, ?&gt; record : records) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = record.value(); String topic = record.topic(); long offset = record.offset(); log.info("topic-3监听消息,topic=&#123;&#125;, offset=&#123;&#125;, 消息内容=[&#123;&#125;]", topic, offset, message); &#125; &#125; // 手动提交，设置offset, 确认消息被消费 ack.acknowledge(); &#125; catch (Exception e) &#123; log.error("topic-3监听异常&#123;&#125;", e.getMessage(), e); &#125;&#125; 编写测试方法，运行后可以方法监听方法能收到消息，紧接着注释ack.acknowledge()方法，重新测试，同样你会发现监听容器能接收到消息，这个时候如果你重启项目还是可以看到未被确认的那几条消息。 4. 多主题订阅消费者： 1234@KafkaListener(id = "topic1-2", topics = &#123;"topic1", "topic2"&#125;)public void listen2(ConsumerRecord&lt;?, ?&gt; record) &#123; log.info("topic1, topic2 多主题消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;", record.topic(), record.key(), record.value());&#125; 结果： 5. 多主题指定分区指定偏移量订阅消费者： 1234567891011121314151617/** * id是消费者监听容器 * 配置topic和分区：监听两个topic，分别为topic1、topic2，topic1只接收分区0，3的消息， * topic2接收分区0和分区1的消息，但是分区1的消费者初始位置为5 * * @param record 消费内容 */@KafkaListener(id = "topic1-topic2", clientIdPrefix = "my", topicPartitions = &#123; @TopicPartition(topic = "topic1", partitions = &#123;"0", "3"&#125;), @TopicPartition(topic = "topic2", partitions = "0", partitionOffsets = @PartitionOffset(partition = "1", initialOffset = "4")) &#125;)public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; log.info("topic1-topic2消息监听，topic=&#123;&#125;,key=&#123;&#125;,value=&#123;&#125;", record.topic(), record.key(), record.value());&#125; 控制台： 向topic1的分区2中发送消息，topic1-topic2未监听到，因为我们只监听了topic1的 0 3分区，topic2的 0 1分区。 自定义offset：auto.offset.reset=&quot;earliest&quot;时会从设置的initialOffset开始消费 七. 扩展1. Kafka常用命令12345678910111213141516171819202122232425262728293031323334353637383940414243# 启动kafka服务，集群则在不同服务器上各自执行此命令./bin/kafka-server-start.sh config/server.properties# 创建主题（4个分区，2个副本）./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic &lt;topic-name&gt;# 查看服务器中所有的topic./kafka-topics.sh --list --zookeeper localhost:2181# 查看集群描述# 结果说明：# 这是输出的解释。第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。由于我们只有一个分 区用于此主题，因此只有一行。# “leader”是负责给定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。（因为在kafka中 如果有多个副本的话，就会存在leader和follower的关系，表示当前这个副本为leader所在的broker是哪一个） # “replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。（所有副本列表0,1,2） # “isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。（可用的列表数）./kafka-topics.sh --describe --zookeeper localhost:2181# 查看某个topic详情./kafka-topics.sh --topic &lt;topic-name&gt; --describe --zookeeper localhost:2181# 删除某个topic./kafka-topics.sh --delete --zookeeper localhost:2181 --topic &lt;topic-name&gt;# 显示某个消费组的消费详情（0.10.1.0版本+）./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group &lt;group-name&gt;# 新消费者列表查询（支持0.10版本+）./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list# 启动生产者 发送消息./kafka-console-producer.sh --broker-list localhost:9092 --topic &lt;topic-name&gt;# 启动消费者 接收消息 ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic &lt;topic-name&gt;# 高级点的用法./kafka-simple-consumer-shell.sh --brist localhost:9092 --topic test --partition 0 --offset 1234 --max-messages 10# 查看消费组详情./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group &lt;group-name&gt; --describe# 查询topic的最大（小）offset 最大：--time -1 最小： --time -2./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic &lt;topic-name&gt; --time -1 zookeeper: 123456789101112131415161718192021222324252627282930313233343536# 连接 zookeeper./zookeeper-shell.sh 127.0.0.1:2181ls /[zookeeper, brokers]#查看broker的idls /brokers/ids[3, 2, 0]#查看消息ls /brokers/topics[new, __consumer_offsets, test]#查看删除的消息ls /admin/delete_topics[account]#查看broker的信息get /brokers/ids/0&#123;"jmx_port":-1,"timestamp":"1516184048700","endpoints":["PLAINTEXT://hs01:9092"],"host":"hs01","version":2,"port":9092&#125;cZxid = 0x29000048b2ctime = Wed Jan 17 18:14:08 CST 2018mZxid = 0x29000048b2mtime = Wed Jan 17 18:14:08 CST 2018pZxid = 0x29000048b2cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x261033b8886032adataLength = 119numChildren = 0#删除消息rmr /brokers/topics/testrmr /admin/delete_topics/test 2. Kafka管理工具 KafkaOffsetMonitor：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。 Kafka Web Console：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。 Kafka Manager：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。 3. Spring Boot Kafka参数配置 生产者 重要配置 1234567891011121314151617181920212223242526272829303132# 高优先级配置# 以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接spring.kafka.producer.bootstrap-servers=TopKafka1:9092,TopKafka2:9092,TopKafka3:9092 # 设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。spring.kafka.producer.retries=0 # 每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求，# 这有助于提升客户端和服务端之间的性能，此配置控制默认批量大小（以字节为单位），默认值为16384spring.kafka.producer.batch-size=16384 # producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。这项设置将和producer能够使用的总内存相关，但并不是一个硬性的限制，因为不是producer使用的所有内存都是用于缓存。一些额外的内存会用于压缩（如果引入压缩机制），同样还有一些用于维护请求。spring.kafka.producer.buffer-memory=33554432 # key的Serializer类，实现了org.apache.kafka.common.serialization.Serializer接口spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer # 值的Serializer类，实现了org.apache.kafka.common.serialization.Serializer接口spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer # procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：# acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。# acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。# acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。# 可以设置的值为：all, -1, 0, 1spring.kafka.producer.acks=-1 # 当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪spring.kafka.producer.client-id=1 # producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。压缩最好用于批量处理，批量处理消息越多，压缩性能越好spring.kafka.producer.compression-type=none 其他配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 中优先级配置# 以毫秒为单位的时间，是在我们强制更新metadata的时间间隔。即使我们没有看到任何partition leadership改变。默认值：5 * 60 * 1000 = 300000spring.kafka.producer.properties.metadata.max.age.ms=300000 # producer组将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。通常来说，这只有在记录产生速度大于发送速度的时候才能发生。然而，在某些条件下，客户端将希望降低请求的数量，甚至降低到中等负载一下。这项设置将通过增加小的延迟来完成–即，不是立即发送一条记录，producer将会等待给定的延迟时间以允许其他消息记录发送，这些消息记录可以批量处理。这可以认为是TCP种Nagle的算法类似。这项设置设定了批量处理的更高的延迟边界：一旦我们获得某个partition的batch.size，他将会立即发送而不顾这项设置，然而如果我们获得消息字节数比这项设置要小的多，我们需要“linger”特定的时间以获取更多的消息。 这个设置默认为0，即没有延迟。设定linger.ms=5，例如，将会减少请求数目，但是同时会增加5ms的延迟。spring.kafka.producer.properties.linger.ms=0 # 发送数据时的缓存空间大小，默认：128 * 1024 = 131072spring.kafka.producer.properties.send.buffer.bytes=131072 # socket的接收缓存空间大小,当阅读数据时使用，默认：32 * 1024 = 32768spring.kafka.producer.properties.receive.buffer.bytes=32768 # 请求的最大字节数。这也是对最大记录尺寸的有效覆盖。注意：server具有自己对消息记录尺寸的覆盖，这些尺寸和这个设置不同。此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。默认：1 * 1024 * 1024 = 1048576spring.kafka.producer.properties.max.request.size=1048576 # 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连，默认值：50spring.kafka.producer.properties.reconnect.backoff.ms=50 # producer客户端连接一个kafka服务（broker）失败重连的总时间，每次连接失败，重连时间都会指数级增加，每次增加的时间会存在20%的随机抖动，以避免连接风暴。默认：1000# spring.kafka.producer.properties.reconnect.backoff.max.ms=1000 # 控制block的时长,当buffer空间不够或者metadata丢失时产生block，默认：60 * 1000 = 60000spring.kafka.producer.properties.max.block.ms=60000 # 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中，默认：100spring.kafka.producer.properties.retry.backoff.ms=100 # metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口退出后，我们会擦除并重写最老的窗口，默认：30000spring.kafka.producer.properties.metrics.sample.window.ms=30000 # 用于维护metrics的样本数，默认：2spring.kafka.producer.properties.metrics.num.samples=2 # 用于metrics的最高纪录等级。# spring.kafka.producer.properties.metrics.recording.level=Sensor.RecordingLevel.INFO.toString() # 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计#spring.kafka.producer.properties.metric.reporters=Collections.emptyList() # kafka可以在一个connection中发送多个请求，叫作一个flight,这样可以减少开销，但是如果产生错误，可能会造成数据的发送顺序改变,默认是5 (修改）spring.kafka.producer.properties.max.in.flight.requests.per.connection=5 # 关闭连接空闲时间，默认：9 * 60 * 1000 = 540000spring.kafka.producer.properties.connections.max.idle.ms=540000 # 分区类，默认：org.apache.kafka.clients.producer.internals.DefaultPartitionerspring.kafka.producer.properties.partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner # 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认：30 * 1000 = 30000spring.kafka.producer.properties.request.timeout.ms=30000 # 用户自定义interceptor。#spring.kafka.producer.properties.interceptor.classes=none # 是否使用幂等性。如果设置为true，表示producer将确保每一条消息都恰好有一份备份；如果设置为false，则表示producer因发送数据到broker失败重试使，可能往数据流中写入多分重试的消息。#spring.kafka.producer.properties.enable.idempotence=false # 在主动中止正在进行的事务之前，事务协调器将等待生产者的事务状态更新的最长时间（以ms为单位）。#spring.kafka.producer.properties.transaction.timeout.ms=60000 # 用于事务传递的TransactionalId。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的TransactionalId的事务已经完成。 如果没有提供TransactionalId，则生产者被限制为幂等传递。请注意，如果配置了TransactionalId，则必须启用enable.idempotence。默认值为空，这意味着无法使用事务。#spring.kafka.producer.properties.transactional.id=null# 连接风暴#应用启动的时候，经常可能发生各应用服务器的连接数异常飙升的情况。假设连接数的设置为：min值3,max值10，正常的业务使用连接数在5个左右，当重启应用时，各应用连接数可能会飙升到10个，瞬间甚至还有可能部分应用会报取不到连接。启动完成后接下来的时间内，连接开始慢慢返回到业务的正常值。这就是所谓的连接风暴。 消费者 重要配置 1234567891011121314151617181920212223242526272829303132333435363738394041# 以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接spring.kafka.consumer.bootstrap-servers=TopKafka1:9092,TopKafka2:9092,TopKafka3:9092 # 用来唯一标识consumer进程所在组的字符串，如果设置同样的group id，表示这些processes都是属于同一个consumer group，默认：&quot;&quot;spring.kafka.consumer.group-id=TyyLoveZyy # max.poll.records条数据需要在session.timeout.ms这个时间内处理完，默认：500spring.kafka.consumer.max-poll-records=500 # 消费超时时间，大小不能超过session.timeout.ms，默认：3000spring.kafka.consumer.heartbeat-interval=3000 # 如果为真，consumer所fetch的消息的offset将会自动的同步到zookeeper。这项提交的offset将在进程挂掉时，由新的consumer使用，默认：truespring.kafka.consumer.enable-auto-commit=true # consumer自动向zookeeper提交offset的频率，默认：5000spring.kafka.consumer.auto-commit-interval=5000 # 没有初始化的offset时，可以设置以下三种情况：（默认：latest）# earliest# 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费# latest# 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据# none# topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常spring.kafka.consumer.auto-offset-reset=earliest # 每次fetch请求时，server应该返回的最小字节数。如果没有足够的数据返回，请求会等待，直到足够的数据才会返回。默认：1spring.kafka.consumer.fetch-min-size=1 # Fetch请求发给broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长。这个配置就是来配置consumer最多等待response多久。spring.kafka.consumer.fetch-max-wait=500 # 消费者进程的标识。如果设置一个人为可读的值，跟踪问题会比较方便。。默认：&quot;&quot;spring.kafka.consumer.client-id=1 # key的反序列化类。实现了org.apache.kafka.common.serialization.Deserializer接口spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer # 值的反序列化类。实现了org.apache.kafka.common.serialization.Deserializer接口spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer 其他配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# consumer是通过拉取的方式向服务端拉取数据，当超过指定时间间隔max.poll.interval.ms没有向服务端发送poll()请求，而心跳heartbeat线程仍然在继续，会认为该consumer锁死，就会将该consumer退出group，并进行再分配。默认：300000spring.kafka.consumer.properties.max.poll.interval.ms=300000 # 会话的超时限制。如果consumer在这段时间内没有发送心跳信息，则它会被认为挂掉了，并且reblance将会产生，必须在[group.min.session.timeout.ms, group.max.session.timeout.ms]范围内。默认：10000spring.kafka.consumer.properties.session.timeout.ms=10000 # 在“range”和“roundrobin”策略之间选择一种作为分配partitions给consumer 数据流的策略； 循环的partition分配器分配所有可用的partitions以及所有可用consumer 线程。它会将partition循环的分配到consumer线程上。如果所有consumer实例的订阅都是确定的，则partitions的划分是确定的分布。循环分配策略只有在以下条件满足时才可以：（1）每个topic在每个consumer实例上都有同样数量的数据流。（2）订阅的topic的集合对于consumer group中每个consumer实例来说都是确定的。spring.kafka.consumer.properties.partition.assignment.strategy=range # 一次fetch请求，从一个broker中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。默认：50 * 1024 * 1024 = 52428800spring.kafka.consumer.properties.fetch.max.bytes=52428800 # Metadata数据的刷新间隔。即便没有任何的partition订阅关系变更也能执行。默认：5 * 60 * 1000 = 300000spring.kafka.consumer.properties.metadata.max.age.ms=300000 # 一次fetch请求，从一个partition中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。broker、topic都会对producer发给它的message size做限制。所以在配置这值时，可以参考broker的message.max.bytes 和 topic的max.message.bytes的配置。默认：1 * 1024 * 1024 = 1048576spring.kafka.consumer.properties.max.partition.fetch.bytes=1048576 # 最大发送的TCP大小。默认：128 * 1024 = 131072，如果设置为 -1 则为操作系统默认大小spring.kafka.consumer.properties.send.buffer.bytes=131072 # 消费者接受缓冲区的大小。这个值在创建Socket连接时会用到。取值范围是：[-1, Integer.MAX]。默认值是：65536 （64 KB），如果值设置为-1，则会使用操作系统默认的值。默认：64 * 1024 = 65536spring.kafka.consumer.properties.receive.buffer.bytes=65536 # 连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连，默认：50spring.kafka.consumer.properties.reconnect.backoff.ms=50 # producer客户端连接一个kafka服务（broker）失败重连的总时间，每次连接失败，重连时间都会指数级增加，每次增加的时间会存在20%的随机抖动，以避免连接风暴。默认：1000spring.kafka.consumer.properties.reconnect.backoff.max.ms=1000 # 在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中，默认：100spring.kafka.consumer.properties.retry.backoff.ms=100 # metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口退出后，我们会擦除并重写最老的窗口，默认：30000spring.kafka.consumer.properties.metrics.sample.window.ms=30000 # 用于维护metrics的样本数，默认：2spring.kafka.consumer.properties.metrics.num.samples=2 # 用于metrics的最高纪录等级。默认：Sensor.RecordingLevel.INFO.toString()#spring.kafka.consumer.properties.metrics.recording.level=Sensor.RecordingLevel.INFO.toString() # 类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计。默认：Collections.emptyList()#spring.kafka.consumer.properties.metric.reporters=Collections.emptyList() # 自动检查所消耗记录的CRC32。这可以确保没有线上或磁盘损坏的消息发生。此检查会增加一些开销，因此在寻求极高性能的情况下可能会被禁用。默认：truespring.kafka.consumer.properties.check.crcs=true # 连接空闲超时时间。因为consumer只与broker有连接（coordinator也是一个broker），所以这个配置的是consumer到broker之间的。默认：9 * 60 * 1000 = 540000spring.kafka.consumer.properties.connections.max.idle.ms=540000 # 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认：30000spring.kafka.consumer.properties.request.timeout.ms=30000 # 用于阻止的KafkaConsumer API的默认超时时间。KIP还为这样的阻塞API添加了重载，以支持指定每个阻塞API使用的特定超时，而不是使用default.api.timeout.ms设置的默认超时。特别是，添加了一个新的轮询（持续时间）API，它不会阻止动态分区分配。旧的poll（long）API已被弃用，将在以后的版本中删除。还为其他KafkaConsumer方法添加了重载，例如partitionsFor，listTopics，offsetsForTimes，beginningOffsets，endOffsets和close，它们接收持续时间。默认：60 * 1000 = 60000spring.kafka.consumer.properties.default.api.timeout.ms=60000 # 用户自定义interceptor。默认：Collections.emptyList()#spring.kafka.consumer.properties.interceptor.classes=Collections.emptyList() # 是否将内部topics的消息暴露给consumer。默认：truespring.kafka.consumer.properties.exclude.internal.topics=true # 默认：truespring.kafka.consumer.properties.internal.leave.group.on.close=true # 默认：IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT)#spring.kafka.consumer.properties.isolation.level=IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT) 4. 一些使用规范一个真实的公司要求的使用规范实例 Producer 部分参数设定: acks 设置为 “all” 即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失. retries = MAX 无限重试，直到你意识到出现了问题. 使用 callback 来处理消息失败发送逻辑. min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用. 其他一些超时参数: reconnect.backoff.ms, retry.backoff.ms , linger.ms 结合 batch.size 等. Consumer 部分参数设定: auto.offset.reset 设置为 “earliest” 避免 offset 丢失时跳过未消费的消息. 目前消息存储不统一, 部分使用 zookeeper, 部分使用 kafka topic. enable.auto.commit=false 关闭自动提交位移, 在消息被完整处理之后再手动提交位移. consumer 的并发受 partition 的限制. 如果消息处理量比较大的情况请提前与运维联系, 增加 partition 数量应对消费端并发. 默认topic partition 为6-8个.partition 也不是越多越好. 首先会增加 file 和 memory, 其次会延长选举时间, 并且会延长 offset 的查询时间. partition可以扩容但无法缩减. 极限情况的数据丢失现象. 即使将 ack 设置为 “all” 也会在一定情况下丢失消息. 因为 kafka 的高性能特性, 消息在写入 kafka 时并没有落盘 而是写入了 OS buffer 中. 使用 OS 的脏页刷新策略周期性落盘, 就算落盘 仍然会有 raid buffer. 前者机器宕机数据丢失, 后者机器跳电数据丢失. 对数据可靠性较高的场景建议 offset 手动提交. 自动提交当遇到业务系统上线被关闭时, 消息读取并且 offset 已经提交, 但是数据没有存储或者仍没来得及消费时, 消息状态在内存中无法保留, 重启应用会跳过消息 致使消息丢失. 5. 参考资料Spring-Kafka doc Kafka中文网]]></content>
      <tags>
        <tag>Kafka 消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次使用node.js的child_process模块来调用其他子进程学习]]></title>
    <url>%2F2019%2F08%2F23%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8node-js%E7%9A%84child-process%E6%A8%A1%E5%9D%97%E6%9D%A5%E5%90%AF%E5%8A%A8%E5%85%B6%E4%BB%96%E5%AD%90%E8%BF%9B%E7%A8%8B%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[背景 由于最近项目中遇到了一个问题,因公司项目是需要electron环境来运行,同时还依赖其他独立应用程序,所以出现了有一个场景: 就是在electron项目在被强制结束进程的情况下,独立程序并未被关闭,导致出现某些异常,所以现在需要在electron主进程重新启动时,关闭并重启依赖的其他独立应用程序(进程). 思路 首先会去寻找 electron 的官方文档,寻找是否本身就拥有调用和关闭其他进程的 API 来满足我们的需求.当然,经过一番寻找发现这条路是行不通的,官方并未提供这类 API. 我们接着可以发现官方文档有如下介绍: Electron是由Github开发，用HTML，CSS和JavaScript来构建跨平台桌面应用程序的一个开源库。 Electron通过将Chromium和Node.js合并到同一个运行时环境中，并将其打包为Mac，Windows和Linux系统下的应用来实现这一目的.所以我们可以想到可以利用node.js相关能力来操作第三方程序来作为子进程. 实现 第一步,查看官方文档,如下: 123456789101112131415161718`child_process` 模块提供了衍生子进程的能力（以一种与 popen(3) 类似但不相同的方式）。 此功能主要由 `child_process.spawn()` 函数提供：const &#123; spawn &#125; = require('child_process');const ls = spawn('ls', ['-lh', '/usr']);ls.stdout.on('data', (data) =&gt; &#123; console.log(`stdout: $&#123;data&#125;`);&#125;);ls.stderr.on('data', (data) =&gt; &#123; console.error(`stderr: $&#123;data&#125;`);&#125;);ls.on('close', (code) =&gt; &#123; console.log(`子进程退出，使用退出码 $&#123;code&#125;`);&#125;);默认情况下， stdin、 stdout 和 stderr 的管道会在父 Node.js 进程和衍生的子进程之间建立。 这些管道具有有限的（且平台特定的）容量。 如果子进程写入 stdout 时超出该限制且没有捕获输出，则子进程将会阻塞并等待管道缓冲区接受更多的数据。 这与 shell 中的管道的行为相同。 如果不消费输出，则使用 &#123; stdio: 'ignore' &#125; 选项。 看官方文档及实例可知,我们可以使用child_process模块的child_process.spawn()函数来调用指定的命令,并并捕获 stdout、 stderr、以及退出码. 无论在windows下还是在linux下,我们知道都可以通过执行相应的命令来启动其他应用程序,按照这个思路,我们可以利用child_process.spawn()函数来执行我们想要的命令,从而达到我们想要实现的效果.如果是想执行多条命令,例如首先查找该应用是否启动,若启动的话,先结束该进程,然后再启动它,我们可以编写一个批处理文件,同样可以使用child_process.spawn()函数来执行.官方对于使用.bat、.cad文件也给出了示例: 在 Windows 上衍生 .bat 和 .cmd 文件中英对照提交修改child_process.exec() 和 child_process.execFile() 之间区别的重要性可能因平台而异。 在 Unix 类型的操作系统（Unix、Linux、macOS）上，child_process.execFile() 可以更高效，因为默认情况下它不会衍生 shell。 但是在 Windows 上， .bat 和 .cmd 文件在没有终端的情况下不能自行执行，因此无法使用 child_process.execFile() 启动。 当在 Windows 上运行时，要调用 .bat 和 .cmd 文件，可以使用设置了 shell 选项的 child_process.spawn()、或 child_process.exec()、或衍生 cmd.exe 并将 .bat 或 .cmd 文件作为参数传入（也就是 shell 选项和 child_process.exec() 所做的）。 在任何情况下，如果脚本的文件名包含空格，则需要加上引号。 好的,到此我们就开始正式编码. child_process.exec() 和 child_process.execFile() 之间区别的重要性可能因平台而异。 在 Unix 类型的操作系统（Unix、Linux、macOS）上，child_process.execFile() 可以更高效，因为默认情况下它不会衍生 shell。 但是在 Windows 上， .bat 和 .cmd 文件在没有终端的情况下不能自行执行，因此无法使用 child_process.execFile() 启动。 当在 Windows 上运行时，要调用 .bat 和 .cmd 文件，可以使用设置了 shell 选项的 child_process.spawn()、或 child_process.exec()、或衍生 cmd.exe 并将 .bat 或 .cmd 文件作为参数传入（也就是 shell 选项和 child_process.exec() 所做的）。 在任何情况下，如果脚本的文件名包含空格，则需要加上引号。 因为child_process.exec()底层也使用了spawn()函数,所以我们直接选择使用child_process.spawn()来完成我们的需求 12345678910111213141516171819202122232425262728293031// 首先我们编写一个我们需要的.bat文件,代码如下,保存为startApp.batecho create softphone windows service@rem 进入当前文件夹cd /d %cd%# 以下命令可根据你想要启动的应用程序所在位置的不同而调整cd ../# 获取当前所在根目录的路径set rootPath=%cd%# 设置启动程序的路径set spPath=\xxxx\xxxx.exeset rdPath = \xxxx\xxxx.exe# 拼接完整路径地址set test1Path=%rootPath%%spPath%set test2Path=%rootPath%%rdPath%# 查找该进程是否启动,若启动则结束该进程tasklist | find /i "xxxx.exe" &amp;&amp; taskkill /F /im xxxx.exetasklist | find /i "test.exe" &amp;&amp; taskkill /F /im test.exe# 启动进程 '-a': 启动进程的参数start %test1Path% -astart %test2Path% -aecho start serviceexit 接下来使用child_process.spawn()函数来调用该批处理文件.首先展示一下目录结构. 123456|-src |-common |-startApp.bat |-test.js |- main.js该目录结构是在一个标准的 electron工程下的目录结构,调用.bat文件时要注意的一点就是文件与代码所处路径的问题. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 在common下新建一个test.js文件// 下面三个模块时为了在windows下一些输出错误信息中文字符会乱码的问题var iconv = require('iconv-lite');var encoding = 'cp936';var binaryEncoding = 'binary';function restartApp() &#123; // 引入spawn函数 const &#123; spawn &#125; = require('child_process'); const path=require('path'); // 使用cmd命令 `start restartApp`: 执行restartApp.bat const bat = spawn( ['/c', 'start restartApp'], &#123; cwd: path.join(__dirname,"../common"), //运行子进程的目录,此处千万要写对路径,否则会报'xxxx'不是不是内部或外部命令，也不是可运行的程序或批处理文件的提示 detached: true, //让父进程退出后，子进程能独立运行 shell: process.platform === 'win32', windowsHide: true &#125; ); bat.stdout.on('data', (data) =&gt; &#123; // 标准输出 console.log(data.toString()); &#125;); bat.stderr.on('data', (data) =&gt; &#123; // 输出错误信息 console.log('stderr: ' + iconv.decode(new Buffer(data, binaryEncoding), encoding), iconv.decode(new Buffer(data, binaryEncoding), encoding)); &#125;); bat.on('exit', (code) =&gt; &#123; console.log(`子进程退出，退出码 $&#123;code&#125;`); &#125;);&#125;closeService = () =&gt; &#123; const &#123; exec &#125; = require('child_process'); exec('tasklist | find /i "xxxx.exe" &amp;&amp; taskkill /F /im xxxx.exe', (error, stdout, stderr) =&gt; &#123; if(error !== null)&#123; console.info('stderr: ' + error); &#125; else console.info("成功") console.info('stdout: ' + stdout); console.info('stderr: ' + stderr); &#125;);&#125;;module.exports = &#123; restartApp, closeService&#125; 然后在electron项目里的main.js中调用 123456789101112131415161718...const &#123; app &#125; = require('electron');const &#123; restartApp &#125; = require('./test');...app.once('ready', () =&gt; &#123; ... // 调用刚刚的方法,在electron启动时,来重新启动其他进程 restartApp(); ...&#125;);app.on('window-all-closed', () =&gt; &#123; if (process.platform !== 'darwin') &#123; // electron关闭时,来关闭其他进程 closeService(); app.quit(); &#125;&#125;); 综上,就是一次使用在寻找如何在electron中创建其他子进程的全过程,在有一个需求之后我们首先应该去分析,为什么会产生这个需求,我们的目的是什么,有哪些熟知的手段去解决,然后可以去查看官方文档,一步一部的去尝试,就能解决问题.当然child_process模块中还有许多方法,可以实现类似的效果,还可以同步的创建子进程,监听相关事件然后响应,等待各位去探索.作者水平有限,文中有所不足之处还望大家不吝赐教. 参考资料Electron 文档 Node.js 中文网 v10.16.3]]></content>
      <tags>
        <tag>node.js</tag>
        <tag>electron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot+maven+mybatis整合时遇到一些需要注意的问题]]></title>
    <url>%2F2019%2F01%2F13%2Fspringboot-maven-mybatis%E6%95%B4%E5%90%88%E6%97%B6%E9%81%87%E5%88%B0%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[昨天在整合springboot+maven+mybatis遇到了一些问题，也花了一些时间去解决，其主要问题都是些配置的问题，由于版本的变动，相应的一些包的版本和配置文件的写法也会有些许不同，如果依旧按照之前的写法，可能编译通过，但是运行时就会报错，下面就简要记录一下遇到的一些坑。 mybatis-generator-core包无法下载 有的同学可能在此处会遇到该依赖无法下载下来，此时只需要将该依赖移到&lt;build&gt;&lt;/build&gt;标签外的&lt;dependencies&gt;&lt;/dependencies&gt;标签内就可以把jar包下载下来，再移回原处即可。只要有jar包就不会报错了，出现这个问题的原因尚不明确，笔者推测可能是maven的依赖传递存在的一些问题。 如图，找不到xml中的sql语句 该问题不影响springboot服务的正常启动，但是在访问该方法去执行mybatis生成的查询方法时则会出现，网上关于此类似的问题解决的方案主要分为三种： ​ 第一种：语法错误 Java DAO层接口 1public void delete(@Param(&quot;id&quot;)String id); Java 对应的mapper.xml文件 12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;xxx.xxx.xxx.Mapper&quot;&gt; &lt;!-- 删除数据 --&gt; &lt;delete id=&quot;delete&quot; parameterType=&quot;java.lang.String&quot;&gt; DELETE FROM xxx WHERE id=#&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 检查：1. 接口中方法名（delete）与xml文件中 id=”delete”是否一致 2. xml文件中的 namespace=”xxx.xxx.xxx.Mapper” 中的路径是否与接口文件路径一致 3.parameterType类型 与 resultType类型是否准确；resultMap与resultType是不一样的。 ​ 第二种：编译错误 定位到项目路径下：target\classes\ 中报错路径下，寻找对应的xml文件是否存在。 （1）若不存在对应的xml文件，则需要在pom.xml中加入以下代码： 12345678910111213141516&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 删除classes文件夹中文件，重新编译，出现了对应的xml文件即可。 （2）若存在xml文件，则打开xml文件，检查其中报错部分是否与源文件一致，不一致，则 先清除classes文件夹中文件，执行命令：mvn clean 清理内容，重新编译后即可。 ​ 第三种：配置错误 在配置文件中指定扫描包时，配置路径有问题。例如：spring配置文件中”basePackage”属性包名的指定一定要具体到接口所在包，而不要写父级甚至更高级别的包 ，否则可能出现问题；cn.dao与cn.*也可能导致错误；注解扫描时，可能没有扫描到包等。 mysql版本及驱动包问题 我安装的MySQL版本为8.0.13，对应的mysql-connector-java包的版本应该为8.0.11,MySQL5.7之前的版本可以使用5.1.41，同时对应的MySQL8以上的数据库连接url及driver和之前的有所区别，当运行时出现有关数据连接方面的错误可以先检查一下数据库连接url，在MySQL8中url要指定ssl,编码格式，时区等。先将MySQL8的url及driver写法展示如下： application.yml中： 12345678910spring: datasource: data: miaosha url: jdbc:mysql://localhost:3306/miaosha?serverTimezone=UTC&amp;acharacterEncoding=utf-8&amp;useSSL=false&amp;serverTimezone=GMT&amp;rewriteBatchedStatements=true username: root password: ****** #使用druid数据源 type: com.alibaba.druid.pool.DruidDataSource # mysql8以上推荐的驱动包 driver-class-name: com.mysql.cj.jdbc.Driver mybatis-generator.xml中： 12345678&lt;context id="DB2Tables" targetRuntime="MyBatis3"&gt; &lt;!--数据库连接地址账号密码--&gt; &lt;jdbcConnection driverClass="com.mysql.cj.jdbc.Driver" connectionURL="jdbc:mysql://localhost:3306/miaosha?serverTimezone=UTC&amp;amp;characterEncoding=utf-8&amp;amp;useSSL=false&amp;amp;serverTimezone=GMT&amp;amp;rewriteBatchedStatements=true" userId="root" password="******"&gt; &lt;/jdbcConnection&gt;&lt;/context&gt; 注：&amp;amp;为&amp;转义，在Idea中编写XML文件直接使用&amp;貌似会报错]]></content>
      <tags>
        <tag>Springboot</tag>
        <tag>Maven</tag>
        <tag>Mybatis</tag>
        <tag>MySQL</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F12%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker入门简易教程]]></title>
    <url>%2F2019%2F01%2F05%2Fdocker%E5%85%A5%E9%97%A8%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一. Docker简介关于Docker的介绍网上已经有了不少,简单来说,Docker是一种容器技术,但是是经过改进的容器技术,其改进的地方在于Docker容器引入了镜像,使的容器的创建和迁移变得方便快捷不少 Docker是一个开源的引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。 Docker通常用于如下场景： web应用的自动化打包和发布； 自动化测试和持续集成、发布； 在服务型环境中部署和调整数据库或其他的后台应用； 从头编译或者扩展现有的OpenShift或Cloud Foundry平台来搭建自己的PaaS环境。 Docker明显的特点: 轻量,内存占用小,密度高 快速,启动时间为毫秒级 隔离,沙盒技术类似于虚拟机 Docker技术的基础： namespace，容器隔离的基础，保证A容器看不到B容器. 6个名空间：User,Mnt,Network,UTS,IPC,Pid cgroups，容器资源统计和隔离。主要用到的cgroups子系统：cpu,blkio,device,freezer,memory unionfs，典型：aufs/overlayfs，分层镜像实现的基础 Docker组件： docker Client客户端————&gt;向docker服务器进程发起请求，如:创建、停止、销毁容器等操作 docker Server服务器进程—–&gt;处理所有docker的请求，管理所有容器 docker Registry镜像仓库——&gt;镜像存放的中央仓库，可看作是存放二进制的scm Docker的安装:安装在此处就不赘述, 因为其安装比较简单,且支持目前主流操作系统,相关Mac到Windows到Linux发行版的具体安装说明可以参考官方文档: docker安装 二.准备 Docker系统有两个程序: Docker服务端和Docker客户端.Docker服务端主要是作为管理所有容器的一个服务进程,管理着所有的容器.Docker客户端则相当于Docker服务端的远程控制器,可以用来控制Docker的服务端进程,Docker服务端和客户端通常情况下都运行在一台机器上。 本次的目标是安装Docker成功后检查Docker版本，以便确认Docker服务在运行并且可以通过客户端连接。 提示： 同其他Linux命令相似可以通过docker命令来查看所有的参数 通过docker version命令可以查看docker的版本 三.寻找可用的Docker镜像对于新手来讲，使用Docker最简单快捷的方式就是从现有容器镜像入手了，那么该如何去寻找现有的镜像呢，Docker官网专门有一个页面用来存储所有的可用镜像。（网址 ）。我们可以通过浏览这个网页来查找我们所需要的镜像或者是通过命令行工具来检索。 本节的目标是学会通过命令行工具来检索镜像。 命令行的用法为docker search 镜像名字 示例： 四.下载Docker镜像在第三小节中我们检索到了名为tutorial的镜像，我们可以使用如下命令来下载该镜像(docker命令和git命令有些类似的地方) 命令： docker pull learn/tutorial 当执行上述命令时可能会出现镜像已存在的情况，那么此时我们想删除之前的镜像该如何操作呢？如图所示： 总结： 当想删除镜像时，必须保证该镜像没有容器在使用，否则需要停止并删除该容器，才能进一步删除镜像。 停止所有的container，这样才能够删除其中的images：docker stop $(docker ps -a -q) 如果想要删除所有container的话再加一个指令：docker rm $(docker ps -a -q) 查看当前有些什么images: docker images 删除images，通过image的id来指定删除谁:docker rmi &lt;image id&gt; 想要删除untagged images，也就是那些id为&lt;None&gt;的image的话可以用 docker rmi $(docker images | grep &quot;^&lt;none&gt;&quot; | awk &quot;{print $3}&quot;) 要删除全部image的话docker rmi $(docker images -q) 五. 在Docker容器中运行“hello docker”通过docker run命令可以启动某一个镜像并运行一个命令 在此之前我们可能会问，什么是docker容器呢，和docker又有什么区别呢？ docker容器可以理解为在沙盒中运行的进程。这个沙盒包含了该进程运行的所有的必须的资源包括文件系统、系统类库、shell 环境等等。但这个沙盒默认是不会运行任何程序的。你需要在沙盒中运行一个进程来启动某一个容器。这个进程是该容器的唯一进程，所以当该进程结束的时候，容器也会完全的停止。 而docker则是一种容器技术· 现在我们想在刚刚下载的镜像中输出”hello docker”。为了达到这个目的，我们需要在这个容器中运行”echo”命令，输出”hello docker”。 六. 如何在容器中安装新程序下一步我们要做的事情是在容器里面安装一个简单的程序(ping)。你可以使用apt-get命令来安装ping程序： apt-get install -y ping。 备注：apt-get 命令执行完毕之后，容器就会停止，但对容器的改动不会丢失。 现在我们想在learn/tutorial镜像里面安装ping程序。 在执行apt-get命令的时候，要带上-y参数。如果不指定-y参数的话，apt-get命令会进入交互模式，需要用户输入命令来进行确认，但在docker环境中是无法响应这种交互的。 七. 保存对容器的修改当你对某一个容器做了修改之后（通过在容器中运行某一个命令），可以把对容器的修改保存下来，这样下次可以从保存后的最新状态运行该容器。docker中保存状态的过程称之为committing，它保存的新旧状态之间的区别，从而产生一个新的版本。 首先使用docker ps -l命令获得安装完ping命令之后容器的id。然后把这个镜像保存为learn/ping。 运行docker commit，可以查看该命令的参数列表。 你需要指定要提交保存容器的ID。(译者按：通过docker ps -l 命令获得) 无需拷贝完整的id，通常来讲最开始的三至四个字母即可区分。（备注：非常类似git里面的版本号) 八. 运行新的镜像ok，到现在为止，你已经建立了一个完整的、自成体系的docker环境，并且安装了ping命令在里面。它可以在任何支持docker环境的系统中运行啦！下面就让我们来体验一下吧！ 在新的镜像中运行ping www.baidu.com（当然也可以www.google.com， but……）命令。 一定要使用新的镜像名learn/ping来运行ping命令。(注：最开始下载的learn/tutorial镜像中是没有ping命令的) 九. 检查运行中的镜像现在你已经运行了一个docker容器，让我们来看下正在运行的容器。 使用docker ps命令可以查看所有正在运行中的容器列表，使用docker inspect命令我们可以查看更详细的关于某一个容器的信息。 查找某一个运行中容器的id，然后使用docker inspect命令查看容器的信息。 可以使用镜像id的前面部分，不需要完整的id。 十. 发布自己的docker镜像现在我们已经验证了新镜像可以正常工作，我们可以选择将其发布到官方的索引网站。还记得我们最开始下载的learn/tutorial镜像吧，我们也可以把我们自己编译的镜像发布到索引页面，一方面可以自己重用，另一方面也可以分享给其他人使用。 docker push命令可以将某一个镜像发布到官方网站。 注：push之前需要我们登陆docker repository的账号，这里就不再赘述了。 参考资料docker中文网 一小时Docker教程]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有关Hexo博客图片插件使用报错问题]]></title>
    <url>%2F2018%2F10%2F05%2F%E6%9C%89%E5%85%B3Hexo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在我们搭建好Hexo博客平台后,我们通常都会尝试写一篇博客去测试是否搭建成功,但是在博客里通常带有一些图片,在没有经过配置的情况下,图片是不会被正常显示的,下面就来简单介绍一下如何来配置让图片能够正常显示. hexo默认无法自动处理文章插入本地图片，需要通过扩展插件支持。 首先找到配置_config.yml文件中的post_asset_folder:false这个选项设置为true,启动 Asset 文件夹.当您设置post_asset_folder为true参数后，在建立文件时，Hexo 会自动建立一个与文章同名的文件夹，您可以把与该文章相关的所有资源都放到那个文件夹，如此一来，您便可以更方便的使用资源。 在hexo的目录下执行npm install https://github.com/CodeFalling/hexo-asset-image --save（需要等待一段时间,安装插件） 完成安装后,运行hexo n &quot;xxxx&quot;来生成md博文时会发现_posts目录下面会多出一个和文章名字一样的文件夹。图片就可以放在文件夹下面。结构如下： 12345本地图片测试├── test1.jpg├── test2.jpg└── test3.jpg本地图片测试.md 这样的目录结构（目录名和文章名一致），只要使用标准的md语法来引入![logo](本地图片测试/test1.jpg) 就可以插入图片。其中[]里面不写文字则没有图片标题。生成的结构为 12345public/2016/3/9/本地图片测试├── test1.jpg├── index.html├── test2.jpg└── test3.jpg 同时，生成的 html 是 &lt;img src=&quot;/2016/3/9/本地图片测试/test1.jpg&quot; alt=&quot;test1&quot;&gt; 而不是愚蠢的 &lt;img src=&quot;本地图片测试/test1.jpg&quot; alt=&quot;test1&quot;&gt; Issue: 由于hexo3版本后对很多插件支持有问题，hexo-asset-image插件在处理data.permalink链接时出现路径错误，把年月去掉了，导致最后生成的路径为%d/xxx/xxx需要对其做兼容处理。通过判断当前版本是否等于3的版本做不同的路径分割。 在代码中加入： 123456789var version = String(hexo.version).split('.');修改date.permalink处理：var link = data.permalink; if(version.length &gt; 0 &amp;&amp; Number(version[0]) == 3) var beginPos = getPosition(link, '/', 1) + 1;else var beginPos = getPosition(link, '/', 3) + 1;重新生成静态文件即可正确显示。 可直接安装已经修改过得插件npm install https://github.com/7ym0n/hexo-asset-image --save 参考链接：https://www.jianshu.com/p/3db6a61d3782]]></content>
      <tags>
        <tag>小问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Angular与其他前端框架的对比]]></title>
    <url>%2F2018%2F10%2F04%2FAngular%E4%B8%8E%E5%85%B6%E4%BB%96%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[与React对比 速度:react会先更新虚拟dom,再和实际dom进行对比 angular与之不相上下 FLUX架构:组件化 数据单向更新 服务器端渲染 预渲染 seo优化 与vue对比 个人主导(angular为google开发,号召力资金充足,vue相对来说有些不足) 只关注web(angular还可以开发客户端应用) 服务器端渲染(vue通过第三方库,angular有官方库) angular程序架构 组件:是Angular应用的基本构建块,你可以把一个组件理解为一段带有业务逻辑和数据的html 服务:用来封装可重用的业务逻辑 指令:允许你向html中添加自定义行为 模块:用来将应用中不同的部分组织成一个Angular框架可以理解的单元 组件、服务、模块是为了完成某些功能的,模块是为了打包、分发这些功能的. 组件组件(Component)必备三要素: 装饰器( @Component() ): 模版(Template) 控制器(Controller) 控制器←→控制器进行数据绑定 :插值表达式 启动angular应用 启动时加载了哪个页面?angular.json中有一项index属性，该属性对应的src目录下的indexhtml指向的是root src目录下的index.html,main属性则指向src下的main.ts文件 启动时加载了哪些脚本?main.ts 这些脚本做了什么事?负责引导angular应用的启动,引入相关的库及文件,来帮助angular应用程序启动 如何引用第三方类库(2-5)开发准备 npm install 安装到本地库 package.json -&gt; dependencies 引入到项目中 angular6 后在angular.json中在styles/script中分别引入,之前的版本在angular-cli.json中 npm install @types/jquery -D 安装jquery的ts类型描述文件,npm i @types/bootstrap -D安装bootstrap的ts类型描述文件 组件开发 App组件 导航栏组件 页脚组件 搜索表单组件 轮播图组件 商品展示组件 星级评价组件 *ngFor:指令 命令自动生成组件 ng g component xxx 在当前项目下生成组件 商品展示组件重点:angular始终是根据后台数据的变化来生成页面的,而不是去操纵dom元素 bootstrap元素居中:在bootstrap中实现元素居中的方法主要有这几种： 加类.text-center（子元素居中） 加类.center-block（自身居中） 利用bootstrap中列偏移的概念。例如：col-md-offset-2(外边距向右偏移两列) 数据绑定 通过插值表达式 直接将对象的属性展示出来 属性绑定:&lt;img [src]=&quot;imgUrl&quot; alt=&quot;商品图片&quot;&gt; 通过[]将标签属性绑定控制器的属性 样式绑定: [class.glyphicon-star-empty]=&quot;star&quot; 前面的class表示的是我后面要绑定的是一个class样式这个样式是由star的值决定的,star为true时会多出这么个样式,反之则不会 属性传递:]]></content>
      <tags>
        <tag>Angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[路由中传递参数]]></title>
    <url>%2F2018%2F10%2F04%2F%E5%A6%82%E4%BD%95%E5%9C%A8%E8%B7%AF%E7%94%B1%E4%B8%AD%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[修改路由配置中的path属性,使其可以携带参数 path: &#39;product/:id&#39; path的参数后加上 /:id 名为id变量 12345678910111213141516const routes: Routes = [ &#123;path: '', redirectTo: '/home', pathMatch: 'full'&#125;, &#123;path: 'chat', component: ChatComponent, outlet: 'aux'&#125;, &#123;path: 'home', component: HomeComponent&#125;, &#123;path: 'product/:id', component: ProductComponent, children: [ &#123;path: '', component: ProductDescComponent&#125;, &#123;path: 'seller/:id', component: SellerINfoComponent&#125; ],canActivate: [LoginGuard],canDeactivate: [UnsavedGuard], resolve: &#123; product: ProductResolve &#125;&#125;, &#123;path: '**', component: Code404Component&#125;]; 修改路由链接的参数来传递参数 &lt;a [routerLink]=&quot;[&#39;/product&#39;, 2]&quot; &gt;商品详情页&lt;/a&gt; 在控制器中修改代码 1234ngOnInit() &#123; // 参数订阅 订阅后声明一个匿名函数传进来之后从参数中取出ID赋值给本地的product // subscribe rxjs的语法 this.routeInfo.params.subscribe((params: Params) =&gt; this.productId = params['id']);]]></content>
      <tags>
        <tag>Angular6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 在layout 为 draft 的情况使用记录]]></title>
    <url>%2F2018%2F10%2F04%2FHexo%20%E5%9C%A8layout%20%E4%B8%BA%20draft%20%E7%9A%84%E6%83%85%E5%86%B5%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[layout 为 draft 的情况通过以上的学习，我们大概了解了一般情况下生成一篇博客的流程以及各个内容设置对页面效果的影响。那么我们现在学习一下 hexo new draft myDraftBlog 的情况。在没有执行这个命令之前，你可以去看下你的source文件夹，如果你是按照我之前的文章一步步进行的话，你会发现其中只有_post一个文件夹。 当我们执行上面的命令之后，类似post一样，我们同样会发现在根目录下source/_drafts文件夹中会出现一个myDraftBlog.md的文件。也就是说，_draft文件夹和里面的内容是你执行上述命令之后生成的。同样的，你开始编辑myDraftBlog.md文件，然后保存。 以此执行hexo g 和 hexo s 命令，打开4000端口之后你会发现，你完全看不到你刚才编辑的myDraftBlog这篇博客。这是为什么呢？因为layout 为 draft的时候，其实这个md文件是草稿状态，也就是说，这篇文章仅仅是作为你的草稿而不是正式稿，所以不会发表在博客主页上。草稿就是需要你不断完善的文章，知道有一天你觉得这篇文章可以正式发表了，那么如何才能将草稿发表成为正式稿件呢？官方文档其实有非常详细的说明：上面的官方文档会告诉你，通过publish命令可以将draft移动到_post文件夹下，形成正式的博客。也就是说，当你认为你的草稿已经完善到可以发表的状态时，执行hexo publish draft myDraftBlog.md，你就会发现，source/_draft文件夹下的myDraftBlog.md文件消失了，而在_post文件夹下你会找到myDraftBlog.md文件。这个时候再按照生成post文件的方式，依次执行hexo g hexo d 就可以将这篇草稿正式转为发表在网上的博客了。 因此，draft都是草稿，需要和publish命令配合使用。 当然，根据上面的官网说明，如果我们一定要查看我们的草稿，我们可以使用hexo g --draft， hexo s --draft命令来在本地预览我们的草稿效果。]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
